{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"anyschema: From Type Specifications to Dataframe Schemas","text":"<p><code>anyschema</code> is a Python library that enables conversions from type specifications (such as Pydantic models, SQLAlchemy tables, attrs classes, TypedDict, dataclasses) to native dataframe schemas (such as PyArrow, Polars, and Pandas).</p> <p>Development Status</p> <p><code>anyschema</code> is still in early development and possibly unstable.</p>"},{"location":"#installation","title":"Installation","text":"<p><code>anyschema</code> is available on pypi, and it can be installed directly via any package manager. For instance:</p> pipuv <pre><code>python -m pip install anyschema\n</code></pre> <pre><code>uv pip install anyschema\n</code></pre> <p>We suggest to install also <code>pydantic</code>, <code>attrs</code>, or <code>sqlalchemy</code> to follow along with the examples.</p> <ul> <li><code>anyschema</code> interoperability with pydantic models requires <code>pydantic&gt;=2.0.0</code>.</li> <li><code>anyschema</code> interoperability with attrs classes requires <code>attrs&gt;=24.0.0</code>.</li> <li><code>anyschema</code> interoperability with SQLAlchemy tables requires <code>sqlalchemy&gt;=2.0.0</code>.</li> </ul> pipuv <pre><code>python -m pip install \"anyschema[pydantic]\"\n# or\npython -m pip install \"anyschema[attrs]\"\n# or\npython -m pip install \"anyschema[sqlalchemy]\"\n</code></pre> <pre><code>uv pip install \"anyschema[pydantic]\"\n# or\nuv pip install \"anyschema[attrs]\"\n# or\nuv pip install \"anyschema[sqlalchemy]\"\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Here's a simple example showing how <code>anyschema</code> works.</p> <p>First define the type specification via a Pydantic model and create an <code>AnySchema</code> instance from it:</p> <pre><code>from anyschema import AnySchema\nfrom pydantic import BaseModel, PositiveInt\n\n\nclass Student(BaseModel):\n    name: str\n    age: PositiveInt  # (1)\n    classes: list[str]\n\n\nschema = AnySchema(spec=Student)\n</code></pre> <ol> <li>By using <code>PositiveInt</code> instead of python <code>int</code>, we ensure that the age is always a positive integer.     And we can translate this constraint into the resulting dataframe schema.</li> </ol> <p>Then, you can convert it to different dataframe schemas:</p> pyarrowpolarspandas <pre><code>pa_schema = schema.to_arrow()\nprint(pa_schema)\n</code></pre> <pre><code>name: string not null\nage: uint64 not null\nclasses: list&lt;item: string&gt; not null\n  child 0, item: string\n</code></pre> <pre><code>pl_schema = schema.to_polars()\nprint(pl_schema)\n</code></pre> <pre><code>Schema({'name': String, 'age': UInt64, 'classes': List(String)})\n</code></pre> <pre><code>pd_schema = schema.to_pandas()\nprint(pd_schema)\n</code></pre> <pre><code>{'name': &lt;class 'str'&gt;, 'age': 'uint64', 'classes': list&lt;item: string&gt;[pyarrow]}\n</code></pre>"},{"location":"#when-to-use-anyschema","title":"When to use <code>anyschema</code>","text":"<p><code>anyschema</code> is designed for scenarios where type specifications (e.g., Pydantic models, SQLAlchemy tables) serve as a single source of truth for both validation and dataframe schema generation.</p> <p>The typical use cases are: Data pipelines, database-to-dataframe workflows, API to database workflows, schema generation, type-safe data processing.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multiple Input Formats: Support for Pydantic models, SQLAlchemy tables (ORM and Core), attrs classes, TypedDict,     dataclasses, Python mappings and sequence of field specifications.</li> <li>Multiple Output Formats: Convert to PyArrow, Polars, or Pandas schemas.</li> <li>Modular Architecture: Extensible parser pipeline for custom type handling.</li> <li>Rich Type Support: Handles complex types including Optional, Union, List, nested structures, Pydantic-specific     types, SQLAlchemy types, and attrs classes.</li> <li>Narwhals Integration: Leverages Narwhals as the intermediate     representation.</li> </ul>"},{"location":"#core-components","title":"Core Components","text":""},{"location":"#type-parsers-steps","title":"Type Parsers (Steps)","text":"<p>Parser steps are modular components that convert type annotations to Narwhals dtypes. Each parser handles specific type patterns:</p> <ul> <li><code>ForwardRefStep</code>: Resolves forward references.</li> <li><code>UnionTypeStep</code>: Handles <code>Union</code> and <code>Optional</code> types.</li> <li><code>AnnotatedStep</code>: Extracts metadata from <code>typing.Annotated</code>.</li> <li><code>AnnotatedTypesStep</code>: Refines types based on constraints from the <code>annotated-types</code>     library.</li> <li><code>PydanticTypeStep</code>: Handles Pydantic-specific types.</li> <li><code>AttrsTypeStep</code>: Handles attrs classes.</li> <li><code>SQLAlchemyTypeStep</code>: Handles SQLAlchemy column types.</li> <li><code>PyTypeStep</code>: Handles basic Python types (fallback).</li> </ul> <p>Learn more about how these work together in the Architecture section.</p>"},{"location":"#spec-adapters","title":"Spec Adapters","text":"<p>Adapters convert input specifications into a common format that the parser pipeline can process:</p> <ul> <li><code>into_ordered_dict_adapter</code>: Handles Python dicts and sequences.</li> <li><code>typed_dict_adapter</code>: Extracts field information from TypedDict classes.</li> <li><code>dataclass_adapter</code>: Extracts field information from dataclasses.</li> <li><code>attrs_adapter</code>: Extracts field information from attrs classes.</li> <li><code>pydantic_adapter</code>: Extracts field information from Pydantic models.</li> <li><code>sqlalchemy_adapter</code>: Extracts field information from SQLAlchemy tables.</li> </ul> <p>See the API Reference for detailed documentation.</p>"},{"location":"#next-steps","title":"Next Steps","text":""},{"location":"#learning-path","title":"Learning Path","text":"<p>We recommend following this order to get the most out of anyschema:</p> <ol> <li>Getting Started: Learn the basics.</li> <li>Architecture: Understand the internal design and how components work together.</li> <li>Advanced Usage: Create custom parser steps and adapters for your specific needs.</li> <li>Best Practices: Learn patterns and anti-patterns for custom components.</li> <li>End-to-End Example: See a complete real-world example.</li> </ol>"},{"location":"#reference-materials","title":"Reference Materials","text":"<ul> <li>API Reference: Complete API documentation.</li> <li>Troubleshooting: Common issues and solutions.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please check out the GitHub repository to get started.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache-2.0 license.</p>"},{"location":"#why-anyschema","title":"Why <code>anyschema</code>?","text":"<p>The project was inspired by a Talk Python podcast episode featuring the creator of LanceDB, who mentioned the need to convert from Pydantic models to PyArrow schemas.</p> <p>This challenge led to a realization: such conversion could be generalized to many dataframe libraries by using Narwhals as an intermediate representation. <code>anyschema</code> makes this conversion seamless and extensible.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>This page provides a deep dive into anyschema's internal design and architecture. Understanding these concepts will help you extend anyschema and troubleshoot issues.</p>"},{"location":"architecture/#tldr-quick-summary","title":"TL;DR - Quick Summary","text":"<ul> <li> <p><code>anyschema</code> uses a pipeline architecture with two main phases:</p> <ol> <li>Adapters normalize input specs (Pydantic, dict, etc.) into <code>(name, type, constraints, metadata)</code> tuples.</li> <li>Parser pipeline converts each type to a Narwhals dtype by running parser steps in sequence.</li> </ol> </li> <li> <p>Parser steps run in order and the first step to handle a type to Narwhals returns:</p> <ol> <li><code>ForwardRefStep</code> - Resolves <code>ForwardRef('ClassName')</code></li> <li><code>UnionTypeStep</code> - Extracts non-None from <code>T | None</code></li> <li><code>AnnotatedStep</code> - Separates <code>Annotated[T, ...]</code> into type + metadata</li> <li><code>AnnotatedTypesStep</code> - Refines types based on constraints (e.g., <code>PositiveInt</code> -&gt; <code>UInt64</code>)</li> <li><code>PydanticTypeStep</code> - Handles Pydantic-specific types</li> <li><code>PyTypeStep</code> - Fallback for standard Python types</li> </ol> </li> <li> <p>Key concepts:</p> <ul> <li>Return <code>None</code> if your parser can't handle a type (lets next parser try).</li> <li>Use <code>self.pipeline.parse(..., constraints=constraints, metadata=metadata)</code> for recursion (handles nested types     like <code>list[YourType]</code>).</li> <li>Pass metadata through when recursing (<code>metadata=metadata</code>).</li> <li>Order matters - specialized parsers before general ones.</li> </ul> </li> <li> <p>To extend anyschema:</p> <ul> <li>Create custom parser steps to handle new types.</li> <li>Create custom adapters to support new schema formats.</li> <li>See Advanced Usage for examples.</li> </ul> </li> </ul>"},{"location":"architecture/#overview","title":"Overview","text":"<p><code>anyschema</code> follows a pipeline architecture with two main components:</p> <ol> <li>Spec Adapters: Convert input specifications into a normalized format.     For specifications supported directly by anyschema there is no need to create custom adapters.</li> <li>Parser Pipeline: A sequence of parser steps that convert types into Narwhals dtypes.</li> </ol> <pre><code>---\nconfig:\n  look: handDrawn\n  theme: neutral\n---\nflowchart TD\n    A[Input Specification] --&gt; B[Spec Adapter]\n    B --&gt; C[ParserPipeline]\n    C --&gt; D[Narwhals Schema]\n    D --&gt; E[Output Format]\n\n    NA[\"Pydantic Models&lt;br/&gt;Python mappings or sequences\"]\n    NB[\"pydantic_adapter&lt;br/&gt;into_ordered_dict_adapter\"]\n    NE[\"pyarrow, polars or pandas schema\"]\n\n    A -.-&gt; NA\n    B -.-&gt; NB\n    E -.-&gt; NE\n\n    C -.-&gt;|steps| P[Parser Steps]\n\n    subgraph Pipeline[\"Parser Pipeline Steps\"]\n        P1[ForwardRefStep] --&gt; P2[UnionTypeStep]\n        P2 --&gt; P3[AnnotatedStep]\n        P3 --&gt; P4[AnnotatedTypesStep]\n        P4 --&gt; P5[PydanticTypeStep]\n        P5 --&gt; P6[PyTypeStep]\n\n        N1[\"Resolves forward references\"]\n        N2[\"Extracts non-None types\"]\n        N3[\"Separates base type from metadata\"]\n        N4[\"Refines types based on constraints\"]\n        N5[\"Handles Pydantic-specific types\"]\n        N6[\"Fallback for standard Python types\"]\n\n        P1 -.-&gt; N1\n        P2 -.-&gt; N2\n        P3 -.-&gt; N3\n        P4 -.-&gt; N4\n        P5 -.-&gt; N5\n        P6 -.-&gt; N6\n    end\n\n    P -.-&gt; P1\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e6\n    style C fill:#f3e5f5\n    style D fill:#e8f5e9\n    style E fill:#e1f5ff\n    style NA fill:#fff,stroke:#999,stroke-dasharray: 5 5\n    style NB fill:#fff,stroke:#999,stroke-dasharray: 5 5\n    style NE fill:#fff,stroke:#999,stroke-dasharray: 5 5\n    style N1 fill:#fff,stroke:#999,stroke-dasharray: 5 5\n    style N2 fill:#fff,stroke:#999,stroke-dasharray: 5 5\n    style N3 fill:#fff,stroke:#999,stroke-dasharray: 5 5\n    style N4 fill:#fff,stroke:#999,stroke-dasharray: 5 5\n    style N5 fill:#fff,stroke:#999,stroke-dasharray: 5 5\n    style N6 fill:#fff,stroke:#999,stroke-dasharray: 5 5\n    style P fill:#f3e5f5</code></pre>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#spec-adapters","title":"Spec Adapters","text":"<p>Spec adapters are functions that convert various input formats into a unified representation, namely an iterable of <code>(name, type, constraints, metadata)</code> tuples.</p> <p>See the API Reference for detailed documentation.</p>"},{"location":"architecture/#parser-pipeline","title":"Parser Pipeline","text":"<p>The <code>ParserPipeline</code> orchestrates multiple parser steps, executing each step in sequence until one successfully handles the type (or raises an error).</p> <pre><code>from anyschema.parsers import ParserPipeline, PyTypeStep\n\npipeline = ParserPipeline(steps=[PyTypeStep()])\n\ndtype = pipeline.parse(int, constraints=(), metadata={})\nprint(dtype)\n</code></pre> <pre><code>Int64\n</code></pre>"},{"location":"architecture/#parser-steps","title":"Parser Steps","text":"<p>Each parser step is responsible for handling specific type patterns. Steps inherit from the <code>ParserStep</code> abstract base class.</p> Step Purpose Handles Order <code>ForwardRefStep</code> Resolves forward references before any type inspection can happen <code>ForwardRef('ClassName')</code> Must be first <code>UnionTypeStep</code> Extracts non-None types from Optional/Union types (dataframe libraries don't have Union types) <code>Union[T, None]</code>, <code>T | None</code>, <code>Optional[T]</code> Early - simplifies downstream parsers <code>AnnotatedStep</code> Separates base types from their constraints/metadata for independent processing <code>Annotated[T, metadata1, ...]</code> Before type-specific parsers <code>AnnotatedTypesStep</code> Refines types based on constraints (e.g., positive integers become unsigned types) Types with <code>annotated_types</code> or Pydantic constraints After <code>AnnotatedStep</code>, before type parsers <code>PydanticTypeStep</code> Handles Pydantic-specific types that need special processing beyond Python type inspection Pydantic types like <code>FutureDate</code> After metadata extraction, before fallback <code>PyTypeStep</code> Fallback parser for all standard Python types Basic (<code>int</code>, <code>str</code>, <code>bool</code>), temporal (<code>date</code>, <code>datetime</code>), container (<code>list[T]</code>, <code>tuple[T, ...]</code>), other (<code>Decimal</code>, <code>Enum</code>) Must be last"},{"location":"architecture/#parser-order-and-rationale","title":"Parser Order and Rationale","text":"<p>The order of parsers is critical:</p> <pre><code>steps = (\n    ForwardRefStep(),  # 1. Resolve forward references first, before any type inspection\n    UnionTypeStep(),  # 2. Extract non-None types from Optional/Union, it simplifies all downstream parsers\n    AnnotatedStep(),  # 3. Extract metadata from Annotated, it should happen before type-specific logic\n    AnnotatedTypesStep(),  # 4. Refine types based on metadata in `annotated_types` library\n    PydanticTypeStep(),  # 5. Handle Pydantic-specific types before falling back to Python types\n    PyTypeStep(),  # 6. The catch-all fallback for standard Python types\n)\n</code></pre>"},{"location":"architecture/#benefits-of-this-architecture","title":"Benefits of This Architecture","text":"<p>With this architecture we aim to achieve multiple goals at once:</p> <ul> <li>Modularity: Each parser has a single, well-defined responsibility.</li> <li>Composability: Parsers can be re-ordered, re-used, mixed and matched.</li> <li>Extensibility: New parsers can be added without modifying existing code.</li> <li>Recursion Simplification: Union/Optional extraction happens once, simplifying other parsers.</li> <li>Metadata Flow: Metadata is preserved and passed through the pipeline.</li> </ul>"},{"location":"architecture/#creating-custom-components","title":"Creating Custom Components","text":"<p>This architecture is designed to be easily extensible and customizable, both in terms of adding new parser steps and creating custom adapters.</p>"},{"location":"architecture/#adding-custom-parser-steps","title":"Adding Custom Parser Steps","text":"<p>You can extend the parser pipeline in two ways:</p> <ol> <li> <p>Using <code>ParserPipeline.with_steps</code> method (recommended): Extend the pipeline     with custom steps:</p> <pre><code>from anyschema import AnySchema\nfrom anyschema.parsers import ParserPipeline, ParserStep\nimport narwhals as nw\n\n\nclass MyCustomStep(ParserStep):\n    def parse(self, input_type, constraints, metadata):\n        # Your custom parsing logic\n        ...\n\n\n# Extend the auto pipeline with custom step\ncustom_pipeline = ParserPipeline.from_auto(steps=MyCustomStep())\n\n# Use it with AnySchema\nschema = AnySchema(spec=my_spec, pipeline=custom_pipeline)\n</code></pre> <p>By default, <code>with_steps</code> positions your custom step right after the last preprocessing step found (trying <code>AnnotatedStep</code>, <code>UnionTypeStep</code>, <code>ForwardRefStep</code> in that order), ensuring it runs after type preprocessing but before library-specific or fallback parsers.</p> </li> <li> <p>Building from scratch: Create a pipeline with explicit steps:</p> <pre><code>from anyschema.parsers import ParserPipeline, PyTypeStep\n\ncustom_pipeline = ParserPipeline(steps=[MyCustomStep(), PyTypeStep()])\nschema = AnySchema(spec=my_spec, pipeline=custom_pipeline)\n</code></pre> </li> </ol> <p>Learn how to extend <code>anyschema</code> with custom functionality. For more detailed examples, see the Advanced Usage guide.</p>"},{"location":"architecture/#metadata-preservation","title":"Metadata Preservation","text":"<p>Metadata flows through the pipeline:</p> <pre><code>from typing import Annotated\nfrom pydantic import BaseModel, Field, PositiveInt\n\n\nclass Product(BaseModel):\n    quantity: PositiveInt  # PositiveInt is itself an Annotated type with constraints\n</code></pre> <p>The pipeline processes this as:</p> <ol> <li><code>pydantic_adapter</code> extracts: <code>(\"quantity\", int, (Gt(gt=0),), {})</code></li> <li><code>AnnotatedStep</code> extracts constraints: <code>(Gt(gt=0),)</code> and passes through</li> <li><code>AnnotatedTypesStep</code> refines based on constraints and converts to <code>UInt64</code> (instead of <code>Int64</code>)</li> </ol>"},{"location":"architecture/#recursion-and-nested-types","title":"Recursion and Nested Types","text":"<p>Parser steps can recursively call the pipeline for nested types:</p> <pre><code>from anyschema import AnySchema\nfrom pydantic import BaseModel\n\n\nclass Address(BaseModel):\n    street: str\n    city: str\n\n\nclass Person(BaseModel):\n    name: str\n    addresses: list[Address]  # Nested type!\n</code></pre> <p>Processing flow:</p> <ol> <li><code>pydantic_adapter</code> yields: <code>(\"addresses\", list[Address], (), {})</code></li> <li><code>PyTypeStep</code> sees <code>list[T]</code> and recursively calls: <code>pipeline.parse(Address, constraints=(), metadata={})</code></li> <li>The pipeline handles <code>Address</code> as a Pydantic model (which is considered a <code>Struct</code>)</li> <li>Result: <code>List(Struct([('street', String), ('city', String)]))</code></li> </ol>"},{"location":"architecture/#complete-flow-example","title":"Complete Flow Example","text":"<p>Let's trace a complete example through the system:</p> <pre><code>from pydantic import BaseModel, PositiveInt\nfrom anyschema import AnySchema\n\n\nclass Student(BaseModel):\n    name: str\n    age: PositiveInt\n    classes: list[str] | None\n\n\nschema = AnySchema(spec=Student)\n</code></pre> <p>Complete processing flow:</p> <ol> <li> <p>Spec Adapter: (<code>pydantic_adapter</code>):</p> <ul> <li>Extracts: <code>(\"name\", str, (), {})</code></li> <li>Extracts: <code>(\"age\", PositiveInt, (), {})</code></li> <li>Extracts: <code>(\"classes\", list[str] | None, (), {})</code></li> </ul> </li> <li> <p>Parser pipeline for <code>name: str</code>:</p> <ul> <li><code>ForwardRefStep</code>: Not a <code>ForwardRef</code> -&gt; returns <code>None</code></li> <li><code>UnionTypeStep</code>: Not a <code>Union</code> -&gt; returns <code>None</code></li> <li><code>AnnotatedStep</code>: Not <code>Annotated</code> -&gt; returns <code>None</code></li> <li><code>AnnotatedTypesStep</code>: No metadata -&gt; returns <code>None</code></li> <li><code>PydanticTypeStep</code>: Not a Pydantic type -&gt; returns <code>None</code></li> <li><code>PyTypeStep</code>: <code>str</code> -&gt; returns <code>String()</code></li> <li>Result: <code>String()</code></li> </ul> </li> <li> <p>Parser pipeline for <code>age: PositiveInt</code>:</p> <ul> <li><code>ForwardRefStep</code>: Not a <code>ForwardRef</code> -&gt; returns <code>None</code></li> <li><code>UnionTypeStep</code>: Not a <code>Union</code> -&gt; returns <code>None</code></li> <li><code>AnnotatedStep</code>: <code>PositiveInt</code> is <code>Annotated[int, ...]</code> -&gt; extracts <code>int</code> with metadata</li> <li> <p>Recursively parse <code>int</code> with metadata:</p> <ul> <li><code>AnnotatedTypesStep</code>: Metadata indicates positive constraint -&gt; returns <code>UInt64()</code></li> <li>Result: <code>UInt64()</code></li> </ul> </li> </ul> </li> <li> <p>Parse <code>classes: list[str] | None</code>:</p> <ul> <li><code>ForwardRefStep</code>: Not a <code>ForwardRef</code> -&gt; returns <code>None</code></li> <li><code>UnionTypeStep</code>: Is a <code>Union</code>! Extracts <code>list[str]</code> (non-None type)</li> <li> <p>Recursively parse <code>list[str]</code>:</p> <ul> <li><code>ForwardRefStep</code>: Not a <code>ForwardRef</code> -&gt; returns <code>None</code></li> <li><code>UnionTypeStep</code>: Not a <code>Union</code> -&gt; returns <code>None</code></li> <li><code>AnnotatedStep</code>: Not <code>Annotated</code> -&gt; returns <code>None</code></li> <li><code>AnnotatedTypesStep</code>: No metadata -&gt; returns <code>None</code></li> <li><code>PydanticTypeStep</code>: Not a Pydantic type -&gt; returns <code>None</code></li> <li><code>PyTypeStep</code>: <code>list[str]</code> -&gt; recursively parse <code>str</code> (within the generic) -&gt; returns <code>List(String())</code></li> <li>Result: <code>List(String())</code> (nullable)</li> </ul> </li> </ul> </li> <li> <p>Final Schema:</p> <pre><code>Schema({\"name\": String(), \"age\": UInt64(), \"classes\": List(String())})\n</code></pre> </li> </ol>"},{"location":"api-reference/","title":"API Reference","text":"<p>This page provides detailed documentation for all public APIs in anyschema.</p> <p>For conceptual explanations, see the Architecture page. For practical examples, see Getting Started and Advanced Usage.</p>"},{"location":"api-reference/adapters/","title":"Spec Adapters","text":"<p>Adapters convert various input specifications into a normalized format for parsing.</p> <p>Learn how to create custom adapters in the Advanced Usage guide.</p> <p>The following built-in adapters are not meant to be used directly. They serve more as an example than anything else.</p>"},{"location":"api-reference/adapters/#anyschema.adapters","title":"<code>anyschema.adapters</code>","text":""},{"location":"api-reference/adapters/#anyschema.adapters.attrs_adapter","title":"<code>attrs_adapter(spec: AttrsClassType) -&gt; FieldSpecIterable</code>","text":"<p>Adapter for attrs classes.</p> <p>Extracts field information from an attrs class and converts it into an iterator yielding field information as <code>(field_name, field_type, constraints, metadata)</code> tuples.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>AttrsClassType</code> <p>An attrs class (not an instance).</p> required <p>Yields:</p> Type Description <code>FieldSpecIterable</code> <p>A tuple of <code>(field_name, field_type, constraints, metadata)</code> for each field. - <code>field_name</code>: The name of the field as defined in the attrs class - <code>field_type</code>: The type annotation of the field - <code>constraints</code>: Always empty tuple (attrs doesn't use constraints) - <code>metadata</code>: A dict of custom metadata from the field's metadata dict</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from attrs import define, field\n&gt;&gt;&gt;\n&gt;&gt;&gt; @define\n... class Student:\n...     name: str\n...     age: int = field(metadata={\"description\": \"Student age\"})\n&gt;&gt;&gt;\n&gt;&gt;&gt; list(attrs_adapter(Student))\n[('name', &lt;class 'str'&gt;, (), {}), ('age', &lt;class 'int'&gt;, (), {'description': 'Student age'})]\n</code></pre> Source code in <code>anyschema/adapters.py</code> <pre><code>def attrs_adapter(spec: AttrsClassType) -&gt; FieldSpecIterable:\n    \"\"\"Adapter for attrs classes.\n\n    Extracts field information from an attrs class and converts it into an iterator\n    yielding field information as `(field_name, field_type, constraints, metadata)` tuples.\n\n    Arguments:\n        spec: An attrs class (not an instance).\n\n    Yields:\n        A tuple of `(field_name, field_type, constraints, metadata)` for each field.\n            - `field_name`: The name of the field as defined in the attrs class\n            - `field_type`: The type annotation of the field\n            - `constraints`: Always empty tuple (attrs doesn't use constraints)\n            - `metadata`: A dict of custom metadata from the field's metadata dict\n\n    Examples:\n        &gt;&gt;&gt; from attrs import define, field\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @define\n        ... class Student:\n        ...     name: str\n        ...     age: int = field(metadata={\"description\": \"Student age\"})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; list(attrs_adapter(Student))\n        [('name', &lt;class 'str'&gt;, (), {}), ('age', &lt;class 'int'&gt;, (), {'description': 'Student age'})]\n    \"\"\"\n    import attrs\n\n    # get_type_hints eagerly evaluates annotations, which alleviates us from\n    # needing to evaluate ForwardRef's by hand later on.\n    # However, it may fail for classes defined in local scopes (e.g., nested classes in functions)\n    # so we fall back to using field.type directly if get_type_hints fails.\n    try:\n        annot_map = get_type_hints(spec)\n    except Exception:  # pragma: no cover  # noqa: BLE001\n        # If we can't get type hints, use field.type directly\n        annot_map = {}\n\n    attrs_fields = attrs.fields(spec)\n    attrs_field_names = {field.name for field in attrs_fields}\n\n    # Check for annotations that aren't attrs fields\n    # This can happen when a class inherits from an attrs class but isn't decorated itself\n    if annot_map and (missing_fields := tuple(field for field in annot_map if field not in attrs_field_names)):\n        missing_str = \", \".join(f\"'{f}'\" for f in sorted(missing_fields))\n        msg = (\n            f\"Class '{spec.__name__}' has annotations ({missing_str}) that are not attrs fields. \"\n            f\"If this class inherits from an attrs class, you must also decorate it with @attrs.define \"\n            f\"or @attrs.frozen to properly define these fields.\"\n        )\n        raise AssertionError(msg)\n\n    for field in attrs_fields:\n        field_name = field.name\n        field_type = annot_map.get(field_name, field.type)\n\n        # Extract metadata if present - attrs stores it as a mapping\n        # Create a copy to avoid mutating the original attrs field metadata\n        metadata = dict(field.metadata) if field.metadata else {}\n\n        yield field_name, field_type, (), metadata\n</code></pre>"},{"location":"api-reference/adapters/#anyschema.adapters.dataclass_adapter","title":"<code>dataclass_adapter(spec: DataclassType) -&gt; FieldSpecIterable</code>","text":"<p>Adapter for dataclasses.</p> <p>Converts a dataclass into an iterator yielding field information as <code>(field_name, field_type, constraints, metadata)</code> tuples.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>DataclassType</code> <p>A dataclass with annotated fields.</p> required <p>Yields:</p> Type Description <code>FieldSpecIterable</code> <p>A tuple of <code>(field_name, field_type, constraints, metadata)</code> for each field.</p> <code>FieldSpecIterable</code> <p>Constraints are always empty, and metadata is extracted from dataclass field.metadata.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from dataclasses import dataclass, field\n&gt;&gt;&gt;\n&gt;&gt;&gt; @dataclass\n... class Student:\n...     name: str\n...     age: int = field(metadata={\"description\": \"Student age\"})\n&gt;&gt;&gt;\n&gt;&gt;&gt; list(dataclass_adapter(Student))\n[('name', &lt;class 'str'&gt;, (), {}), ('age', &lt;class 'int'&gt;, (), {'description': 'Student age'})]\n</code></pre> Source code in <code>anyschema/adapters.py</code> <pre><code>def dataclass_adapter(spec: DataclassType) -&gt; FieldSpecIterable:\n    \"\"\"Adapter for dataclasses.\n\n    Converts a dataclass into an iterator yielding field information as\n    `(field_name, field_type, constraints, metadata)` tuples.\n\n    Arguments:\n        spec: A dataclass with annotated fields.\n\n    Yields:\n        A tuple of `(field_name, field_type, constraints, metadata)` for each field.\n        Constraints are always empty, and metadata is extracted from dataclass field.metadata.\n\n    Examples:\n        &gt;&gt;&gt; from dataclasses import dataclass, field\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @dataclass\n        ... class Student:\n        ...     name: str\n        ...     age: int = field(metadata={\"description\": \"Student age\"})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; list(dataclass_adapter(Student))\n        [('name', &lt;class 'str'&gt;, (), {}), ('age', &lt;class 'int'&gt;, (), {'description': 'Student age'})]\n    \"\"\"\n    # get_type_hints eagerly evaluates annotations, which alleviates us from\n    #  needing to evaluate ForwardRef's by hand later on.\n    annot_map = get_type_hints(spec)\n\n    # Get dataclass fields\n    dataclass_fields = dc_fields(spec)\n    dataclass_field_names = {field.name for field in dataclass_fields}\n\n    # Check for annotations that aren't dataclass fields\n    # This can happen when a class inherits from a dataclass but isn't decorated itself\n    if missing_fields := tuple(field for field in annot_map if field not in dataclass_field_names):\n        missing_str = \", \".join(f\"'{f}'\" for f in missing_fields)\n        msg = (\n            f\"Class '{spec.__name__}' has annotations ({missing_str}) that are not dataclass fields. \"\n            f\"If this class inherits from a dataclass, you must also decorate it with @dataclass \"\n            f\"to properly define these fields.\"\n        )\n        raise AssertionError(msg)\n\n    for field in dataclass_fields:\n        # Extract metadata dict from dataclass field\n        # Create a copy to avoid mutating the original dataclass field metadata\n        metadata = dict(field.metadata) if field.metadata else {}\n\n        # Python 3.14+ dataclass fields have a doc parameter\n        # Check if field has doc attribute and if it's not None\n        if (doc := getattr(field, \"doc\", None)) and (get_anyschema_value_by_key(metadata, key=\"description\") is None):\n            set_anyschema_meta(metadata, key=\"description\", value=doc)\n\n        yield field.name, annot_map[field.name], (), metadata\n</code></pre>"},{"location":"api-reference/adapters/#anyschema.adapters.into_ordered_dict_adapter","title":"<code>into_ordered_dict_adapter(spec: IntoOrderedDict) -&gt; FieldSpecIterable</code>","text":"<p>Adapter for Python mappings and sequences of field definitions.</p> <p>Converts a mapping (e.g., <code>dict</code>) or sequence of 2-tuples into an iterator yielding field information as <code>(field_name, field_type, constraints, metadata)</code> tuples.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>IntoOrderedDict</code> <p>A mapping from field names to types, or a sequence of <code>(name, type)</code> tuples.</p> required <p>Yields:</p> Type Description <code>FieldSpecIterable</code> <p>A tuple of <code>(field_name, field_type, constraints, metadata)</code> for each field.</p> <code>FieldSpecIterable</code> <p>Both constraints and metadata are always empty for this adapter.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; list(into_ordered_dict_adapter({\"name\": str, \"age\": int}))\n[('name', &lt;class 'str'&gt;, (), {}), ('age', &lt;class 'int'&gt;, (), {})]\n</code></pre> <pre><code>&gt;&gt;&gt; list(into_ordered_dict_adapter([(\"age\", int), (\"name\", str)]))\n[('age', &lt;class 'int'&gt;, (), {}), ('name', &lt;class 'str'&gt;, (), {})]\n</code></pre> Source code in <code>anyschema/adapters.py</code> <pre><code>def into_ordered_dict_adapter(spec: IntoOrderedDict) -&gt; FieldSpecIterable:\n    \"\"\"Adapter for Python mappings and sequences of field definitions.\n\n    Converts a mapping (e.g., `dict`) or sequence of 2-tuples into an iterator yielding field information as\n    `(field_name, field_type, constraints, metadata)` tuples.\n\n    Arguments:\n        spec: A mapping from field names to types, or a sequence of `(name, type)` tuples.\n\n    Yields:\n        A tuple of `(field_name, field_type, constraints, metadata)` for each field.\n        Both constraints and metadata are always empty for this adapter.\n\n    Examples:\n        &gt;&gt;&gt; list(into_ordered_dict_adapter({\"name\": str, \"age\": int}))\n        [('name', &lt;class 'str'&gt;, (), {}), ('age', &lt;class 'int'&gt;, (), {})]\n\n        &gt;&gt;&gt; list(into_ordered_dict_adapter([(\"age\", int), (\"name\", str)]))\n        [('age', &lt;class 'int'&gt;, (), {}), ('name', &lt;class 'str'&gt;, (), {})]\n    \"\"\"\n    for field_name, field_type in OrderedDict(spec).items():\n        yield field_name, field_type, (), {}\n</code></pre>"},{"location":"api-reference/adapters/#anyschema.adapters.pydantic_adapter","title":"<code>pydantic_adapter(spec: type[BaseModel]) -&gt; FieldSpecIterable</code>","text":"<p>Adapter for Pydantic BaseModel classes.</p> <p>Extracts field information from a Pydantic model class and converts it into an iterator yielding field information as <code>(field_name, field_type, constraints, metadata)</code> tuples.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>type[BaseModel]</code> <p>A Pydantic <code>BaseModel</code> class (not an instance).</p> required <p>Yields:</p> Type Description <code>FieldSpecIterable</code> <p>A tuple of <code>(field_name, field_type, constraints, metadata)</code> for each field. - <code>field_name</code>: The name of the field as defined in the model - <code>field_type</code>: The type annotation of the field - <code>constraints</code>: A tuple of constraint items from <code>Annotated</code> types (e.g., <code>Gt</code>, <code>Le</code>) - <code>metadata</code>: A dict of custom metadata from <code>json_schema_extra</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pydantic import BaseModel, Field\n&gt;&gt;&gt; from typing import Annotated\n&gt;&gt;&gt;\n&gt;&gt;&gt; class Student(BaseModel):\n...     name: str = Field(description=\"Student name\")\n...     age: Annotated[int, Field(ge=0)]\n&gt;&gt;&gt;\n&gt;&gt;&gt; spec_fields = list(pydantic_adapter(Student))\n&gt;&gt;&gt; spec_fields[0]\n('name', &lt;class 'str'&gt;, (), {'anyschema': {'description': 'Student name'}})\n&gt;&gt;&gt; spec_fields[1]\n('age', ForwardRef('Annotated[int, Field(ge=0)]', is_class=True), (), {})\n</code></pre> Source code in <code>anyschema/adapters.py</code> <pre><code>def pydantic_adapter(spec: type[BaseModel]) -&gt; FieldSpecIterable:\n    \"\"\"Adapter for Pydantic BaseModel classes.\n\n    Extracts field information from a Pydantic model class and converts it into an iterator\n    yielding field information as `(field_name, field_type, constraints, metadata)` tuples.\n\n    Arguments:\n        spec: A Pydantic `BaseModel` class (not an instance).\n\n    Yields:\n        A tuple of `(field_name, field_type, constraints, metadata)` for each field.\n            - `field_name`: The name of the field as defined in the model\n            - `field_type`: The type annotation of the field\n            - `constraints`: A tuple of constraint items from `Annotated` types (e.g., `Gt`, `Le`)\n            - `metadata`: A dict of custom metadata from `json_schema_extra`\n\n    Examples:\n        &gt;&gt;&gt; from pydantic import BaseModel, Field\n        &gt;&gt;&gt; from typing import Annotated\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; class Student(BaseModel):\n        ...     name: str = Field(description=\"Student name\")\n        ...     age: Annotated[int, Field(ge=0)]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; spec_fields = list(pydantic_adapter(Student))\n        &gt;&gt;&gt; spec_fields[0]\n        ('name', &lt;class 'str'&gt;, (), {'anyschema': {'description': 'Student name'}})\n        &gt;&gt;&gt; spec_fields[1]\n        ('age', ForwardRef('Annotated[int, Field(ge=0)]', is_class=True), (), {})\n    \"\"\"\n    for field_name, field_info in spec.model_fields.items():\n        # Extract constraints from metadata (these are the annotated-types constraints)\n        constraints = tuple(field_info.metadata)\n\n        json_schema_extra = field_info.json_schema_extra\n        # Create a copy of metadata to avoid mutating the original Pydantic Field\n        metadata = dict(json_schema_extra) if json_schema_extra and not callable(json_schema_extra) else {}\n        # Extract description from Pydantic Field if present and not already in metadata\n        if (description := field_info.description) is not None and (\n            get_anyschema_value_by_key(metadata, key=\"description\") is None\n        ):\n            set_anyschema_meta(metadata, key=\"description\", value=description)\n\n        yield field_name, field_info.annotation, constraints, metadata\n</code></pre>"},{"location":"api-reference/adapters/#anyschema.adapters.sqlalchemy_adapter","title":"<code>sqlalchemy_adapter(spec: SQLAlchemyTableType) -&gt; FieldSpecIterable</code>","text":"<p>Adapter for SQLAlchemy tables.</p> <p>Extracts field information from a SQLAlchemy Table (Core) or DeclarativeBase class (ORM) and converts it into an iterator yielding field information as <code>(field_name, field_type, metadata)</code> tuples.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>SQLAlchemyTableType</code> <p>A SQLAlchemy Table instance or DeclarativeBase subclass.</p> required <p>Yields:</p> Type Description <code>FieldSpecIterable</code> <p>A tuple of <code>(field_name, field_type, metadata)</code> for each column. - <code>field_name</code>: The name of the column - <code>field_type</code>: The SQLAlchemy column type - <code>metadata</code>: A tuple containing column metadata (nullable, etc.)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sqlalchemy import Table, Column, Integer, String, MetaData\n&gt;&gt;&gt;\n&gt;&gt;&gt; metadata = MetaData()\n&gt;&gt;&gt; user_table = Table(\n...     \"user\",\n...     metadata,\n...     Column(\"id\", Integer, primary_key=True),\n...     Column(\"name\", String(50)),\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; spec_fields = list(sqlalchemy_adapter(user_table))\n&gt;&gt;&gt; spec_fields[0]\n('id', Integer(), (), {'anyschema': {'nullable': False}})\n&gt;&gt;&gt; spec_fields[1]\n('name', String(length=50), (), {'anyschema': {'nullable': True}})\n</code></pre> <pre><code>&gt;&gt;&gt; from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\n&gt;&gt;&gt;\n&gt;&gt;&gt; class Base(DeclarativeBase):\n...     pass\n&gt;&gt;&gt;\n&gt;&gt;&gt; class User(Base):\n...     __tablename__ = \"user\"\n...     id: Mapped[int] = mapped_column(primary_key=True)\n...     name: Mapped[str]\n&gt;&gt;&gt;\n&gt;&gt;&gt; spec_fields = list(sqlalchemy_adapter(User))\n&gt;&gt;&gt; spec_fields[0]\n('id', Integer(), (), {'anyschema': {'nullable': False}})\n&gt;&gt;&gt; spec_fields[1]\n('name', String(length=50), (), {'anyschema': {'nullable': True}})\n</code></pre> Source code in <code>anyschema/adapters.py</code> <pre><code>def sqlalchemy_adapter(spec: SQLAlchemyTableType) -&gt; FieldSpecIterable:\n    \"\"\"Adapter for SQLAlchemy tables.\n\n    Extracts field information from a SQLAlchemy Table (Core) or DeclarativeBase class (ORM)\n    and converts it into an iterator yielding field information as `(field_name, field_type, metadata)` tuples.\n\n    Arguments:\n        spec: A SQLAlchemy Table instance or DeclarativeBase subclass.\n\n    Yields:\n        A tuple of `(field_name, field_type, metadata)` for each column.\n            - `field_name`: The name of the column\n            - `field_type`: The SQLAlchemy column type\n            - `metadata`: A tuple containing column metadata (nullable, etc.)\n\n    Examples:\n        &gt;&gt;&gt; from sqlalchemy import Table, Column, Integer, String, MetaData\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; metadata = MetaData()\n        &gt;&gt;&gt; user_table = Table(\n        ...     \"user\",\n        ...     metadata,\n        ...     Column(\"id\", Integer, primary_key=True),\n        ...     Column(\"name\", String(50)),\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; spec_fields = list(sqlalchemy_adapter(user_table))\n        &gt;&gt;&gt; spec_fields[0]\n        ('id', Integer(), (), {'anyschema': {'nullable': False}})\n        &gt;&gt;&gt; spec_fields[1]\n        ('name', String(length=50), (), {'anyschema': {'nullable': True}})\n\n        &gt;&gt;&gt; from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column  # doctest: +SKIP\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; class Base(DeclarativeBase):  # doctest: +SKIP\n        ...     pass  # doctest: +SKIP\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; class User(Base):  # doctest: +SKIP\n        ...     __tablename__ = \"user\"  # doctest: +SKIP\n        ...     id: Mapped[int] = mapped_column(primary_key=True)  # doctest: +SKIP\n        ...     name: Mapped[str]  # doctest: +SKIP\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; spec_fields = list(sqlalchemy_adapter(User))  # doctest: +SKIP\n        &gt;&gt;&gt; spec_fields[0]  # doctest: +SKIP\n        ('id', Integer(), (), {'anyschema': {'nullable': False}})\n        &gt;&gt;&gt; spec_fields[1]  # doctest: +SKIP\n        ('name', String(length=50), (), {'anyschema': {'nullable': True}})\n    \"\"\"\n    from sqlalchemy import Table\n\n    table = spec if isinstance(spec, Table) else spec.__table__\n\n    meta_mapping: dict[Literal[\"nullable\", \"unique\", \"description\"], Literal[\"nullable\", \"unique\", \"doc\"]] = {\n        \"nullable\": \"nullable\",\n        \"unique\": \"unique\",\n        \"description\": \"doc\",\n    }\n\n    for column in table.columns:\n        # Create a copy of column.info to avoid mutating the original SQLAlchemy column\n        metadata = dict(column.info)\n\n        # Extract anyschema metadata from SQLAlchemy column attributes\n        for key, column_attr in meta_mapping.items():\n            if (value := getattr(column, column_attr, None)) is not None and (\n                get_anyschema_value_by_key(metadata, key=key) is None\n            ):\n                set_anyschema_meta(metadata, key=key, value=value)\n\n        yield (column.name, column.type, (), metadata)\n</code></pre>"},{"location":"api-reference/adapters/#anyschema.adapters.typed_dict_adapter","title":"<code>typed_dict_adapter(spec: TypedDictType) -&gt; FieldSpecIterable</code>","text":"<p>Adapter for TypedDict classes.</p> <p>Converts a TypedDict into an iterator yielding field information as <code>(field_name, field_type, constraints, metadata)</code> tuples.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>TypedDictType</code> <p>A TypedDict class (not an instance).</p> required <p>Yields:</p> Type Description <code>FieldSpecIterable</code> <p>A tuple of <code>(field_name, field_type, constraints, metadata)</code> for each field.</p> <code>FieldSpecIterable</code> <p>Both constraints and metadata are always empty for this adapter.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from typing_extensions import TypedDict\n&gt;&gt;&gt;\n&gt;&gt;&gt; class Student(TypedDict):\n...     name: str\n...     age: int\n&gt;&gt;&gt;\n&gt;&gt;&gt; list(typed_dict_adapter(Student))\n[('name', &lt;class 'str'&gt;, (), {}), ('age', &lt;class 'int'&gt;, (), {})]\n</code></pre> Source code in <code>anyschema/adapters.py</code> <pre><code>def typed_dict_adapter(spec: TypedDictType) -&gt; FieldSpecIterable:\n    \"\"\"Adapter for TypedDict classes.\n\n    Converts a TypedDict into an iterator yielding field information as\n    `(field_name, field_type, constraints, metadata)` tuples.\n\n    Arguments:\n        spec: A TypedDict class (not an instance).\n\n    Yields:\n        A tuple of `(field_name, field_type, constraints, metadata)` for each field.\n        Both constraints and metadata are always empty for this adapter.\n\n    Examples:\n        &gt;&gt;&gt; from typing_extensions import TypedDict\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; class Student(TypedDict):\n        ...     name: str\n        ...     age: int\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; list(typed_dict_adapter(Student))\n        [('name', &lt;class 'str'&gt;, (), {}), ('age', &lt;class 'int'&gt;, (), {})]\n    \"\"\"\n    type_hints = get_type_hints(spec)\n    for field_name, field_type in type_hints.items():\n        yield field_name, field_type, (), {}\n</code></pre>"},{"location":"api-reference/adapters/#adapters-specification","title":"Adapters specification","text":"<p>Adapters must follow this signature:</p> <pre><code>from typing import Iterator, TypeAlias, Callable, Any, Generator\nfrom anyschema.typing import FieldConstraints, FieldMetadata, FieldName, FieldType\n\nFieldSpec: TypeAlias = tuple[FieldName, FieldType, FieldConstraints, FieldMetadata]\n\n\ndef my_custom_adapter(spec: Any) -&gt; Iterator[FieldSpec]:\n    \"\"\"\n    Yields tuples of (field_name, field_type, constraints, metadata).\n\n    - name (str): The name of the field\n    - type (type): The type annotation of the field\n    - constraints (tuple): Type constraints (e.g., Gt(0), Le(100) from annotated-types)\n    - metadata (dict): Custom metadata dictionary for additional information\n    \"\"\"\n    ...\n</code></pre> <p>They don't need to be functions; any callable is acceptable.</p>"},{"location":"api-reference/anyschema/","title":"<code>anyschema</code> top level API","text":""},{"location":"api-reference/anyschema/#anyschema","title":"<code>anyschema</code>","text":""},{"location":"api-reference/anyschema/#anyschema.AnyField","title":"<code>AnyField</code>  <code>dataclass</code>","text":"<p>A structured field descriptor.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the field.</p> required <code>dtype</code> <code>DType</code> <p>The Narwhals data type.</p> required <code>nullable</code> <code>bool</code> <p>Whether the field accepts null values.</p> <code>False</code> <code>unique</code> <code>bool</code> <p>Whether values must be unique.</p> <code>False</code> <code>description</code> <code>str | None</code> <p>Optional field description.</p> <code>None</code> <code>metadata</code> <code>Mapping[str, Any]</code> <p>Custom metadata dictionary.</p> <code>dict()</code> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the field.</p> <code>dtype</code> <code>DType</code> <p>The Narwhals data type of the field.</p> <code>nullable</code> <code>bool</code> <p>Whether the field can contain null values. Defaults to False. Parsing a type specification will flag this as True if:</p> <ul> <li>The <code>{\"anyschema\": {\"nullable\": True}}</code> metadata key is explicitly set to True, or</li> <li>The type is <code>Optional[T]</code> or <code>T | None</code> (which automatically sets the metadata)</li> </ul> <code>unique</code> <code>bool</code> <p>Whether all values in this field must be unique. Defaults to False. Determined by the <code>{\"anyschema\": {\"unique\": ...}}</code> metadata key or SQLAlchemy column unique argument.</p> <code>description</code> <code>str | None</code> <p>Human-readable field description.</p> <code>metadata</code> <code>Mapping[str, Any]</code> <p>Custom metadata dict containing any metadata that is not under the <code>anyschema</code> or <code>x-anyschema</code> namespace.</p> <p>Examples:</p> <p>Creating a simple field:</p> <pre><code>&gt;&gt;&gt; import narwhals as nw\n&gt;&gt;&gt; from anyschema import AnyField\n&gt;&gt;&gt;\n&gt;&gt;&gt; field = AnyField(\n...     name=\"user_id\",\n...     dtype=nw.Int64(),\n...     nullable=False,\n...     unique=True,\n...     description=\"Primary key\",\n... )\n&gt;&gt;&gt; field\nAnyField(name='user_id', dtype=Int64, nullable=False, unique=True, description='Primary key', metadata={})\n</code></pre> <p>AnyField with optional type:</p> <pre><code>&gt;&gt;&gt; field = AnyField(\n...     name=\"email\",\n...     dtype=nw.String(),\n...     nullable=True,\n...     unique=False,\n...     metadata={\"fmt\": \"email\"},\n... )\n&gt;&gt;&gt; field\nAnyField(name='email', dtype=String, nullable=True, unique=False, description=None, metadata={'fmt': 'email'})\n</code></pre>"},{"location":"api-reference/anyschema/#anyschema.AnyField.__hash__","title":"<code>__hash__() -&gt; int</code>","text":"<p>Return hash of the instance.</p> <p>Creates a hashable tuple representation. <code>metadata</code> is a dict and not hashable, we convert it to a sorted tuple of items</p>"},{"location":"api-reference/anyschema/#anyschema.AnySchema","title":"<code>AnySchema</code>","text":"<p>The class implements the workflow to convert from a (schema) specification to a native dataframe schema object.</p> <p>The <code>AnySchema</code> class enables to convert from type specifications (such as Pydantic models) to native dataframe schemas (such as <code>pandas</code>, <code>polars</code> and <code>pyarrow</code>).</p> <p>This class provides a unified interface for generating dataframe schemas from various input formats, with extensible type parsing through a modular pipeline architecture.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>Spec</code> <p>The input specification. This can be:</p> <ul> <li>A Narwhals Schema.     In this case parsing data types is a no-op and the schema is used directly.</li> <li>A python mapping (like <code>dict</code>) or     sequence of tuples containing     the field name and type (e.g., <code>{\"name\": str, \"age\": int}</code> or <code>[(\"name\", str), (\"age\", int)]</code>).</li> <li>A TypedDict class (not an instance).     The fields are extracted using type hint introspection.</li> <li>A dataclass class (not an instance).     The fields are extracted using dataclass introspection.</li> <li>A Pydantic Model class (not an instance).     The fields are extracted using Pydantic's schema introspection.</li> <li>An attrs class (not an instance).     The fields are extracted using attrs introspection.</li> <li>A SQLAlchemy Table     instance or DeclarativeBase     subclass (not an instance).     The fields are extracted using SQLAlchemy's schema introspection.</li> </ul> required <code>pipeline</code> <code>ParserPipeline | IntoParserPipeline</code> <p>Control how types are parsed into Narwhals dtypes. Options:</p> <ul> <li><code>\"auto\"</code> (default): Automatically select the appropriate parser steps based on the installed dependencies.</li> <li>A <code>ParserPipeline</code> instance: Use this pipeline directly.</li> <li> <p>A sequence of <code>ParserStep</code> instances to build a pipeline.</p> <p>Warning: Order matters! More specific parsers should come before general ones.</p> </li> </ul> <p>This allows for custom type parsing logic and extensibility through user-defined parser steps.</p> <code>'auto'</code> <code>adapter</code> <code>Adapter | None</code> <p>A custom adapter callable that converts the spec into a sequence of field specifications. The callable should yield tuples of <code>(name, type, constraints, metadata)</code> for each field in the spec.</p> <ul> <li><code>name</code> (str): The name of the field</li> <li><code>type/annotation</code> (type): The type annotation of the field</li> <li><code>constraints</code> (tuple): Constraints associated with the field     (e.g., <code>Gt</code>, <code>Le</code> constraints from <code>annotated-types</code>).</li> <li><code>metadata</code> (dict): Custom metadata dictionary.     (e.g., from <code>json_schema_extra</code> in Pydantic Field's, <code>metadata</code> in attrs and dataclasses field's)</li> </ul> <p>This allows for custom field specification logic and extensibility from user-defined adapters.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fields</code> <code>dict[str, AnyField]</code> <p>A mapping from field names to <code>AnyField</code> objects, containing the parsed dtype and field-level metadata (nullable, unique, etc.).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>spec</code> type is unknown and <code>adapter</code> is not specified.</p> <code>NotImplementedError</code> <p>If a type in the spec cannot be parsed by any parser in the pipeline.</p> <code>UnsupportedDTypeError</code> <p>If a type is explicitly unsupported (e.g., Union with mixed types).</p> <p>Examples:</p> <p>Basic usage with a Pydantic model:</p> <pre><code>&gt;&gt;&gt; from anyschema import AnySchema\n&gt;&gt;&gt; from pydantic import BaseModel, PositiveInt\n&gt;&gt;&gt;\n&gt;&gt;&gt; class Student(BaseModel):\n...     name: str\n...     age: PositiveInt\n...     classes: list[str] | None\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema = AnySchema(spec=Student)\n</code></pre> <p>Convert to PyArrow schema:</p> <pre><code>&gt;&gt;&gt; pa_schema = schema.to_arrow()\n&gt;&gt;&gt; print(pa_schema)\nname: string not null\nage: uint64 not null\nclasses: list&lt;item: string&gt;\n  child 0, item: string\n</code></pre> <p>Convert to Polars schema:</p> <pre><code>&gt;&gt;&gt; pl_schema = schema.to_polars()\n&gt;&gt;&gt; print(pl_schema)\nSchema({'name': String, 'age': UInt64, 'classes': List(String)})\n</code></pre> <p>Convert to Pandas schema:</p> <pre><code>&gt;&gt;&gt; pd_schema = schema.to_pandas()\n&gt;&gt;&gt; print(pd_schema)\n{'name': &lt;class 'str'&gt;, 'age': 'uint64', 'classes': list&lt;item: string&gt;[pyarrow]}\n</code></pre> <p>Using a plain Python dict:</p> <pre><code>&gt;&gt;&gt; schema = AnySchema(spec={\"id\": int, \"name\": str, \"active\": bool})\n&gt;&gt;&gt; print(schema.to_arrow())\nid: int64 not null\nname: string not null\nactive: bool not null\n</code></pre> <p>Using a TypedDict:</p> <pre><code>&gt;&gt;&gt; from typing_extensions import TypedDict\n&gt;&gt;&gt;\n&gt;&gt;&gt; class Product(TypedDict):\n...     id: int\n...     name: str\n...     price: float | None\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema = AnySchema(spec=Product)\n&gt;&gt;&gt; print(schema.to_arrow())\nid: int64 not null\nname: string not null\nprice: double\n</code></pre> See also <ul> <li>ParserStep: Base class for custom parser steps</li> <li>ParserPipeline: Pipeline for chaining parser steps</li> <li>Spec Adapters: Adapters for various specifications</li> </ul>"},{"location":"api-reference/anyschema/#anyschema.AnySchema.fields","title":"<code>fields: dict[str, AnyField]</code>  <code>property</code>","text":"<p>Get all fields as a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, AnyField]</code> <p>Dictionary mapping field names to AnyField objects.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema import AnySchema\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema = AnySchema(spec={\"id\": int, \"name\": str})\n&gt;&gt;&gt; fields = schema.fields\n&gt;&gt;&gt; fields[\"name\"].dtype\nString\n&gt;&gt;&gt; list(fields.keys())\n['id', 'name']\n</code></pre>"},{"location":"api-reference/anyschema/#anyschema.AnySchema.__eq__","title":"<code>__eq__(other: object) -&gt; bool</code>","text":"<p>Check equality between two AnySchema instances.</p> <p>Two AnySchema instances are considered equal if they have the same fields.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>object</code> <p>Object to compare with.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if both instances have identical fields, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema import AnySchema\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema1 = AnySchema(spec={\"name\": str, \"age\": int})\n&gt;&gt;&gt; schema2 = AnySchema(spec={\"name\": str, \"age\": int})\n&gt;&gt;&gt; schema3 = AnySchema(spec={\"name\": str})\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema1 == schema2\nTrue\n&gt;&gt;&gt; schema1 == schema3\nFalse\n</code></pre>"},{"location":"api-reference/anyschema/#anyschema.AnySchema.__hash__","title":"<code>__hash__() -&gt; int</code>","text":"<p>Compute hash value for the AnySchema instance.</p> <p>This allows AnySchema instances to be used in sets and as dictionary keys.</p> <p>Returns:</p> Type Description <code>int</code> <p>Hash value of the schema.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema import AnySchema\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema1 = AnySchema(spec={\"name\": str, \"age\": int})\n&gt;&gt;&gt; schema2 = AnySchema(spec={\"name\": str, \"age\": int})\n&gt;&gt;&gt;\n&gt;&gt;&gt; hash(schema1) == hash(schema2)\nTrue\n&gt;&gt;&gt; len({schema1, schema2})\n1\n</code></pre>"},{"location":"api-reference/anyschema/#anyschema.AnySchema.descriptions","title":"<code>descriptions(*, named: bool = False) -&gt; dict[str, str | None] | tuple[str | None, ...]</code>","text":"<pre><code>descriptions(*, named: Literal[True]) -&gt; dict[str, str | None]\n</code></pre><pre><code>descriptions(*, named: Literal[False] = False) -&gt; tuple[str | None, ...]\n</code></pre> <p>Get the descriptions of all fields.</p> <p>Parameters:</p> Name Type Description Default <code>named</code> <code>bool</code> <p>If True, return a dict mapping field names to descriptions. If False (default), return a tuple of descriptions in field order.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, str | None] | tuple[str | None, ...]</code> <p>Either a dict of {field_name: description} or a tuple of descriptions. Description values are None for fields without descriptions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema import AnySchema\n&gt;&gt;&gt; from pydantic import BaseModel, Field\n&gt;&gt;&gt;\n&gt;&gt;&gt; class User(BaseModel):\n...     id: int = Field(description=\"User ID\")\n...     name: str\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema = AnySchema(spec=User)\n&gt;&gt;&gt; schema.descriptions()\n('User ID', None)\n&gt;&gt;&gt; schema.descriptions(named=True)\n{'id': 'User ID', 'name': None}\n</code></pre>"},{"location":"api-reference/anyschema/#anyschema.AnySchema.dtypes","title":"<code>dtypes(*, named: bool = False) -&gt; dict[str, DType] | tuple[DType, ...]</code>","text":"<pre><code>dtypes(*, named: Literal[True]) -&gt; dict[str, DType]\n</code></pre><pre><code>dtypes(*, named: Literal[False] = False) -&gt; tuple[DType, ...]\n</code></pre> <p>Get the data types of all fields.</p> <p>Parameters:</p> Name Type Description Default <code>named</code> <code>bool</code> <p>If True, return a dict mapping field names to dtypes. If False (default), return a tuple of dtypes in field order.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, DType] | tuple[DType, ...]</code> <p>Either a dict of {field_name: dtype} or a tuple of dtypes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema import AnySchema\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema = AnySchema(spec={\"id\": int, \"name\": str, \"score\": float})\n&gt;&gt;&gt; schema.dtypes()\n(Int64, String, Float64)\n&gt;&gt;&gt; schema.dtypes(named=True)\n{'id': Int64, 'name': String, 'score': Float64}\n</code></pre>"},{"location":"api-reference/anyschema/#anyschema.AnySchema.field","title":"<code>field(name: str) -&gt; AnyField</code>","text":"<p>Get a specific field by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the field to retrieve.</p> required <p>Returns:</p> Type Description <code>AnyField</code> <p>The AnyField object for the specified field name.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the field name does not exist in the schema.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema import AnySchema\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema = AnySchema(spec={\"id\": int, \"name\": str})\n&gt;&gt;&gt; field = schema.field(\"name\")\n&gt;&gt;&gt; field.dtype\nString\n&gt;&gt;&gt; field.nullable\nFalse\n</code></pre>"},{"location":"api-reference/anyschema/#anyschema.AnySchema.names","title":"<code>names() -&gt; tuple[str, ...]</code>","text":"<p>Return the names of all fields in the schema.</p> <p>Returns:</p> Type Description <code>tuple[str, ...]</code> <p>Tuple of field names in the order they appear in the schema.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema import AnySchema\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema = AnySchema(spec={\"id\": int, \"name\": str, \"age\": int})\n&gt;&gt;&gt; schema.names()\n('id', 'name', 'age')\n</code></pre>"},{"location":"api-reference/anyschema/#anyschema.AnySchema.nullables","title":"<code>nullables(*, named: bool = False) -&gt; dict[str, bool] | tuple[bool, ...]</code>","text":"<pre><code>nullables(*, named: Literal[True]) -&gt; dict[str, bool]\n</code></pre><pre><code>nullables(*, named: Literal[False] = False) -&gt; tuple[bool, ...]\n</code></pre> <p>Get the nullable flags of all fields.</p> <p>Parameters:</p> Name Type Description Default <code>named</code> <code>bool</code> <p>If True, return a dict mapping field names to nullable flags. If False (default), return a tuple of nullable flags in field order.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, bool] | tuple[bool, ...]</code> <p>Either a dict of {field_name: nullable} or a tuple of nullable flags. A field is nullable if it accepts None values (e.g., <code>Optional[int]</code> or <code>int | None</code>).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema import AnySchema\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema = AnySchema(spec={\"id\": int, \"name\": str, \"age\": int | None})\n&gt;&gt;&gt; schema.nullables()\n(False, False, True)\n&gt;&gt;&gt; schema.nullables(named=True)\n{'id': False, 'name': False, 'age': True}\n</code></pre>"},{"location":"api-reference/anyschema/#anyschema.AnySchema.to_arrow","title":"<code>to_arrow() -&gt; pa.Schema</code>","text":"<p>Converts input model into pyarrow schema.</p> <p>Returns:</p> Type Description <code>Schema</code> <p>The converted pyarrow schema.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema import AnySchema\n&gt;&gt;&gt; from pydantic import BaseModel\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; class User(BaseModel):\n...     id: int\n...     username: str\n...     email: str | None\n...     is_active: bool\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema = AnySchema(spec=User)\n&gt;&gt;&gt; schema.to_arrow()\nid: int64 not null\nusername: string not null\nemail: string\nis_active: bool not null\n</code></pre>"},{"location":"api-reference/anyschema/#anyschema.AnySchema.to_pandas","title":"<code>to_pandas(*, dtype_backend: DTypeBackend | Iterable[DTypeBackend] = None) -&gt; dict[str, str | pd.ArrowDtype | type]</code>","text":"<p>Converts input model into mapping of {field_name: pandas_dtype}.</p> <p>Parameters:</p> Name Type Description Default <code>dtype_backend</code> <code>DTypeBackend | Iterable[DTypeBackend]</code> <p>which kind of data type backend to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, str | ArrowDtype | type]</code> <p>The converted pandas schema.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema import AnySchema\n&gt;&gt;&gt; from pydantic import BaseModel\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; class User(BaseModel):\n...     id: int\n...     username: str\n...     email: str\n...     is_active: bool\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema = AnySchema(spec=User)\n&gt;&gt;&gt; schema.to_pandas(dtype_backend=(\"pyarrow\", \"numpy_nullable\", \"pyarrow\", None))\n{'id': 'Int64[pyarrow]', 'username': 'string', 'email': string[pyarrow], 'is_active': 'bool'}\n</code></pre>"},{"location":"api-reference/anyschema/#anyschema.AnySchema.to_polars","title":"<code>to_polars() -&gt; pl.Schema</code>","text":"<p>Converts input model into polars Schema.</p> <p>Returns:</p> Type Description <code>Schema</code> <p>The converted polars schema.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema import AnySchema\n&gt;&gt;&gt; from pydantic import BaseModel\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; class User(BaseModel):\n...     id: int\n...     username: str\n...     email: str\n...     is_active: bool\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema = AnySchema(spec=User)\n&gt;&gt;&gt; schema.to_polars()\nSchema({'id': Int64, 'username': String, 'email': String, 'is_active': Boolean})\n</code></pre>"},{"location":"api-reference/anyschema/#anyschema.AnySchema.to_pyspark","title":"<code>to_pyspark() -&gt; PySparkStructType</code>","text":"<p>Converts input model into pyspark StructType.</p> <p>Returns:</p> Type Description <code>StructType</code> <p>The converted pyspark StructType.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema import AnySchema\n&gt;&gt;&gt; from pydantic import BaseModel\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; class User(BaseModel):\n...     id: int\n...     username: str | None\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema = AnySchema(spec=User)\n&gt;&gt;&gt; schema.to_pyspark()\nStructType([StructField('id', LongType(), False), StructField('username', StringType(), True)])\n</code></pre>"},{"location":"api-reference/anyschema/#anyschema.AnySchema.uniques","title":"<code>uniques(*, named: bool = False) -&gt; dict[str, bool] | tuple[bool, ...]</code>","text":"<pre><code>uniques(*, named: Literal[True]) -&gt; dict[str, bool]\n</code></pre><pre><code>uniques(*, named: Literal[False] = False) -&gt; tuple[bool, ...]\n</code></pre> <p>Get the unique constraint flags of all fields.</p> <p>Parameters:</p> Name Type Description Default <code>named</code> <code>bool</code> <p>If True, return a dict mapping field names to unique flags. If False (default), return a tuple of unique flags in field order.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, bool] | tuple[bool, ...]</code> <p>Either a dict of {field_name: unique} or a tuple of unique flags. A field is unique if it has a uniqueness constraint (e.g., from SQLAlchemy or metadata).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema import AnySchema\n&gt;&gt;&gt; from sqlalchemy import Column, Integer, String, Table, MetaData\n&gt;&gt;&gt;\n&gt;&gt;&gt; users_table = Table(\n...     \"users\",\n...     MetaData(),\n...     Column(\"id\", Integer, primary_key=True),\n...     Column(\"email\", String, unique=True),\n...     Column(\"name\", String),\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; schema = AnySchema(spec=users_table)\n&gt;&gt;&gt; schema.uniques()\n(False, True, False)\n&gt;&gt;&gt; schema.uniques(named=True)\n{'id': False, 'email': True, 'name': False}\n</code></pre>"},{"location":"api-reference/anyschema/#anyschema.show_versions","title":"<code>show_versions() -&gt; None</code>","text":"<p>Print useful debugging information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema import show_versions\n&gt;&gt;&gt; show_versions()\n</code></pre>"},{"location":"api-reference/exceptions/","title":"Exceptions","text":""},{"location":"api-reference/exceptions/#anyschema.exceptions","title":"<code>anyschema.exceptions</code>","text":""},{"location":"api-reference/exceptions/#anyschema.exceptions.UnavailablePipelineError","title":"<code>UnavailablePipelineError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Exception raised when a parser does not have a ParserPipeline set.</p> Source code in <code>anyschema/exceptions.py</code> <pre><code>class UnavailablePipelineError(ValueError):\n    \"\"\"Exception raised when a parser does not have a ParserPipeline set.\"\"\"\n</code></pre>"},{"location":"api-reference/exceptions/#anyschema.exceptions.UnsupportedDTypeError","title":"<code>UnsupportedDTypeError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Exception raised when a DType is not supported.</p> Source code in <code>anyschema/exceptions.py</code> <pre><code>class UnsupportedDTypeError(ValueError):\n    \"\"\"Exception raised when a DType is not supported.\"\"\"\n</code></pre>"},{"location":"api-reference/parsers/","title":"Parsers","text":""},{"location":"api-reference/parsers/#pipeline","title":"Pipeline","text":"<p>A parser pipeline is a sequence of parser steps that process type annotations to produce Narwhals dtypes.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.ParserPipeline","title":"<code>anyschema.parsers.ParserPipeline</code>","text":"<p>A pipeline of parser steps that tries each parser in sequence.</p> <p>This allows for composable parsing where multiple parsers can be tried until one successfully handles the type.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>IntoParserPipeline</code> <p>Control how parser steps are configured:</p> <ul> <li><code>\"auto\"</code> (default): Automatically select appropriate parser steps based on installed dependencies.</li> <li>A <code>ParserPipeline</code> instance: Use the pipeline as-is.</li> <li>A sequence of <code>ParserStep</code> instances: Create pipeline with these steps.</li> </ul> <code>'auto'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the steps are not a sequence of <code>ParserStep</code> instances or a <code>ParserPipeline</code>.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.ParserPipeline.from_auto","title":"<code>from_auto(steps: ParserStep | Sequence[ParserStep], *more_steps: ParserStep, at_position: int | Literal['auto'] = 'auto') -&gt; Self</code>  <code>classmethod</code>","text":"<p>Create an auto pipeline with custom steps efficiently (no copying needed).</p> Tip <p>This is the most efficient way to create a pipeline with custom steps when starting from the auto configuration, as it doesn't need to copy the auto pipeline's steps.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>ParserStep | Sequence[ParserStep]</code> <p><code>ParserStep</code>(s) to add to the auto pipeline.</p> required <code>*more_steps</code> <code>ParserStep</code> <p>Additional <code>ParserStep</code>(s) to add, specified as positional arguments.</p> <code>()</code> <code>at_position</code> <code>int | Literal['auto']</code> <p>Position where to insert the step(s). Options:</p> <ul> <li>An integer index (can be negative for counting from the end).</li> <li><code>\"auto\"</code> (default): Automatically determines the \"best\" position.     The step(s) will be inserted after the last preprocessing step found.</li> </ul> <code>'auto'</code> <p>Returns:</p> Type Description <code>Self</code> <p>A new <code>ParserPipeline</code> instance with the auto steps and custom steps.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema.parsers import ParserPipeline, ParserStep\n&gt;&gt;&gt; import narwhals as nw\n&gt;&gt;&gt; from anyschema.typing import FieldConstraints, FieldMetadata, FieldType\n&gt;&gt;&gt;\n&gt;&gt;&gt; class CustomType: ...\n&gt;&gt;&gt;\n&gt;&gt;&gt; class CustomParserStep(ParserStep):\n...     def parse(\n...         self, input_type: FieldType, constraints: FieldConstraints, metadata: FieldMetadata\n...     ) -&gt; nw.DType | None:\n...         return nw.String() if input_type is CustomType else None\n&gt;&gt;&gt;\n&gt;&gt;&gt; pipeline = ParserPipeline.from_auto(CustomParserStep())\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Add multiple custom steps\n&gt;&gt;&gt; pipeline = ParserPipeline.from_auto(CustomParserStep(), CustomParserStep())\n&gt;&gt;&gt;\n&gt;&gt;&gt; pipeline.parse(CustomType, constraints=(), metadata={})\nString\n</code></pre>"},{"location":"api-reference/parsers/#anyschema.parsers.ParserPipeline.parse","title":"<code>parse(input_type: FieldType, constraints: FieldConstraints, metadata: FieldMetadata, *, strict: bool = True) -&gt; DType | None</code>","text":"<pre><code>parse(\n    input_type: FieldType,\n    constraints: FieldConstraints,\n    metadata: FieldMetadata,\n    *,\n    strict: Literal[True] = True\n) -&gt; DType\n</code></pre><pre><code>parse(\n    input_type: FieldType,\n    constraints: FieldConstraints,\n    metadata: FieldMetadata,\n    *,\n    strict: Literal[False]\n) -&gt; DType | None\n</code></pre> <p>Try each parser in sequence until one succeeds.</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>FieldType</code> <p>The type to parse.</p> required <code>constraints</code> <code>FieldConstraints</code> <p>Constraints associated with the type.</p> required <code>metadata</code> <code>FieldMetadata</code> <p>Custom metadata dictionary.</p> required <code>strict</code> <code>bool</code> <p>Whether or not to raise if unable to parse <code>input_type</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>DType | None</code> <p>A Narwhals DType from the first successful parser, or None if no parser succeeded and <code>strict=False</code>.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.ParserPipeline.parse_into_field","title":"<code>parse_into_field(name: str, input_type: FieldType, constraints: FieldConstraints, metadata: FieldMetadata) -&gt; AnyField</code>","text":"<p>Parse a field specification into a AnyField object.</p> <p>This is the recommended method for parsing field specifications at the top level. It wraps the DType parsing with additional field-level information like nullability, uniqueness, and custom metadata.</p> <p>The metadata dictionary is populated during parsing (e.g., <code>UnionTypeStep</code> sets <code>{\"anyschema\": {\"nullable\": True}}</code> for <code>Optional[T]</code> types), ensuring that forward references are properly evaluated and avoiding code duplication.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the field.</p> required <code>input_type</code> <code>FieldType</code> <p>The type to parse.</p> required <code>constraints</code> <code>FieldConstraints</code> <p>Constraints associated with the type.</p> required <code>metadata</code> <code>FieldMetadata</code> <p>Custom metadata dictionary. This dictionary may be modified during parsing to add field-level metadata like <code>{\"anyschema\": {\"nullable\": True}}</code>.</p> required <p>Returns:</p> Type Description <code>AnyField</code> <p>A <code>AnyField</code> instance containing the parsed dtype and field-level metadata.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema.parsers import make_pipeline\n&gt;&gt;&gt; pipeline = make_pipeline()\n&gt;&gt;&gt; field = pipeline.parse_into_field(\"age\", int, (), {})\n&gt;&gt;&gt; field\nAnyField(name='age', dtype=Int64, nullable=False, unique=False, description=None, metadata={})\n</code></pre> <p>With nullable=True metadata:</p> <pre><code>&gt;&gt;&gt; field = pipeline.parse_into_field(\"email\", str, (), {\"anyschema\": {\"nullable\": True}})\n&gt;&gt;&gt; field.nullable\nTrue\n</code></pre> <p>With Optional type (auto-detected as nullable):</p> <pre><code>&gt;&gt;&gt; from typing import Optional\n&gt;&gt;&gt; field = pipeline.parse_into_field(\"email\", Optional[str], (), {})\n&gt;&gt;&gt; field.nullable\nTrue\n</code></pre>"},{"location":"api-reference/parsers/#anyschema.parsers.ParserPipeline.with_steps","title":"<code>with_steps(steps: ParserStep | Sequence[ParserStep], *more_steps: ParserStep, at_position: int | Literal['auto'] = 'auto') -&gt; Self</code>","text":"<p>Create a new pipeline with additional parser step(s) inserted at the specified position.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>ParserStep | Sequence[ParserStep]</code> <p><code>ParserStep</code>(s) to add to the pipeline.</p> required <code>*more_steps</code> <code>ParserStep</code> <p>Additional <code>ParserStep</code>(s) to add, specified as positional arguments.</p> <code>()</code> <code>at_position</code> <code>int | Literal['auto']</code> <p>Position where to insert the step(s). Options:</p> <ul> <li>An integer index (can be negative for counting from the end).</li> <li><code>\"auto\"</code> (default): Automatically determines the \"best\" position.     The step(s) will be inserted after the last preprocessing step found (trying <code>AnnotatedStep</code>,     <code>UnionTypeStep</code>, <code>ForwardRefStep</code> in that order), ensuring custom parsers run after type     preprocessing but before library-specific or fallback parsers.     If no preprocessing steps are found, inserts at the beginning.</li> </ul> <code>'auto'</code> <p>Returns:</p> Type Description <code>Self</code> <p>A new <code>ParserPipeline</code> instance with the step(s) added at the specified position.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema.parsers import ParserPipeline, ParserStep\n&gt;&gt;&gt; import narwhals as nw\n&gt;&gt;&gt; from anyschema.typing import FieldConstraints, FieldMetadata, FieldType\n&gt;&gt;&gt;\n&gt;&gt;&gt; class CustomType: ...\n&gt;&gt;&gt;\n&gt;&gt;&gt; class CustomParserStep(ParserStep):\n...     def parse(\n...         self, input_type: FieldType, constraints: FieldConstraints, metadata: FieldMetadata\n...     ) -&gt; nw.DType | None:\n...         if input_type is CustomType:\n...             return nw.String()\n...         return None\n&gt;&gt;&gt;\n&gt;&gt;&gt; pipeline = ParserPipeline(\"auto\")  # Start with auto pipeline\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Add single custom step\n&gt;&gt;&gt; custom_pipeline = pipeline.with_steps(CustomParserStep())\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Add multiple custom steps at once\n&gt;&gt;&gt; custom_pipeline = pipeline.with_steps([CustomParserStep(), CustomParserStep()])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Or add at specific position\n&gt;&gt;&gt; custom_pipeline = pipeline.with_steps(CustomParserStep(), at_position=0)\n&gt;&gt;&gt;\n&gt;&gt;&gt; custom_pipeline.parse(CustomType, constraints=(), metadata={})\nString\n</code></pre>"},{"location":"api-reference/parsers/#anyschema.parsers.make_pipeline","title":"<code>anyschema.parsers.make_pipeline(steps: IntoParserPipeline = 'auto') -&gt; ParserPipeline</code>","text":"<p>Create a <code>ParserPipeline</code> with the specified steps.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>IntoParserPipeline</code> <p>Control how parser steps are configured:</p> <ul> <li><code>\"auto\"</code> (default): Automatically select appropriate parser steps based on installed dependencies.</li> <li>A <code>ParserPipeline</code> instance: Return the pipeline as-is.</li> <li>A sequence of <code>ParserStep</code> instances: Create pipeline with these steps.</li> </ul> <code>'auto'</code> <p>Returns:</p> Type Description <code>ParserPipeline</code> <p>A <code>ParserPipeline</code> instance with the configured steps.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema.parsers import make_pipeline\n&gt;&gt;&gt; from anyschema.parsers import PyTypeStep, UnionTypeStep, AnnotatedStep\n&gt;&gt;&gt;\n&gt;&gt;&gt; pipeline = make_pipeline(steps=[UnionTypeStep(), AnnotatedStep(), PyTypeStep()])\n&gt;&gt;&gt; print(pipeline.steps)\n(UnionTypeStep, AnnotatedStep, PyTypeStep)\n</code></pre> <pre><code>&gt;&gt;&gt; pipeline = make_pipeline(steps=\"auto\")\n&gt;&gt;&gt; print(pipeline.steps)\n(ForwardRefStep, UnionTypeStep, ..., PydanticTypeStep, SQLAlchemyTypeStep, PyTypeStep)\n</code></pre> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the steps are not a sequence of <code>ParserStep</code> instances or a <code>ParserPipeline</code>.</p>"},{"location":"api-reference/parsers/#parser-steps","title":"Parser Steps","text":"<p>Parser steps are the building blocks of the type parsing pipeline. Each step handles specific type patterns.</p> <p>For more details on how these work together, see the parser steps section in the Architecture guide.</p> <p>The following steps are built-in and come dependency-free.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.ParserStep","title":"<code>anyschema.parsers.ParserStep</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for parser steps that convert type annotations to Narwhals dtypes.</p> <p>This class provides a framework for parsing different types of type annotations and converting them into appropriate Narwhals data types. Each concrete parser implementation handles specific type patterns or annotation styles.</p> <p>Attributes:</p> Name Type Description <code>pipeline</code> <code>ParserPipeline</code> <p>Property to access the <code>ParserPipeline</code>, raises <code>UnavailablePipelineError</code> if not set.</p> <p>Raises:</p> Type Description <code>UnavailablePipelineError</code> <p>When accessing pipeline before it's been set.</p> <code>TypeError</code> <p>When setting pipeline with an object that's not a ParserPipeline instance.</p> Note <p>Subclasses must implement the <code>parse</code> method to define their specific parsing logic.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from typing import get_origin, get_args\n&gt;&gt;&gt; import narwhals as nw\n&gt;&gt;&gt; from anyschema.parsers import ParserPipeline, ParserStep, PyTypeStep\n&gt;&gt;&gt; from anyschema.typing import FieldConstraints, FieldMetadata, FieldType\n&gt;&gt;&gt;\n&gt;&gt;&gt; class CustomType: ...\n&gt;&gt;&gt; class CustomList[T]: ...\n&gt;&gt;&gt;\n&gt;&gt;&gt; class CustomParserStep(ParserStep):\n...     def parse(\n...         self, input_type: FieldType, constraints: FieldConstraints, metadata: FieldMetadata\n...     ) -&gt; DType | None:\n...         if input_type is CustomType:\n...             return nw.String()\n...\n...         if get_origin(input_type) is CustomList:\n...             inner = get_args(input_type)[0]\n...             # Delegate to pipeline for recursion\n...             inner_dtype = self.pipeline.parse(inner, constraints=constraints, metadata=metadata)\n...             return nw.List(inner_dtype)\n...\n...         # Return None if we can't handle it\n...         return None\n&gt;&gt;&gt;\n&gt;&gt;&gt; pipeline = ParserPipeline(steps=[CustomParserStep(), PyTypeStep()])\n&gt;&gt;&gt; pipeline.parse(CustomType, constraints=(), metadata={})\nString\n&gt;&gt;&gt; pipeline.parse(CustomList[int], constraints=(), metadata={})\nList(Int64)\n&gt;&gt;&gt; pipeline.parse(CustomList[str], constraints=(), metadata={})\nList(String)\n</code></pre>"},{"location":"api-reference/parsers/#anyschema.parsers.ParserStep.pipeline","title":"<code>pipeline: ParserPipeline</code>  <code>property</code> <code>writable</code>","text":"<p>Property that returns the parser chain instance.</p> <p>Returns:</p> Name Type Description <code>ParserPipeline</code> <code>ParserPipeline</code> <p>The parser chain object used for parsing operations.</p> <p>Raises:</p> Type Description <code>UnavailablePipelineError</code> <p>If the parser chain has not been initialized (i.e., <code>_pipeline</code> is None).</p>"},{"location":"api-reference/parsers/#anyschema.parsers.ParserStep.clone","title":"<code>clone() -&gt; Self</code>","text":"<p>Create a clone of this parser step without a pipeline reference.</p> <p>This method creates a shallow copy of the step, preserving any internal state (such as configuration parameters) but resetting the pipeline reference to None. This allows the cloned step to be added to a different pipeline without violating the \"pipeline can only be set once\" constraint.</p> <p>Returns:</p> Type Description <code>Self</code> <p>A new instance of the same parser step class with the same configuration</p> <code>Self</code> <p>but without a pipeline reference.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from anyschema.parsers import ParserPipeline, PyTypeStep\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a step and assign it to a pipeline\n&gt;&gt;&gt; step = PyTypeStep()\n&gt;&gt;&gt; pipeline1 = ParserPipeline([step])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Clone the step to use in another pipeline\n&gt;&gt;&gt; cloned_step = step.clone()\n&gt;&gt;&gt; pipeline2 = ParserPipeline([cloned_step])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Both pipelines work independently\n&gt;&gt;&gt; pipeline1.parse(int, (), {})\nInt64\n&gt;&gt;&gt; pipeline2.parse(int, (), {})\nInt64\n</code></pre>"},{"location":"api-reference/parsers/#anyschema.parsers.ParserStep.parse","title":"<code>parse(input_type: FieldType, constraints: FieldConstraints, metadata: FieldMetadata) -&gt; DType | None</code>  <code>abstractmethod</code>","text":"<p>Parse a type annotation into a Narwhals dtype.</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>FieldType</code> <p>The type to parse (e.g., int, str, list[int], etc.)</p> required <code>constraints</code> <code>FieldConstraints</code> <p>Constraints associated with the type (e.g., Gt, Le from annotated-types)</p> required <code>metadata</code> <code>FieldMetadata</code> <p>Custom metadata dictionary</p> required <p>Returns:</p> Type Description <code>DType | None</code> <p>A Narwhals DType if the parser can handle this type, None otherwise.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.ForwardRefStep","title":"<code>anyschema.parsers.ForwardRefStep</code>","text":"<p>               Bases: <code>ParserStep</code></p> <p>Parser for ForwardRef types (string annotations and forward references).</p> <p>This parser handles type annotations that are forward references (ForwardRef), which occur when using string annotations or referencing types before they're defined.</p> <p>The parser resolves the ForwardRef to the actual type and delegates to the parser chain.</p> <p>Initialization can be customized by providing a global and local namespace that will be used to evaluate the forward references when resolving the type. The default namespace is built with common types, yet you can override it with your own types.</p> <p>Parameters:</p> Name Type Description Default <code>globalns</code> <code>dict[str, Any] | None</code> <p>Global namespace for evaluating forward references. Defaults to a namespace with common types.</p> <code>None</code> <code>localns</code> <code>Mapping[str, Any] | None</code> <p>Local namespace for evaluating forward references. Defaults to an empty namespace.</p> <code>None</code>"},{"location":"api-reference/parsers/#anyschema.parsers.ForwardRefStep.parse","title":"<code>parse(input_type: FieldType, constraints: FieldConstraints, metadata: FieldMetadata) -&gt; DType | None</code>","text":"<p>Parse ForwardRef types by resolving them and delegating to the chain.</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>FieldType</code> <p>The type to parse (may be a ForwardRef).</p> required <code>constraints</code> <code>FieldConstraints</code> <p>Constraints associated with the type.</p> required <code>metadata</code> <code>FieldMetadata</code> <p>Custom metadata dictionary.</p> required <p>Returns:</p> Type Description <code>DType | None</code> <p>A Narwhals DType if this is a ForwardRef that can be resolved, None otherwise.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.UnionTypeStep","title":"<code>anyschema.parsers.UnionTypeStep</code>","text":"<p>               Bases: <code>ParserStep</code></p> <p>Parser for Union types including <code>Optional</code>.</p> <p>Handles:</p> <ul> <li><code>Union[T, None]</code>, <code>T | None</code>, <code>Optional[T]</code></li> <li>Extracts the non-None type and its metadata for further parsing</li> </ul>"},{"location":"api-reference/parsers/#anyschema.parsers.UnionTypeStep.parse","title":"<code>parse(input_type: FieldType, constraints: FieldConstraints, metadata: FieldMetadata) -&gt; DType | None</code>","text":"<p>Parse Union types, particularly Optional types.</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>FieldType</code> <p>The type to parse.</p> required <code>constraints</code> <code>FieldConstraints</code> <p>Constraints associated with the type (will be preserved and passed through).</p> required <code>metadata</code> <code>FieldMetadata</code> <p>Custom metadata dictionary (will be preserved and passed through).</p> required <p>Returns:</p> Type Description <code>DType | None</code> <p>A Narwhals DType by extracting the non-None type and delegating to the chain.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.AnnotatedStep","title":"<code>anyschema.parsers.AnnotatedStep</code>","text":"<p>               Bases: <code>ParserStep</code></p> <p>Parser for <code>typing.Annotated</code> types.</p> <p>Handles:</p> <ul> <li><code>Annotated[T, metadata...]</code> - extracts the type and metadata for further parsing</li> </ul>"},{"location":"api-reference/parsers/#anyschema.parsers.AnnotatedStep.parse","title":"<code>parse(input_type: FieldType, constraints: FieldConstraints, metadata: FieldMetadata) -&gt; DType | None</code>","text":"<p>Parse Annotated types by extracting the base type and constraints.</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>FieldType</code> <p>The type to parse.</p> required <code>constraints</code> <code>FieldConstraints</code> <p>Constraints associated with the type.</p> required <code>metadata</code> <code>FieldMetadata</code> <p>Custom metadata dictionary.</p> required <p>Returns:</p> Type Description <code>DType | None</code> <p>A Narwhals DType by extracting the base type and delegating to the chain.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.PyTypeStep","title":"<code>anyschema.parsers.PyTypeStep</code>","text":"<p>               Bases: <code>ParserStep</code></p> <p>Parser for Python builtin types.</p> <p>Handles:</p> <ul> <li><code>int</code>, <code>float</code>, <code>decimal</code>, <code>str</code>, <code>bytes</code>, <code>bool</code>, <code>date</code>, <code>datetime</code>, <code>timedelta</code>, <code>time</code>, <code>object</code>, <code>Enum</code>,     <code>Literal</code></li> <li>generics: <code>list[T]</code>, <code>Sequence[T]</code>, <code>Iterable[T]</code>, <code>tuple[T, ...]</code></li> <li><code>dict</code>,<code>Mapping[K, V]</code>, and typed dictionaries (<code>TypedDict</code>)</li> </ul>"},{"location":"api-reference/parsers/#anyschema.parsers.PyTypeStep.parse","title":"<code>parse(input_type: FieldType, constraints: FieldConstraints, metadata: FieldMetadata) -&gt; DType | None</code>","text":"<p>Parse Python type annotations into Narwhals dtypes.</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>FieldType</code> <p>The type to parse.</p> required <code>constraints</code> <code>FieldConstraints</code> <p>Constraints associated with the type.</p> required <code>metadata</code> <code>FieldMetadata</code> <p>Custom metadata dictionary.</p> required <p>Returns:</p> Type Description <code>DType | None</code> <p>A Narwhals DType if this parser can handle the type, None otherwise.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.annotated_types.AnnotatedTypesStep","title":"<code>anyschema.parsers.annotated_types.AnnotatedTypesStep</code>","text":"<p>               Bases: <code>ParserStep</code></p> <p>Parser for types with <code>annotated_types</code> metadata.</p> <p>Handles:</p> <ul> <li>Integer constraints (<code>Gt</code>, <code>Ge</code>, <code>Lt</code>, <code>Le</code>, <code>Interval</code>)</li> <li>Type refinement based on metadata</li> </ul> Warning <p>It requires annotated-types to be installed.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.annotated_types.AnnotatedTypesStep.parse","title":"<code>parse(input_type: FieldType, constraints: FieldConstraints, metadata: FieldMetadata) -&gt; DType | None</code>","text":"<p>Parse types with annotated constraints into refined Narwhals dtypes.</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>FieldType</code> <p>The type to parse.</p> required <code>constraints</code> <code>FieldConstraints</code> <p>Constraints associated with the type (e.g., Gt, Ge, Lt, Le).</p> required <code>metadata</code> <code>FieldMetadata</code> <p>Custom metadata dictionary (not used in this parser).</p> required <p>Returns:</p> Type Description <code>DType | None</code> <p>A Narwhals DType if this parser can refine the type based on constraints, None otherwise.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.attrs.AttrsTypeStep","title":"<code>anyschema.parsers.attrs.AttrsTypeStep</code>","text":"<p>               Bases: <code>ParserStep</code></p> <p>Parser for attrs-specific types.</p> <p>Handles:</p> <ul> <li>attrs classes (Struct types)</li> </ul> Warning <p>It requires attrs to be installed.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.attrs.AttrsTypeStep.parse","title":"<code>parse(input_type: FieldType, constraints: FieldConstraints, metadata: FieldMetadata) -&gt; DType | None</code>","text":"<p>Parse attrs-specific types into Narwhals dtypes.</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>FieldType</code> <p>The type to parse.</p> required <code>constraints</code> <code>FieldConstraints</code> <p>Constraints associated with the type.</p> required <code>metadata</code> <code>FieldMetadata</code> <p>Custom metadata dictionary.</p> required <p>Returns:</p> Type Description <code>DType | None</code> <p>A Narwhals DType if this parser can handle the type, None otherwise.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.pydantic.PydanticTypeStep","title":"<code>anyschema.parsers.pydantic.PydanticTypeStep</code>","text":"<p>               Bases: <code>ParserStep</code></p> <p>Parser for Pydantic-specific types.</p> <p>Handles:</p> <ul> <li>Pydantic datetime types (<code>AwareDatetime</code>, <code>NaiveDatetime</code>, etc.)</li> <li>Pydantic date types (<code>PastDate</code>, <code>FutureDate</code>)</li> <li>Pydantic <code>BaseModel</code> (Struct types)</li> </ul> Warning <p>It requires pydantic to be installed.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.pydantic.PydanticTypeStep.parse","title":"<code>parse(input_type: FieldType, constraints: FieldConstraints, metadata: FieldMetadata) -&gt; DType | None</code>","text":"<p>Parse Pydantic-specific types into Narwhals dtypes.</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>FieldType</code> <p>The type to parse.</p> required <code>constraints</code> <code>FieldConstraints</code> <p>Constraints associated with the type.</p> required <code>metadata</code> <code>FieldMetadata</code> <p>Custom metadata dictionary.</p> required <p>Returns:</p> Type Description <code>DType | None</code> <p>A Narwhals DType if this parser can handle the type, None otherwise.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.sqlalchemy.SQLAlchemyTypeStep","title":"<code>anyschema.parsers.sqlalchemy.SQLAlchemyTypeStep</code>","text":"<p>               Bases: <code>ParserStep</code></p> <p>Parser for SQLAlchemy-specific types.</p> <p>Handles:</p> <ul> <li>SQLAlchemy column types (Integer, String, DateTime, etc.)</li> <li>SQLAlchemy relationship types</li> <li>SQLAlchemy custom types</li> </ul> Warning <p>It requires sqlalchemy to be installed.</p>"},{"location":"api-reference/parsers/#anyschema.parsers.sqlalchemy.SQLAlchemyTypeStep.parse","title":"<code>parse(input_type: FieldType, constraints: FieldConstraints, metadata: FieldMetadata) -&gt; DType | None</code>","text":"<p>Parse SQLAlchemy-specific types into Narwhals dtypes.</p> <p>Parameters:</p> Name Type Description Default <code>input_type</code> <code>FieldType</code> <p>The type to parse.</p> required <code>constraints</code> <code>FieldConstraints</code> <p>Constraints associated with the type.</p> required <code>metadata</code> <code>FieldMetadata</code> <p>Custom metadata dictionary.</p> required <p>Returns:</p> Type Description <code>DType | None</code> <p>A Narwhals DType if this parser can handle the type, None otherwise.</p>"},{"location":"api-reference/serde/","title":"Serialization &amp; Deserialization","text":"<p>The <code>serde</code> module provides utilities for serializing and deserializing Narwhals dtypes to and from string representations.</p>"},{"location":"api-reference/serde/#api-reference","title":"API Reference","text":""},{"location":"api-reference/serde/#anyschema.serde","title":"<code>anyschema.serde</code>","text":""},{"location":"api-reference/serde/#anyschema.serde.deserialize_dtype","title":"<code>deserialize_dtype(into_dtype: str) -&gt; DType</code>","text":"<p>Deserialize a string representation of a Narwhals dtype back to the dtype object.</p> <p>Handles both simple and complex nested types using regex and recursion.</p> <p>Parameters:</p> Name Type Description Default <code>into_dtype</code> <code>str</code> <p>String representation of the dtype (e.g., \"Int64\", \"List(String)\", \"Struct({'a': Int64, 'b': List(String)})\")</p> required <p>Returns:</p> Type Description <code>DType</code> <p>The corresponding Narwhals DType object</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; deserialize_dtype(\"Int64\")\nInt64\n&gt;&gt;&gt; deserialize_dtype(\"List(String)\")\nList(String)\n&gt;&gt;&gt; deserialize_dtype(\"Datetime(time_unit='ms', time_zone='UTC')\")\nDatetime(time_unit='ms', time_zone='UTC')\n</code></pre> Source code in <code>anyschema/serde.py</code> <pre><code>def deserialize_dtype(into_dtype: str) -&gt; DType:\n    \"\"\"Deserialize a string representation of a Narwhals dtype back to the dtype object.\n\n    Handles both simple and complex nested types using regex and recursion.\n\n    Arguments:\n        into_dtype: String representation of the dtype (e.g., \"Int64\", \"List(String)\",\n            \"Struct({'a': Int64, 'b': List(String)})\")\n\n    Returns:\n        The corresponding Narwhals DType object\n\n    Examples:\n        &gt;&gt;&gt; deserialize_dtype(\"Int64\")\n        Int64\n        &gt;&gt;&gt; deserialize_dtype(\"List(String)\")\n        List(String)\n        &gt;&gt;&gt; deserialize_dtype(\"Datetime(time_unit='ms', time_zone='UTC')\")\n        Datetime(time_unit='ms', time_zone='UTC')\n    \"\"\"\n    if (dtype := NON_COMPLEX_MAPPING.get(into_dtype)) is not None:\n        return dtype\n\n    if datetime_match := RGX_DATETIME.match(into_dtype):\n        time_unit = cast(\"TimeUnit\", datetime_match.group(\"time_unit\"))\n        time_zone = datetime_match.group(\"time_zone\")\n        return Datetime(time_unit=time_unit, time_zone=time_zone)\n\n    if duration_match := RGX_DURATION.match(into_dtype):\n        time_unit = cast(\"TimeUnit\", duration_match.group(\"time_unit\"))\n        return Duration(time_unit=time_unit)\n\n    if enum_match := RGX_ENUM.match(into_dtype):\n        categories = ast.literal_eval(enum_match.group(\"categories\"))\n        return Enum(categories=categories)\n\n    if list_match := RGX_LIST.match(into_dtype):\n        inner_type = deserialize_dtype(list_match.group(\"inner_type\"))\n        return List(inner_type)\n\n    if array_match := RGX_ARRAY.match(into_dtype):\n        inner_type = deserialize_dtype(array_match.group(\"inner_type\"))\n        shape = ast.literal_eval(array_match.group(\"shape\"))\n        return Array(inner_type, shape=shape)\n\n    if struct_match := RGX_STRUCT.match(into_dtype):\n        fields = _parse_struct_fields(struct_match.group(\"fields\"))\n        return Struct(fields)\n\n    msg = f\"Unable to deserialize '{into_dtype}' into a Narwhals DType\"\n    raise UnsupportedDTypeError(msg)\n</code></pre>"},{"location":"api-reference/serde/#anyschema.serde.serialize_dtype","title":"<code>serialize_dtype(dtype: DType) -&gt; str</code>","text":"<p>Serialize a Narwhals dtype to its string representation.</p> <p>Converts a Narwhals dtype object into a string that can be stored or transmitted and later reconstructed using <code>deserialize_dtype</code>. The serialization is based on the dtype's string representation.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>DType</code> <p>A Narwhals DType object to serialize</p> required <p>Returns:</p> Type Description <code>str</code> <p>String representation of the dtype (e.g., \"Int64\", \"List(String)\", \"Struct({'a': Int64, 'b': String})\")</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; serialize_dtype(Int64())\n'Int64'\n&gt;&gt;&gt; serialize_dtype(List(String()))\n'List(String)'\n&gt;&gt;&gt; serialize_dtype(Datetime(time_unit=\"ms\", time_zone=\"UTC\"))\n\"Datetime(time_unit='ms', time_zone='UTC')\"\n&gt;&gt;&gt; serialize_dtype(Struct({\"a\": Int64(), \"b\": String()}))\n\"Struct({'a': Int64, 'b': String})\"\n</code></pre> Source code in <code>anyschema/serde.py</code> <pre><code>def serialize_dtype(dtype: DType) -&gt; str:\n    \"\"\"Serialize a Narwhals dtype to its string representation.\n\n    Converts a Narwhals dtype object into a string that can be stored or transmitted\n    and later reconstructed using `deserialize_dtype`. The serialization is based on\n    the dtype's string representation.\n\n    Arguments:\n        dtype: A Narwhals DType object to serialize\n\n    Returns:\n        String representation of the dtype (e.g., \"Int64\", \"List(String)\", \"Struct({'a': Int64, 'b': String})\")\n\n    Examples:\n        &gt;&gt;&gt; serialize_dtype(Int64())\n        'Int64'\n        &gt;&gt;&gt; serialize_dtype(List(String()))\n        'List(String)'\n        &gt;&gt;&gt; serialize_dtype(Datetime(time_unit=\"ms\", time_zone=\"UTC\"))\n        \"Datetime(time_unit='ms', time_zone='UTC')\"\n        &gt;&gt;&gt; serialize_dtype(Struct({\"a\": Int64(), \"b\": String()}))\n        \"Struct({'a': Int64, 'b': String})\"\n    \"\"\"\n    return str(dtype)\n</code></pre>"},{"location":"api-reference/typing/","title":"Type Aliases","text":"<p>The following type aliases are used throughout the anyschema codebase:</p>"},{"location":"api-reference/typing/#anyschema.typing","title":"<code>anyschema.typing</code>","text":""},{"location":"api-reference/typing/#anyschema.typing.Adapter","title":"<code>Adapter: TypeAlias = Callable[[Any], FieldSpecIterable]</code>  <code>module-attribute</code>","text":"<p>Adapter expected signature.</p> <p>An adapter is a callable that adapts a spec into field specifications.</p>"},{"location":"api-reference/typing/#anyschema.typing.FieldSpec","title":"<code>FieldSpec: TypeAlias = tuple[FieldName, FieldType, FieldConstraints, FieldMetadata]</code>  <code>module-attribute</code>","text":"<p>Field specification: alias for a tuple of <code>(str, type, tuple(constraints, ...), dict(metadata))</code>.</p>"},{"location":"api-reference/typing/#anyschema.typing.FieldSpecIterable","title":"<code>FieldSpecIterable: TypeAlias = Generator[FieldSpec, None, None]</code>  <code>module-attribute</code>","text":"<p>Return type of an adapter.</p>"},{"location":"api-reference/typing/#anyschema.typing.IntoOrderedDict","title":"<code>IntoOrderedDict: TypeAlias = Mapping[str, type] | Sequence[tuple[str, type]]</code>  <code>module-attribute</code>","text":"<p>An object that can be converted into a python <code>OrderedDict</code>.</p> <p>We check for the object to be either a mapping or a sequence of sized 2 tuples.</p>"},{"location":"api-reference/typing/#anyschema.typing.IntoParserPipeline","title":"<code>IntoParserPipeline: TypeAlias = \"Literal['auto'] | Sequence['ParserStep']\"</code>  <code>module-attribute</code>","text":"<p>An object that can be converted into a <code>ParserPipeline</code>.</p> <p>Either \"auto\" or a sequence of <code>ParserStep</code>.</p>"},{"location":"api-reference/typing/#anyschema.typing.Spec","title":"<code>Spec: TypeAlias = 'Schema | IntoOrderedDict | type[BaseModel] | DataclassType | TypedDictType | AttrsClassType | SQLAlchemyTableType | UnknownSpec'</code>  <code>module-attribute</code>","text":"<p>Input specification supported directly by <code>AnySchema</code>.</p>"},{"location":"user-guide/advanced/","title":"Advanced Usage","text":"<p>This guide covers advanced topics including custom parser steps, custom spec adapters, and extending anyschema for your specific use cases.</p> <p>It might be useful to review the Architecture and have gone through the Getting Started guide before diving into advanced topics.</p>"},{"location":"user-guide/advanced/#custom-parser-steps","title":"Custom Parser Steps","text":"<p>Creating custom parser steps allows you to add support for new type systems or handle special types in your own way. Parser steps should inherit from the ParserStep base class and implement the <code>parse</code> method with the following signature:</p> <pre><code>import narwhals as nw\nfrom anyschema.typing import FieldConstraints, FieldMetadata, FieldType\n\n\ndef parse(\n    self,\n    input_type: FieldType,\n    constraints: FieldConstraints,\n    metadata: FieldMetadata,\n) -&gt; nw.dtypes.DType | None:\n    ...\n</code></pre>"},{"location":"user-guide/advanced/#basic-custom-parser","title":"Basic Custom Parser","text":"<p>Here's a simple custom parser for a hypothetical custom type:</p> <pre><code>import narwhals as nw\nfrom anyschema.parsers import ParserPipeline, ParserStep, PyTypeStep\nfrom anyschema.typing import FieldConstraints, FieldMetadata, FieldType\n\n\nclass Color:\n    \"\"\"A custom type representing a color.\"\"\"\n\n    pass\n\n\nclass ColorStep(ParserStep):\n    \"\"\"Parser for Color types.\"\"\"\n\n    def parse(\n        self,\n        input_type: FieldType,\n        constraints: FieldConstraints,\n        metadata: FieldMetadata,\n    ) -&gt; nw.dtypes.DType | None:\n        \"\"\"Parse Color to String dtype.\n\n        Arguments:\n            input_type: The type to parse.\n            constraints: Constraints associated with the type.\n            metadata: Custom metadata dictionary.\n\n        Returns:\n            String dtype for Color types, None otherwise.\n        \"\"\"\n        if input_type is Color:\n            return nw.String()\n        return None\n\n\n# Create a simple pipeline with the custom parser\ncolor_step = ColorStep()\npython_step = PyTypeStep()\npipeline = ParserPipeline(steps=[color_step, python_step])\n\nresult = pipeline.parse(Color, constraints=(), metadata={})\nprint(result)\n</code></pre> <pre><code>String\n</code></pre>"},{"location":"user-guide/advanced/#custom-parser-with-nested-types","title":"Custom Parser with Nested Types","text":"<p>This example shows how to handle a custom generic type. Note how we use <code>self.pipeline.parse(..., constraints=constraints, metadata=metadata)</code> for recursion, as explained in the Architecture page:</p> <pre><code>from typing import Any, TypeVar, get_args, get_origin\n\nimport narwhals as nw\nfrom anyschema.parsers import ParserStep\n\nfrom anyschema.typing import FieldConstraints, FieldMetadata, FieldType\n\nT = TypeVar(\"T\")\n\nclass MyList[T]:\n    \"\"\"A custom list-like type.\"\"\"\n\n    pass\n\n\nclass MyListStep(ParserStep):\n    \"\"\"Parser for MyList[T] generic types.\"\"\"\n\n    def parse(\n        self,\n        input_type: FieldType,\n        constraints: FieldConstraints,\n        metadata: FieldMetadata,\n    ) -&gt; nw.dtypes.DType | None:\n        \"\"\"Parse MyList[T] to List dtype.\n\n        This parser handles custom generic types by recursively parsing\n        the inner type through the pipeline.\n        \"\"\"\n        origin = get_origin(input_type)\n\n        if origin is MyList:\n            # Get the inner type (e.g., T in MyList[T])\n            args = get_args(input_type)\n            if args:\n                inner_type = args[0]\n                # Recursively parse the inner type\n                inner_dtype = self.pipeline.parse(inner_type, constraints=constraints, metadata=metadata)\n                return nw.List(inner_dtype)\n            else:\n                # MyList without type parameter\n                return nw.List(nw.Object())\n\n        return None\n\nmy_list_step = MyListStep()\npython_step = PyTypeStep()\npipeline = ParserPipeline(steps=[my_list_step, python_step])\nresult = pipeline.parse(MyList[int], (), {})\nprint(result)\n</code></pre> <pre><code>List(Int64)\n</code></pre>"},{"location":"user-guide/advanced/#custom-parser-with-metadata-handling","title":"Custom Parser with Metadata Handling","text":"<p>This example shows how to use metadata to refine type parsing. For more on metadata flow, see the Architecture section:</p> <pre><code>from typing import Any, Annotated\n\nimport narwhals as nw\nfrom anyschema.parsers import AnnotatedStep, ParserStep, PyTypeStep\nfrom anyschema.typing import FieldConstraints, FieldMetadata, FieldType\n\n\nclass SmallInt:\n    \"\"\"Marker for small integers.\"\"\"\n\n    pass\n\n\nclass BigInt:\n    \"\"\"Marker for big integers.\"\"\"\n\n    pass\n\n\nclass CustomConstraintStep(ParserStep):\n    \"\"\"Parser that uses constraints to choose integer size.\"\"\"\n\n    def parse(\n        self,\n        input_type: FieldType,\n        constraints: FieldConstraints,\n        metadata: FieldMetadata,\n    ) -&gt; nw.dtypes.DType | None:\n        \"\"\"Parse integers with size constraints.\n\n        Uses constraints to determine whether to use Int32 or Int64.\n        \"\"\"\n        if input_type is int and constraints:\n            for constraint in constraints:\n                if constraint is SmallInt:\n                    return nw.Int32()\n                elif constraint is BigInt:\n                    return nw.Int64()\n\n        return None\n\n\n# Usage with typing.Annotated\nSmallInteger = Annotated[int, SmallInt]\nBigInteger = Annotated[int, BigInt]\n\n# Create a pipeline with the custom parser\nannotated_step = AnnotatedStep()\ncustom_constraint_step = CustomConstraintStep()\npython_step = PyTypeStep()\npipeline = ParserPipeline(steps=[annotated_step, custom_constraint_step, python_step])\n\nprint(f\"SmallInteger dtype: {pipeline.parse(SmallInteger, (), {})}\")\nprint(f\"BigInteger dtype: {pipeline.parse(BigInteger, (), {})}\")\n</code></pre> <pre><code>SmallInteger dtype: Int32\nBigInteger dtype: Int64\n</code></pre>"},{"location":"user-guide/advanced/#combining-multiple-custom-parsers","title":"Combining Multiple Custom Parsers","text":"<p>Here's how to combine multiple custom parsers using the <code>with_steps</code> method for easy pipeline extension:</p> <pre><code>from anyschema import AnySchema\nfrom anyschema.parsers import ParserPipeline\n\nbase_pipeline = ParserPipeline(\"auto\")  # Start with the auto pipeline\n\n# Add custom parsers using with_steps (automatically positions them optimally)\ncustom_pipeline = base_pipeline.with_steps(ColorStep(), MyListStep())\n\n# Use the custom pipeline\nschema = AnySchema(\n    spec={\"color\": Color, \"items\": MyList[int]},\n    pipeline=custom_pipeline,\n)\nprint(schema.to_arrow())\n</code></pre> <pre><code>color: string not null\nitems: list&lt;item: int64&gt; not null\n  child 0, item: int64\n</code></pre> <p>The <code>with_steps</code> method makes it easy to extend existing pipelines without reconstructing them from scratch. By default, it inserts custom steps right after the last preprocessing step found (trying <code>AnnotatedStep</code>, <code>UnionTypeStep</code>, <code>ForwardRefStep</code> in that order), ensuring they run after type preprocessing but before library-specific parsers.</p> <p>You can also specify a position explicitly:</p> <pre><code>pipeline_at_start = base_pipeline.with_steps(ColorStep(), at_position=0)\npipeline_at_end = base_pipeline.with_steps(ColorStep(), at_position=-1)\n\nprint(pipeline_at_start.steps)\nprint(pipeline_at_end.steps)\n</code></pre> <pre><code>(ColorStep, ForwardRefStep, UnionTypeStep, AnnotatedStep, AnnotatedTypesStep, AttrsTypeStep, PydanticTypeStep, SQLAlchemyTypeStep, PyTypeStep)\n(ForwardRefStep, UnionTypeStep, AnnotatedStep, AnnotatedTypesStep, AttrsTypeStep, PydanticTypeStep, SQLAlchemyTypeStep, ColorStep, PyTypeStep)\n</code></pre> <p>Why use with_steps?</p> <p>As the list of default steps grows, it becomes less practical to redefine a list of step just to add one or few custom parsing steps. With <code>pipeline.with_steps</code>, you can simply extend an existing pipeline:</p> <pre><code>custom_pipeline = ParserPipeline(\"auto\").with_steps([ColorStep(), MyListStep()])\n</code></pre> <p>This approach:</p> <ul> <li>Automatically includes all library-specific parsers based on installed dependencies.</li> <li>Positions your custom parsers either after the preprocessing steps or positionally.</li> </ul>"},{"location":"user-guide/advanced/#custom-spec-adapters","title":"Custom Spec Adapters","text":"<p>Custom adapters allow you to convert from any specification format to anyschema's internal format. Adapters need to follow the Adapter signature described in the API reference.</p>"},{"location":"user-guide/advanced/#basic-custom-adapter","title":"Basic Custom Adapter","text":"<p>Here's a simple adapter for a custom schema format:</p> <pre><code>from typing import TypedDict\nfrom anyschema import AnySchema\nfrom anyschema.typing import FieldSpecIterable\n\n\nclass CustomFieldSpec(TypedDict):\n    \"\"\"Field specification in the custom schema format.\"\"\"\n\n    name: str\n    type: type\n\n\nclass SimpleSchema:\n    \"\"\"A simple schema format.\"\"\"\n\n    def __init__(self, fields: list[CustomFieldSpec]) -&gt; None:\n        self.fields = fields\n\n\ndef simple_schema_adapter(spec: SimpleSchema) -&gt; FieldSpecIterable:\n    \"\"\"Adapter for SimpleSchema format.\n\n    Arguments:\n        spec: A SimpleSchema instance.\n\n    Yields:\n        Tuples of (field_name, field_type, constraints, metadata).\n    \"\"\"\n    for field in spec.fields:\n        yield field[\"name\"], field[\"type\"], (), {}\n\n\nschema_spec = SimpleSchema(\n    fields=[\n        {\"name\": \"id\", \"type\": int},\n        {\"name\": \"name\", \"type\": str},\n    ]\n)\n\nschema = AnySchema(spec=schema_spec, adapter=simple_schema_adapter)\nprint(schema.to_arrow())\n</code></pre> <pre><code>id: int64 not null\nname: string not null\n</code></pre>"},{"location":"user-guide/advanced/#adapter-with-metadata-conversion","title":"Adapter with Metadata Conversion","text":"<p>This example shows how to convert schema metadata to anyschema metadata:</p> <pre><code>from typing import Annotated\nfrom anyschema import AnySchema\nfrom anyschema.typing import FieldSpecIterable\n\n\nclass FieldWithConstraints:\n    \"\"\"A field with type and constraints.\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        type_: type,\n        min_val: int | None = None,\n        max_val: int | None = None,\n    ):\n        self.name = name\n        self.type = type_\n        self.min_val = min_val\n        self.max_val = max_val\n\n\nclass SchemaWithConstraints:\n    \"\"\"A schema format that includes constraints.\"\"\"\n\n    def __init__(self, fields: list[FieldWithConstraints]) -&gt; None:\n        self.fields = fields\n\n\ndef constrained_adapter(spec: SchemaWithConstraints) -&gt; FieldSpecIterable:\n    \"\"\"Adapter that converts constraints to the constraints tuple.\n\n    Arguments:\n        spec: A SchemaWithConstraints instance.\n\n    Yields:\n        Tuples of (field_name, field_type, constraints, metadata).\n    \"\"\"\n    for field in spec.fields:\n        constraints = []\n\n        if field.min_val is not None:\n            constraints.append((\"min\", field.min_val))\n        if field.max_val is not None:\n            constraints.append((\"max\", field.max_val))\n\n        yield field.name, field.type, tuple(constraints), {}\n\n\nschema_spec = SchemaWithConstraints(\n    fields=[\n        FieldWithConstraints(\"age\", int, min_val=0, max_val=120),\n        FieldWithConstraints(\"name\", str),\n    ]\n)\n\nschema = AnySchema(spec=schema_spec, adapter=constrained_adapter)\nprint(schema.to_arrow())\n</code></pre> <pre><code>age: int64 not null\nname: string not null\n</code></pre> <p>Notice that we don't have a parser step to handle the metadata in this example. You would need to implement one if you want to process custom metadata. See an example in the dedicated Custom Parser Steps section.</p>"},{"location":"user-guide/advanced/#adapter-for-nested-structures","title":"Adapter for Nested Structures","text":"<p>Handle nested schemas with a recursive adapter by dynamically creating TypedDict classes:</p> <pre><code>from typing import Any, TypedDict\nfrom anyschema import AnySchema\nfrom anyschema.typing import FieldSpecIterable\n\n\nclass NestedSchema:\n    \"\"\"A schema that can contain nested schemas.\"\"\"\n\n    def __init__(self, fields: dict[str, Any]) -&gt; None:\n        self.fields = fields\n\n\ndef nested_adapter(spec: NestedSchema) -&gt; FieldSpecIterable:\n    \"\"\"Adapter for nested schema structures.\n\n    For nested schemas, we dynamically create a TypedDict so the parser\n    can properly extract the field structure.\n\n    Arguments:\n        spec: A NestedSchema instance.\n\n    Yields:\n        Tuples of (field_name, field_type, constraints, metadata).\n    \"\"\"\n    for field_name, field_value in spec.fields.items():\n        if isinstance(field_value, NestedSchema):\n            # For nested schemas, create a TypedDict with the proper structure\n            nested_dict = {\n                name: type_ for name, type_, _, _ in nested_adapter(field_value)\n            }\n            # Create a dynamic TypedDict with the nested fields\n            nested_typed_dict = TypedDict(\n                f\"{field_name.title()}TypedDict\",  # Generate a unique name\n                nested_dict,  # Field name -&gt; type mapping\n            )\n            yield field_name, nested_typed_dict, (), {}\n        else:\n            yield field_name, field_value, (), {}\n\n\nschema_spec = NestedSchema(\n    fields={\n        \"id\": int,\n        \"profile\": NestedSchema(\n            fields={\n                \"name\": str,\n                \"age\": int,\n            }\n        ),\n    }\n)\nschema = AnySchema(spec=schema_spec, adapter=nested_adapter)\nprint(schema.to_arrow())\n</code></pre> <pre><code>id: int64 not null\nprofile: struct&lt;name: string, age: int64&gt; not null\n  child 0, name: string\n  child 1, age: int64\n</code></pre>"},{"location":"user-guide/advanced/#adapter-for-json-schema","title":"Adapter for JSON Schema","text":"<p>Here's a practical example of adapting from JSON Schema:</p> <pre><code>import json\nfrom anyschema import AnySchema\nfrom anyschema.typing import FieldSpecIterable\n\n\ndef json_schema_adapter(spec: str) -&gt; FieldSpecIterable:\n    \"\"\"Adapter for JSON Schema format.\n\n    Arguments:\n        spec: A JSON Schema with \"type\": \"object\" and \"properties\".\n\n    Yields:\n        Tuples of (field_name, field_type, constraints, metadata).\n    \"\"\"\n    spec = json.loads(spec)\n    if spec.get(\"type\") != \"object\":\n        raise ValueError(\"Only object types supported\")\n\n    properties = spec.get(\"properties\", {})\n    required = set(spec.get(\"required\", []))\n\n    type_mapping = {\n        \"string\": str,\n        \"integer\": int,\n        \"number\": float,\n        \"boolean\": bool,\n        \"array\": list,\n        \"object\": dict,\n    }\n\n    for field_name, field_spec in properties.items():\n        json_type = field_spec.get(\"type\")\n        python_type = type_mapping.get(json_type, object)\n\n        # Handle optional fields\n        if field_name not in required:\n            python_type = python_type | None\n\n        # Handle array types\n        if json_type == \"array\" and \"items\" in field_spec:\n            item_type = type_mapping.get(field_spec[\"items\"].get(\"type\"), object)\n            python_type = list[item_type]\n\n        yield field_name, python_type, (), {}\n\n\njson_schema = json.dumps(\n    {\n        \"type\": \"object\",\n        \"properties\": {\n            \"id\": {\"type\": \"integer\"},\n            \"name\": {\"type\": \"string\"},\n            \"tags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n            \"email\": {\"type\": \"string\"},\n        },\n        \"required\": [\"id\", \"name\"],\n    }\n)\n\nschema = AnySchema(spec=json_schema, adapter=json_schema_adapter)\nprint(schema.to_arrow())\n</code></pre> <pre><code>id: int64 not null\nname: string not null\ntags: list&lt;item: string&gt; not null\n  child 0, item: string\nemail: string\n</code></pre>"},{"location":"user-guide/best-practices/","title":"Best Practices","text":""},{"location":"user-guide/best-practices/#for-custom-parsers","title":"For Custom Parsers","text":"<ul> <li>Return <code>None</code> when you can't handle a type to let other parsers in the chain try.</li> <li>Use <code>self.pipeline.parse(inner_type, constraints=..., metadata=...)</code> for recursion.     This possibly allows to handle nested types by delegating to the pipeline.</li> <li>Preserve constraints and metadata: Pass both through when recursively parsing.</li> <li>Order matters: Place specialized parsers before general ones.</li> <li>Document what types the parser can handle: Make it clear in docstrings.</li> </ul> <pre><code>from typing import Any, TypeVar, get_args, get_origin\n\nimport narwhals as nw\nfrom anyschema.parsers import ParserStep\nfrom anyschema.typing import FieldConstraints, FieldMetadata, FieldType\n\n\nT = TypeVar(\"T\")\n\n\nclass CustomType: ...\n\n\nclass CustomList[T]: ...\n\n\nclass GoodParserStep(ParserStep):\n    \"\"\"Parser for CustomType.\n\n    Handles:\n\n    - CustomType: converts to String\n    - CustomList[T]: converts to List(T)\n    \"\"\"\n\n    def parse(\n        self,\n        input_type: FieldType,\n        constraints: FieldConstraints,\n        metadata: FieldMetadata,\n    ) -&gt; nw.dtypes.DType | None:\n        # Check if we can handle this type\n        if input_type is CustomType:\n            return nw.String()\n\n        # Handle generic version\n        if get_origin(input_type) is CustomList:\n            inner = get_args(input_type)[0]\n            # Delegate to pipeline for recursion\n            inner_dtype = self.pipeline.parse(inner, constraints=constraints, metadata=metadata)\n            return nw.List(inner_dtype)\n\n        # Return None if we can't handle it\n        return None\n</code></pre>"},{"location":"user-guide/best-practices/#for-custom-adapters","title":"For Custom Adapters","text":"<ul> <li>Use generators: Yield instead of returning a list for memory efficiency.</li> <li>Handle nested structures: Recursively convert nested schemas.</li> <li>Validate input: Check that the spec is the expected format.</li> <li>Convert constraints and metadata consistently: Have a clear mapping from your format to anyschema constraints     and metadata.</li> <li>Document the expected input format: Make it clear what spec format you accept.</li> </ul> <pre><code>from typing import Any, TypedDict\nfrom anyschema.typing import FieldSpecIterable\n\n\nclass CustomSchemaSpec:\n    def __init__(self, fields: dict[str, Any]) -&gt; None:\n        self.fields = fields\n\n\ndef good_adapter(spec: CustomSchemaSpec) -&gt; FieldSpecIterable:\n    \"\"\"Adapter for CustomSchemaSpec structures.\n\n    For nested schemas, we dynamically create a TypedDict so the parser\n    can properly extract the field structure.\n\n    Arguments:\n        spec: A CustomSchemaSpec instance.\n\n    Yields:\n        Tuples of (field_name, field_type, constraints, metadata).\n\n    Raises:\n        TypeError: If spec is not a CustomSchemaSpec instance.\n    \"\"\"\n    if not isinstance(spec, CustomSchemaSpec):\n        raise TypeError(f\"Expected `CustomSchemaSpec`, got {type(spec)}\")\n\n    for field_name, field_value in spec.fields.items():\n        if isinstance(field_value, CustomSchemaSpec):\n            # For nested schemas, create a TypedDict with the proper structure\n            nested_dict = {\n                name: type_ for name, type_, _, _ in good_adapter(field_value)\n            }\n            # Create a dynamic TypedDict with the nested fields\n            nested_typed_dict = TypedDict(\n                f\"{field_name.title()}TypedDict\",\n                nested_dict,\n            )\n            yield field_name, nested_typed_dict, (), {}\n        else:\n            yield field_name, field_value, (), {}\n</code></pre>"},{"location":"user-guide/best-practices/#integration-testing","title":"Integration Testing","text":"<p>Test your custom components thoroughly at multiple levels: unit tests for individual parsers and adapters, and integration tests for the complete flow.</p> <pre><code>import polars as pl\nimport pytest\n\nfrom anyschema import AnySchema\nfrom anyschema.parsers import ParserPipeline, ParserPipeline, PyTypeStep\n\n\n@pytest.fixture(scope=\"module\")\ndef custom_step() -&gt; GoodParserStep:\n    custom_step = GoodParserStep()\n    python_step = PyTypeStep()\n    _ = ParserPipeline([custom_step, python_step])\n    return custom_step\n\n\n@pytest.mark.parametrize(\n    (\"input_type\", \"expected_dtype\"),\n    [\n        (CustomType, nw.String()),\n        (CustomList[int], nw.List(nw.Int64())),\n        (str, None),\n    ],\n)\ndef test_custom_step_parse(\n    custom_step: GoodParserStep, input_type: Any, expected_dtype: nw.dtypes.DType\n) -&gt; None:\n    \"\"\"Test that custom parser handles its types correctly.\"\"\"\n    result = custom_step.parse(CustomType, constraints=(), metadata={})\n    assert result == expected_dtype\n\n\ndef test_custom_adapter() -&gt; None:\n    \"\"\"Test that custom adapter converts spec correctly.\"\"\"\n    fields = {\n        \"id\": int,\n        \"name\": str,\n    }\n    spec = CustomSchemaSpec(fields)\n    result = list(good_adapter(spec))\n\n    assert len(result) == len(fields)\n\n    expected = [\n        (\"id\", int, (), {}),\n        (\"name\", str, (), {}),\n    ]\n    assert result == expected\n\n\ndef test_custom_adapter_nested() -&gt; None:\n    \"\"\"Test that custom adapter handles nested schemas.\"\"\"\n    inner_fields = {\n        \"name\": str,\n        \"age\": int,\n    }\n    fields = {\n        \"id\": int,\n        \"profile\": CustomSchemaSpec(fields=inner_fields),\n    }\n    spec = CustomSchemaSpec(fields=fields)\n    result = list(good_adapter(spec))\n\n    assert len(result) == len(fields)\n    assert result[0] == (\"id\", int, (), {})\n\n    # Check that nested field is a TypedDict\n    assert result[1][0] == \"profile\"\n    assert hasattr(result[1][1], \"__annotations__\")\n\n\ndef test_custom_components_integration():\n    \"\"\"Test custom parser and adapter working together end-to-end.\"\"\"\n    schema_spec = CustomSchemaSpec(\n        fields={\n            \"custom_field\": CustomType,\n            \"custom_list\": CustomList[int],\n            \"name\": str,\n        }\n    )\n\n    schema = AnySchema(\n        spec=schema_spec,\n        pipeline=[GoodParserStep(), PyTypeStep()],\n        adapter=good_adapter,\n    )\n\n    # Verify the conversion to Arrow works correctly\n    scehma_pa = schema.to_arrow()\n    assert scehma_pa.names == [\"custom_field\", \"custom_list\", \"name\"]\n\n    # Verify types are converted correctly\n    schema_pl = schema.to_polars()\n    expected_pl = pl.Schema(\n        {\n            \"custom_field\": pl.String(),\n            \"custom_list\": pl.List(pl.Int64()),\n            \"name\": pl.String(),\n        },\n    )\n    assert schema_pl == expected_pl\n</code></pre>"},{"location":"user-guide/custom-end-to-end-example/","title":"End to End Example with Custom Components","text":"<p>Let's now combine the learnings from the previous section to show an example that combines a custom parser and a custom adapter.</p>"},{"location":"user-guide/custom-end-to-end-example/#1-define-custom-types","title":"1. Define custom types","text":"<pre><code>from typing import Any\n\nimport narwhals as nw\nfrom narwhals.dtypes import DType\n\nfrom anyschema import AnySchema\nfrom anyschema.parsers import (\n    ParserStep,\n    ForwardRefStep,\n    UnionTypeStep,\n    AnnotatedStep,\n    PyTypeStep,\n)\nfrom anyschema.typing import FieldSpecIterable\n\n\nclass Email:\n    \"\"\"Email address type.\"\"\"\n\n\nclass PhoneNumber:\n    \"\"\"Phone number type.\"\"\"\n\n\nclass Currency:\n    \"\"\"Monetary value type.\"\"\"\n</code></pre>"},{"location":"user-guide/custom-end-to-end-example/#2-create-custom-parser-for-these-such-types","title":"2. Create custom parser for these such types","text":"<pre><code>from anyschema.typing import FieldConstraints, FieldMetadata, FieldType\n\n\nclass CustomerTypesStep(ParserStep):\n    \"\"\"Parser for custom types.\"\"\"\n\n    def parse(\n        self,\n        input_type: FieldType,\n        constraints: FieldConstraints,\n        metadata: FieldMetadata,\n    ) -&gt; DType | None:\n        if input_type is Email:\n            return nw.String()\n        elif input_type is PhoneNumber:\n            return nw.String()\n        elif input_type is Currency:\n            return nw.Float32()\n        return None\n</code></pre>"},{"location":"user-guide/custom-end-to-end-example/#3-define-custom-schema-format","title":"3. Define custom schema format","text":"<pre><code>class CustomerSchema:\n    \"\"\"Custom schema format.\"\"\"\n\n    def __init__(self, entity_name: str, fields: list[dict]):\n        self.entity_name = entity_name\n        self.fields = fields\n</code></pre>"},{"location":"user-guide/custom-end-to-end-example/#4-create-adapter-for-the-custom-format","title":"4. Create adapter for the custom format","text":"<pre><code>def customer_schema_adapter(spec: CustomerSchema) -&gt; FieldSpecIterable:\n    \"\"\"Adapter for CustomerSchema format.\"\"\"\n    for field in spec.fields:\n        field_name = field[\"name\"]\n        field_type = field[\"type\"]\n        required = field.get(\"required\", True)\n\n        # Convert required=False to Optional\n        if not required:\n            field_type = field_type | None\n\n        yield field_name, field_type, (), {}\n</code></pre>"},{"location":"user-guide/custom-end-to-end-example/#5-create-pipeline-steps-with-custom-parser","title":"5. Create pipeline steps with custom parser","text":"<pre><code>pipeline_steps = [\n    ForwardRefStep(),\n    UnionTypeStep(),\n    AnnotatedStep(),\n    CustomerTypesStep(),\n    PyTypeStep(),\n]\n</code></pre>"},{"location":"user-guide/custom-end-to-end-example/#6-use-everything-together","title":"6. Use everything together","text":"<pre><code>customer_schema = CustomerSchema(\n    entity_name=\"Customer\",\n    fields=[\n        {\"name\": \"id\", \"type\": int, \"required\": True},\n        {\"name\": \"name\", \"type\": str, \"required\": True},\n        {\"name\": \"email\", \"type\": Email, \"required\": True},\n        {\"name\": \"phone\", \"type\": PhoneNumber, \"required\": False},\n        {\"name\": \"balance\", \"type\": Currency, \"required\": True},\n    ],\n)\n\nschema = AnySchema(\n    spec=customer_schema,\n    pipeline=pipeline_steps,\n    adapter=customer_schema_adapter,\n)\n\nprint(schema.to_polars())\n</code></pre> <pre><code>Schema({'id': Int64, 'name': String, 'email': String, 'phone': String, 'balance': Float32})\n</code></pre>"},{"location":"user-guide/getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with <code>anyschema</code> and explore its core functionality.</p>"},{"location":"user-guide/getting-started/#basic-usage","title":"Basic Usage","text":"<p><code>anyschema</code> accepts specifications in multiple formats including Pydantic models, SQLAlchemy tables, TypedDict, dataclasses, attrs classes, and plain Python dicts (and more to come, see anyschema#11).</p> <p>Let's explore each approach and when to use it.</p>"},{"location":"user-guide/getting-started/#with-pydantic-models","title":"With Pydantic Models","text":"<p>The most common way to use anyschema is with Pydantic models:</p> <pre><code>from anyschema import AnySchema\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    id: int\n    username: str\n    email: str\n    is_active: bool\n\n\nschema = AnySchema(spec=User)\n</code></pre> <p>Convert to different schema formats:</p> pyarrowpolarspandas <pre><code>print(schema.to_arrow())\n</code></pre> <pre><code>id: int64 not null\nusername: string not null\nemail: string not null\nis_active: bool not null\n</code></pre> <pre><code>print(schema.to_polars())\n</code></pre> <pre><code>Schema({'id': Int64, 'username': String, 'email': String, 'is_active': Boolean})\n</code></pre> <pre><code>print(schema.to_pandas())\n</code></pre> <pre><code>{'id': 'int64', 'username': &lt;class 'str'&gt;, 'email': &lt;class 'str'&gt;, 'is_active': 'bool'}\n</code></pre> <p>The <code>AnySchema</code> object also provides access to detailed field information through the <code>fields</code> attribute:</p> <pre><code>print(schema.fields[\"id\"])\n</code></pre> <pre><code>AnyField(name='id', dtype=Int64, nullable=False, unique=False, description=None, metadata={})\n</code></pre> <p>You can also provide field descriptions and other metadata. For Pydantic models, the <code>description</code> parameter of <code>Field()</code> is automatically extracted:</p> <pre><code>from anyschema import AnySchema\nfrom pydantic import BaseModel, Field\n\n\nclass Product(BaseModel):\n    id: int = Field(description=\"Unique product identifier\")\n    name: str = Field(description=\"Product name\")\n    price: float = Field(description=\"Product price in USD\")\n\n\nschema = AnySchema(spec=Product)\nfor field_name, field in schema.fields.items():\n    print(f\"{field_name}: {field.description!r}\")\n</code></pre> <pre><code>id: 'Unique product identifier'\nname: 'Product name'\nprice: 'Product price in USD'\n</code></pre> <p>See Metadata for more details on the <code>AnyField</code> class and supported metadata keys.</p>"},{"location":"user-guide/getting-started/#with-typeddict","title":"With TypedDict","text":"<p>You can use <code>TypedDict</code> for a lightweight way to define typed structures:</p> <pre><code>from anyschema import AnySchema\nfrom typing_extensions import TypedDict\n\n\nclass User(TypedDict):\n    id: int\n    username: str\n    email: str\n    is_active: bool\n\n\nschema = AnySchema(spec=User)\nprint(schema.to_arrow())\n</code></pre> <pre><code>id: int64 not null\nusername: string not null\nemail: string not null\nis_active: bool not null\n</code></pre>"},{"location":"user-guide/getting-started/#with-dataclasses","title":"With dataclasses","text":"<p>You can also use plain Python dataclasses</p> <pre><code>from anyschema import AnySchema\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass User:\n    id: int\n    username: str\n    email: str\n    is_active: bool\n\n\nschema = AnySchema(spec=User)\nprint(schema.to_arrow())\n</code></pre> <pre><code>id: int64 not null\nusername: string not null\nemail: string not null\nis_active: bool not null\n</code></pre>"},{"location":"user-guide/getting-started/#with-attrs-classes","title":"With attrs classes","text":"<p>attrs provides a powerful way to write classes with less boilerplate:</p> <pre><code>from anyschema import AnySchema\nfrom attrs import define\n\n\n@define\nclass User:\n    id: int\n    username: str\n    email: str\n    is_active: bool\n\n\nschema = AnySchema(spec=User)\nprint(schema.to_arrow())\n</code></pre> <pre><code>id: int64 not null\nusername: string not null\nemail: string not null\nis_active: bool not null\n</code></pre>"},{"location":"user-guide/getting-started/#with-sqlalchemy-tables","title":"With SQLAlchemy Tables","text":"<p>SQLAlchemy provides powerful ORM and Core table definitions that can be used directly:</p> ORM (DeclarativeBase)Core (Table) <pre><code>from anyschema import AnySchema\nfrom sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\nfrom sqlalchemy import String\n\n\nclass Base(DeclarativeBase):\n    pass\n\n\nclass User(Base):\n    __tablename__ = \"user\"\n\n    id: Mapped[int] = mapped_column(primary_key=True)\n    username: Mapped[str] = mapped_column(String(50))\n    email: Mapped[str]\n    is_active: Mapped[bool]\n\n\nschema = AnySchema(spec=User)\nprint(schema.to_arrow())\n</code></pre> <pre><code>id: int32 not null\nusername: string not null\nemail: string not null\nis_active: bool not null\n</code></pre> <pre><code>from anyschema import AnySchema\nfrom sqlalchemy import Table, Column, Integer, String, Boolean, MetaData\n\n\nmetadata = MetaData()\nuser_table = Table(\n    \"user\",\n    metadata,\n    Column(\"id\", Integer, primary_key=True),\n    Column(\"username\", String(50)),\n    Column(\"email\", String(100)),\n    Column(\"is_active\", Boolean),\n)\n\nschema = AnySchema(spec=user_table)\nprint(schema.to_arrow())\n</code></pre> <pre><code>id: int32 not null\nusername: string\nemail: string\nis_active: bool\n</code></pre> <p>SQLAlchemy support includes comprehensive type mapping for numeric types, strings, dates, binary data, JSON, UUIDs, Enums, and PostgreSQL-specific types like <code>ARRAY</code>.</p> <p>Both dynamic-length arrays (converted to <code>List</code>) and fixed-dimension arrays (converted to <code>Array</code>) are supported.</p>"},{"location":"user-guide/getting-started/#with-python-mappings","title":"With Python Mappings","text":"<p>You can also use plain Python mappings (such as dictionaries):</p> <pre><code>from anyschema import AnySchema\n\nspec = {\n    \"id\": int,\n    \"username\": str,\n    \"email\": str,\n    \"is_active\": bool,\n}\n\nschema = AnySchema(spec=spec)\nprint(schema.to_arrow())\n</code></pre> <pre><code>id: int64 not null\nusername: string not null\nemail: string not null\nis_active: bool not null\n</code></pre>"},{"location":"user-guide/getting-started/#with-sequence-of-tuples","title":"With Sequence of Tuples","text":"<p>Or use a sequence of <code>(name, type)</code> tuples:</p> <pre><code>from anyschema import AnySchema\n\nspec = [\n    (\"id\", int),\n    (\"username\", str),\n    (\"email\", str),\n    (\"is_active\", bool),\n]\n\nschema = AnySchema(spec=spec)\nprint(schema.to_polars())\n</code></pre> <pre><code>Schema({'id': Int64, 'username': String, 'email': String, 'is_active': Boolean})\n</code></pre>"},{"location":"user-guide/getting-started/#understanding-nullability","title":"Understanding nullability","text":"<p>One of the key features of <code>anyschema</code> is accurate nullability tracking:</p> <ul> <li>When you declare a field as <code>str</code>, it's non-nullable.</li> <li>When you declare it as <code>str | None</code> (or <code>Optional[str]</code>), it's nullable.</li> </ul> <p>This distinction is especially valuable when working with PyArrow, which supports <code>not null</code> constraints:</p> <pre><code>from pydantic import BaseModel\nfrom anyschema import AnySchema\nimport pyarrow as pa\n\n\nclass User(BaseModel):\n    name: str  # Non-nullable\n    email: str | None  # Nullable\n\n\nusers = [\n    User(name=\"Alice\", email=\"alice@example.com\"),\n    User(name=\"Bob\", email=None),\n]\n\n# Without anyschema: both fields nullable (PyArrow default)\ndefault_table = pa.Table.from_pylist([user.model_dump() for user in users])\nprint(\"Default PyArrow schema: both fields are nullable\")\nprint(default_table.schema)\n\n# With anyschema: explicit nullability from type annotations\nschema = AnySchema(spec=User)\nexplicit_table = pa.Table.from_pylist(\n    [user.model_dump() for user in users],\n    schema=schema.to_arrow(),\n)\nprint(\"\\nWith anyschema: name is not nullable, email is\")\nprint(explicit_table.schema)\n</code></pre> <pre><code>Default PyArrow schema: both fields are nullable\nname: string\nemail: string\n\nWith anyschema: name is not nullable, email is\nname: string not null\nemail: string\n</code></pre> <p>Notice how <code>name</code> is now marked as <code>not null</code>, accurately reflecting the constraint from your Pydantic model!</p> <p>This also means that if you try to validate data with <code>name=None</code>, Pydantic will reject it:</p> <pre><code>try:\n    User(name=None, email=\"nobody@example.com\")\nexcept Exception as e:\n    print(f\"Validation error: {e}\")\n</code></pre> <pre><code>Validation error: 1 validation error for User\nname\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n</code></pre> <p>See the Metadata guide for more details on nullable semantics and how to override type-based inference.</p>"},{"location":"user-guide/getting-started/#nested-types","title":"Nested Types","text":"<p>You can use nested structures with Pydantic models, dataclasses, or TypedDict:</p> PydanticTypedDictattrs <pre><code>from anyschema import AnySchema\nfrom pydantic import BaseModel\n\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    addresses: list[Address]\n\n\nschema = AnySchema(spec=Person)\npa_schema = schema.to_arrow()\nprint(pa_schema)\n</code></pre> <p>name: string not null age: int64 not null addresses: list&gt; not null   child 0, item: struct       child 0, street: string       child 1, city: string       child 2, country: string <pre><code>from anyschema import AnySchema\nfrom typing_extensions import TypedDict\n\n\nclass Address(TypedDict):\n    street: str\n    city: str\n    country: str\n\n\nclass Person(TypedDict):\n    name: str\n    age: int\n    addresses: list[Address]\n\n\nschema = AnySchema(spec=Person)\npa_schema = schema.to_arrow()\nprint(pa_schema)\n</code></pre> <pre><code>name: string not null\nage: int64 not null\naddresses: list&lt;item: struct&lt;street: string, city: string, country: string&gt;&gt; not null\n  child 0, item: struct&lt;street: string, city: string, country: string&gt;\n      child 0, street: string\n      child 1, city: string\n      child 2, country: string\n</code></pre> <pre><code>from anyschema import AnySchema\nfrom attrs import define\n\n\n@define\nclass Address:\n    street: str\n    city: str\n    country: str\n\n\n@define\nclass Person:\n    name: str\n    age: int\n    addresses: list[Address]\n\n\nschema = AnySchema(spec=Person)\npa_schema = schema.to_arrow()\nprint(pa_schema)\n</code></pre> <pre><code>name: string not null\nage: int64 not null\naddresses: list&lt;item: struct&lt;street: string, city: string, country: string&gt;&gt; not null\n  child 0, item: struct&lt;street: string, city: string, country: string&gt;\n      child 0, street: string\n      child 1, city: string\n      child 2, country: string\n</code></pre> <p>As you can see, a field (<code>addresses</code>) that contains a nested structure is correctly represented as a nested struct in the schema.</p>"},{"location":"user-guide/getting-started/#working-with-integer-constraints","title":"Working with (Integer) Constraints","text":"<p>Constraints are processed by the <code>AnnotatedTypesStep</code></p> <p>parser step, which refines types based on their metadata. The following examples demonstrate how constraints are handled.</p> <p>Pydantic's constrained integer types are automatically converted to appropriate unsigned or signed integers:</p> <pre><code>from anyschema import AnySchema\nfrom pydantic import BaseModel, PositiveInt, NonNegativeInt\n\n\nclass Metrics(BaseModel):\n    count: PositiveInt\n    offset: NonNegativeInt\n    delta: int\n\n\nschema = AnySchema(spec=Metrics)\narrow_schema = schema.to_arrow()\nprint(arrow_schema)\n</code></pre> <pre><code>count: uint64 not null\noffset: uint64 not null\ndelta: int64 not null\n</code></pre>"},{"location":"user-guide/getting-started/#using-annotated-types","title":"Using Annotated Types","text":"<p>You can also use <code>typing.Annotated</code> with constraint metadata:</p> <pre><code>from typing import Annotated\nfrom anyschema import AnySchema\nfrom pydantic import BaseModel, Field\n\n\nclass Product(BaseModel):\n    name: str\n    price: Annotated[float, Field(gt=0)]  # Price must be positive\n    quantity: Annotated[\n        int, Field(ge=0, lt=100)\n    ]  # Quantity must be non-negative, and say we limit it to &lt;100\n\n\nschema = AnySchema(spec=Product)\nprint(schema.to_polars())\n</code></pre> <pre><code>Schema({'name': String, 'price': Float64, 'quantity': UInt8})\n</code></pre>"},{"location":"user-guide/getting-started/#using-narwhals-directly","title":"Using Narwhals Directly","text":"<p>You can also work with Narwhals schemas directly (and pass them to <code>AnySchema</code>, which acts as a no-op in this case):</p> <pre><code>from narwhals.schema import Schema\nimport narwhals as nw\nfrom anyschema import AnySchema\n\n# Create a Narwhals schema\nnw_schema = Schema(\n    {\n        \"id\": nw.Int64(),\n        \"name\": nw.String(),\n        \"scores\": nw.List(nw.Float64()),\n    }\n)\n\nschema = AnySchema(spec=nw_schema)\n</code></pre>"},{"location":"user-guide/getting-started/#pandas-output-format","title":"Pandas output format","text":"<p>pandas schema</p> <p>Unlike pyarrow and polars, pandas does not have a native schema representation. Therefore our output is a dictionary mapping column names to dtypes.</p> <p>pandas multiple <code>dtype_backend</code>'s</p> <p>pandas supports multiple dtype backends that affect types nullability:</p> <ul> <li><code>None</code> (default): Uses standard NumPy dtypes (not nullable).</li> <li><code>\"numpy_nullable\"</code>* Uses pandas nullable dtypes (e.g., <code>Int64</code> instead of <code>int64</code>).</li> <li><code>\"pyarrow\"</code>: Uses PyArrow-backed dtypes (better performance, native nullable support).</li> </ul> <p>You can specify which backend to use via the <code>dtype_backend</code> parameter, either for all fields together, or for each field individually.</p> <p>Let's see it in practice:</p> <pre><code>from anyschema import AnySchema\nfrom pydantic import BaseModel, PositiveInt, NonNegativeInt\n\n\nclass Metrics(BaseModel):\n    count: PositiveInt\n    offset: NonNegativeInt\n    delta: int\n\n\nschema = AnySchema(spec=Metrics)\npd_schema = schema.to_pandas(\n    dtype_backend=(\n        \"pyarrow\",  # `count` will be mapped to a pyarrow dtype\n        \"numpy_nullable\",  # `offset` will be mapped to a pandas nullable numpy dtype\n        None,  # `delta` will be mapped to the default pandas numpy dtype\n    )\n)\nprint(pd_schema)\n</code></pre> <pre><code>{'count': 'UInt64[pyarrow]', 'offset': 'UInt64', 'delta': 'int64'}\n</code></pre>"},{"location":"user-guide/getting-started/#error-handling","title":"Error Handling","text":"<p>anyschema will raise exceptions for unsupported types:</p> <pre><code>from anyschema import AnySchema\n\ntry:\n    # This will fail - set is not supported\n    schema = AnySchema(spec={\"invalid\": set})\n    arrow_schema = schema.to_arrow()\nexcept NotImplementedError as e:\n    print(f\"Error: {e}\")\n    # Error: No parser in the pipeline could handle type: builtins.set\n</code></pre> <p>For Union types with more than two members (excluding None), an error is raised:</p> <pre><code>from anyschema import AnySchema\nfrom anyschema.exceptions import UnsupportedDTypeError\n\ntry:\n    # Union[int, str, float] is not supported\n    schema = AnySchema(spec={\"field\": int | str | float})\nexcept UnsupportedDTypeError as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"user-guide/metadata/","title":"Metadata","text":"<p>Metadata allows you to provide additional information about fields that influences how they are parsed into dataframe schemas.</p> <p>This guide covers how to specify metadata for different specification formats and which metadata keys have special behavior in <code>anyschema</code>.</p>"},{"location":"user-guide/metadata/#overview","title":"Overview","text":"<p>Metadata is a dictionary of key-value pairs attached to individual fields. While you can store any custom metadata, anyschema recognizes a specific nested structure with the <code>\"anyschema\"</code> (or <code>\"x-anyschema\"</code> for OpenAPI compatibility) key that modifies parsing behavior.</p> <p>OpenAPI Compatibility</p> <p>For better integration with OpenAPI tools and specifications, you can use <code>\"x-anyschema\"</code> instead of <code>\"anyschema\"</code>.</p> <p>Both work identically. See the OpenAPI Compatibility guide for more details.</p> <p>Currently supported special metadata keys under the <code>\"anyschema\"</code> (or <code>\"x-anyschema\"</code>) namespace:</p> <ul> <li><code>\"description\": &lt;str | None&gt;</code>: Provides a human-readable description of the field.</li> <li><code>\"dtype\": &lt;DType | str&gt;</code>: Specifies the Narwhals dtype of the field (see the dtype override section).</li> <li><code>\"nullable\": &lt;bool&gt;</code>: Specifies whether the field can contain null values.</li> <li><code>\"time_zone\": &lt;str | None&gt;</code>: Specifies timezone for datetime fields.</li> <li><code>\"time_unit\": &lt;TimeUnit&gt;</code>: Specifies time precision for datetime fields (default: <code>\"us\"</code>).</li> <li><code>\"unique\": &lt;bool&gt;</code>: Specifies whether all values in the field must be unique.</li> </ul>"},{"location":"user-guide/metadata/#the-anyfield-class","title":"The <code>AnyField</code> Class","text":"<p>Starting from version 0.3.0, <code>anyschema</code> provides a <code>AnyField</code> class that encapsulates detailed information about each field in a schema.</p> <p>When you create an <code>AnySchema</code>, it parses each field into a <code>AnyField</code> object that contains:</p> <ul> <li><code>name</code>: The field name</li> <li><code>dtype</code>: The Narwhals data type</li> <li><code>nullable</code>: Whether the field accepts null values</li> <li><code>unique</code>: Whether values must be unique</li> <li><code>description</code>: Human-readable field description</li> <li><code>metadata</code>: Custom metadata dictionary (excluding <code>anyschema</code> and <code>x-anyschema</code> keys)</li> </ul> <p>You can access these fields through the <code>fields</code> attribute:</p> <pre><code>from typing import Optional\nfrom anyschema import AnySchema\n\nschema = AnySchema(spec={\"id\": int, \"name\": str, \"email\": Optional[str]})\n\n# Access fields\nid_field = schema.fields[\"id\"]\nprint(id_field)\n\nemail_field = schema.fields[\"email\"]\nprint(email_field)\n</code></pre> <pre><code>AnyField(name='id', dtype=Int64, nullable=False, unique=False, description=None, metadata={})\nAnyField(name='email', dtype=String, nullable=True, unique=False, description=None, metadata={})\n</code></pre>"},{"location":"user-guide/metadata/#supported-metadata-keys","title":"Supported Metadata Keys","text":"<p>Metadata Precedence</p> <p>Explicit <code>anyschema</code> metadata keys always take precedence over values inferred from types.</p> <p>For example, setting <code>{\"anyschema\": {\"nullable\": False}}</code> will make a field non-nullable even if the type is <code>Optional[T]</code>. This allows you to override type-level inference when needed.</p>"},{"location":"user-guide/metadata/#description","title":"<code>description</code>","text":"<p>Provides a human-readable description of the field's purpose or content.</p> <ul> <li>Applicable to: All field types</li> <li>Default: <code>None</code></li> <li>Values: Any string value</li> </ul> <p>Automatic extraction from:</p> <ol> <li>Pydantic: The <code>description</code> parameter of <code>Field()</code> is automatically extracted</li> <li>SQLAlchemy: The <code>doc</code> parameter of <code>Column()</code> or <code>mapped_column()</code> is automatically extracted</li> <li>Dataclasses (Python 3.14+): The <code>doc</code> parameter of <code>field()</code> is automatically extracted</li> <li>Explicit metadata: Set <code>{\"anyschema\": {\"description\": ...}}</code> in field metadata</li> </ol> <p>Example with Pydantic:</p> <pre><code>from pydantic import BaseModel, Field\nfrom anyschema import AnySchema\n\n\nclass User(BaseModel):\n    id: int = Field(description=\"Unique user identifier\")\n    username: str = Field(description=\"User's login name\")\n    email: str\n\n\nschema = AnySchema(spec=User)\nprint(f\"id description: {schema.fields['id'].description!r}\")\nprint(f\"username description: {schema.fields['username'].description!r}\")\nprint(f\"email description: {schema.fields['email'].description!r}\")\n</code></pre> <pre><code>id description: 'Unique user identifier'\nusername description: \"User's login name\"\nemail description: None\n</code></pre> <p>Example with SQLAlchemy:</p> <pre><code>from sqlalchemy import Column, Integer, MetaData, String, Table\nfrom anyschema import AnySchema\n\nmetadata_obj = MetaData()\nuser_table = Table(\n    \"users\",\n    metadata_obj,\n    Column(\"id\", Integer, primary_key=True, doc=\"Primary key identifier\"),\n    Column(\"username\", String(50), doc=\"User's login name\"),\n    Column(\"email\", String(100)),\n)\n\nschema = AnySchema(spec=user_table)\n\nprint(f\"id description: {schema.fields['id'].description!r}\")\nprint(f\"username description: {schema.fields['username'].description!r}\")\nprint(f\"email description: {schema.fields['email'].description!r}\")\n</code></pre> <pre><code>id description: 'Primary key identifier'\nusername description: \"User's login name\"\nemail description: None\n</code></pre> <p>Example with Dataclasses:</p> <pre><code>from dataclasses import dataclass, field\nfrom anyschema import AnySchema\n\n\n@dataclass\nclass User:\n    id: int = field(metadata={\"anyschema\": {\"description\": \"Unique user identifier\"}})\n    username: str = field(metadata={\"anyschema\": {\"description\": \"User's login name\"}})\n    email: str\n\n\nschema = AnySchema(spec=User)\nprint(f\"id description: {schema.fields['id'].description!r}\")\nprint(f\"username description: {schema.fields['username'].description!r}\")\nprint(f\"email description: {schema.fields['email'].description!r}\")\n</code></pre> <pre><code>id description: 'Unique user identifier'\nusername description: \"User's login name\"\nemail description: None\n</code></pre>"},{"location":"user-guide/metadata/#dtype","title":"<code>dtype</code>","text":"<p>Override the automatically parsed dtype with a specific Narwhals dtype. This provides fine-grained control over individual field dtypes without writing a custom parser.</p> <ul> <li>Applicable to: All field types</li> <li>Values: A <code>narwhals.dtypes.DType</code> instance or its string representation (e.g., <code>\"String\"</code>, <code>\"List(Float64)\"</code>)</li> <li>Behavior: completely bypasses the parser pipeline and uses the specified dtype directly</li> </ul> <p>Pipeline Bypass</p> <p>When you specify a <code>dtype</code> override, the parser pipeline is completely bypassed for that field.</p> <p>This means:</p> <ul> <li>Type information (like <code>Optional[int]</code>) won't affect <code>nullable</code> unless explicitly set</li> <li>Constraints and annotations are ignored</li> <li>The specified dtype is used exactly as provided</li> </ul> <p>If you need nullable or other metadata, set them explicitly alongside <code>dtype</code>.</p>"},{"location":"user-guide/metadata/#when-to-override-dtype-vs-writing-a-custom-parser","title":"When to override dtype vs writing a custom parser","text":"<ul> <li>Use <code>dtype</code> override when you need to change the dtype for specific fields on a case-by-case basis.</li> <li>Use custom parsers when you want to change how a type is always parsed across your entire schema</li> </ul> <p>See the Custom parsers vs dtype override section below for a detailed comparison.</p>"},{"location":"user-guide/metadata/#example-override-with-narwhals-dtype","title":"Example: Override with Narwhals DType","text":"<pre><code>from typing import Optional\n\nfrom anyschema import AnySchema\nfrom pydantic import BaseModel, Field\nimport narwhals as nw\n\n\nclass ProductWithOverrides(BaseModel):\n    # Parse as String even though type hint is int\n    product_id: int = Field(json_schema_extra={\"anyschema\": {\"dtype\": nw.String()}})\n\n    # Parse as Int32 instead of default Int64\n    quantity: int = Field(json_schema_extra={\"anyschema\": {\"dtype\": \"Int32\"}})\n\n    # Without explicit nullable, Optional[int] won't make this nullable\n    price: Optional[int] = Field(json_schema_extra={\"anyschema\": {\"dtype\": \"UInt32\"}})\n\n    # Explicitly set nullable=True along with dtype override\n    name: Optional[str] = Field(\n        json_schema_extra={\"anyschema\": {\"dtype\": \"String\", \"nullable\": True}}\n    )\n\n\nschema = AnySchema(spec=ProductWithOverrides)\nprint(schema._nw_schema)\n</code></pre> <pre><code>Schema({'product_id': String, 'quantity': Int32, 'price': UInt32, 'name': String})\n</code></pre>"},{"location":"user-guide/metadata/#nullable","title":"<code>nullable</code>","text":"<p>Specifies whether the field can contain null values.</p> <p>Precedence rules (highest to lowest):</p> <ol> <li>Explicit <code>nullable</code> metadata key</li> <li>Type inference (<code>Optional[T]</code> or <code>T | None</code>)</li> <li>Default: <code>False</code> (non-nullable by default)</li> </ol> <p>Let's take a look at an example:</p> <pre><code>from pydantic import BaseModel, Field\nfrom anyschema import AnySchema\n\n\nclass User(BaseModel):\n    id: int = Field(json_schema_extra={\"anyschema\": {\"nullable\": False}})\n    username: str\n    email: str | None\n\n\nschema = AnySchema(spec=User)\n\nfor field in schema.fields.values():\n    print(f\"field '{field.name}' nullable: {field.nullable}\")\n</code></pre> <pre><code>field 'id' nullable: False\nfield 'username' nullable: False\nfield 'email' nullable: True\n</code></pre> <p>Overriding type inference with explicit metadata:</p> <pre><code>from typing import Optional\nfrom pydantic import BaseModel, Field\nfrom anyschema import AnySchema\n\n\nclass Config(BaseModel):\n    # Type says Optional, but metadata overrides to non-nullable\n    required_field: Optional[str] = Field(\n        json_schema_extra={\"anyschema\": {\"nullable\": False}}\n    )\n    # Type says non-Optional, but metadata overrides to nullable\n    optional_field: str = Field(json_schema_extra={\"anyschema\": {\"nullable\": True}})\n\n\nschema = AnySchema(spec=Config)\n\nfor field in schema.fields.values():\n    print(f\"field '{field.name}' nullable: {field.nullable}\")\n</code></pre> <pre><code>field 'required_field' nullable: False\nfield 'optional_field' nullable: True\n</code></pre>"},{"location":"user-guide/metadata/#understanding-nullable-semantics","title":"Understanding nullable semantics","text":"<p>The nullable property reflects whether a field accepts null values according to the type specification. This is particularly important when converting validated data (from Pydantic, attrs, dataclasses) to dataframe schemas.</p> <p>Consider this example:</p> <pre><code>from pydantic import BaseModel\nfrom anyschema import AnySchema\n\n\nclass User(BaseModel):\n    name: str  # Non-nullable: validation will reject None\n    email: str | None  # Nullable: None is explicitly allowed\n\n\nschema = AnySchema(spec=User)\n\nfor field in schema.fields.values():\n    print(f\"field '{field.name}' nullable: {field.nullable}\")\n</code></pre> <pre><code>field 'name' nullable: False\nfield 'email' nullable: True\n</code></pre> <p>Why this matters:</p> <p>When you use Pydantic to validate data, attempting to pass <code>None</code> for a non optional field will raise a validation error:</p> <pre><code># This works\nuser1 = User(name=\"Alice\", email=\"alice@example.com\")\nuser2 = User(name=\"Bob\", email=None)  # email can be None\n\ntry:\n    user3 = User(name=None, email=\"nobody@example.com\")  # This raises ValidationError\nexcept Exception as e:\n    print(f\"ValidationError: {type(e).__name__}\")\n</code></pre> <pre><code>ValidationError: ValidationError\n</code></pre> <p>The schema accurately reflects this constraint: <code>name</code> is not nullable, while <code>email</code> is nullable.</p>"},{"location":"user-guide/metadata/#pyarrow-schema-generation-with-nullability","title":"PyArrow schema generation with nullability","text":"<p>One powerful use case for explicit nullability is generating PyArrow schemas with <code>not null</code> constraints. PyArrow (and many database systems) can distinguish between nullable and non-nullable columns, which provides:</p> <ul> <li>Data validation: Catch null values in supposedly non-nullable columns</li> <li>Performance optimizations: Some engines can optimize non-nullable columns</li> <li>Clear data contracts: Document which fields are guaranteed to have values</li> </ul> <p>Without anyschema (default PyArrow behavior):</p> <pre><code>import pyarrow as pa\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    email: str | None\n\n\nusers = [\n    User(name=\"Alice\", email=\"alice@example.com\"),\n    User(name=\"Bob\", email=None),\n]\n\n# PyArrow's default: both fields are nullable\ndefault_table = pa.Table.from_pylist([user.model_dump() for user in users])\nprint(\"Default PyArrow schema:\")\nprint(default_table.schema)\n</code></pre> <pre><code>Default PyArrow schema:\nname: string\nemail: string\n</code></pre> <p>With anyschema (explicit nullability):</p> <pre><code>from anyschema import AnySchema\n\n# Generate schema with explicit nullability from Pydantic model\nanyschema_obj = AnySchema(spec=User)\narrow_schema = anyschema_obj.to_arrow()\n\n# Create table with explicit schema\nexplicit_table = pa.Table.from_pylist(\n    [user.model_dump() for user in users], schema=arrow_schema\n)\nprint(\"anyschema-generated PyArrow schema:\")\nprint(explicit_table.schema)\n</code></pre> <pre><code>anyschema-generated PyArrow schema:\nname: string not null\nemail: string\n</code></pre> <p>Notice that <code>name</code> is now marked as <code>not null</code> in the schema, accurately reflecting the type constraint from the Pydantic model!</p>"},{"location":"user-guide/metadata/#time_zone","title":"<code>time_zone</code>","text":"<p>Specifies the timezone for datetime fields as a string defined in <code>zoneinfo</code>.</p> <p>To see valid values run <code>import zoneinfo; zoneinfo.available_timezones()</code> for the full list.</p> <ul> <li>Applicable to: <code>datetime</code> types and Pydantic datetime types.</li> <li>Default: <code>None</code> (no timezone, i.e., naive datetime)</li> <li>Resulting dtype: <code>nw.Datetime(time_zone = &lt;time_zone value&gt;)</code></li> </ul>"},{"location":"user-guide/metadata/#time_unit","title":"<code>time_unit</code>","text":"<p>Specifies the time precision for datetime fields. Valid values are <code>\"s\"</code> (seconds), <code>\"ms\"</code>(milliseconds),<code>\"us\"</code>(microseconds, default),<code>\"ns\"</code>(nanoseconds).</p> <ul> <li>Applicable to: <code>datetime</code> types and Pydantic datetime types</li> <li>Default: <code>\"us\"</code> (microseconds)</li> <li>Resulting dtype: <code>nw.Datetime(time_unit = &lt;time_unit value&gt;)</code></li> </ul>"},{"location":"user-guide/metadata/#unique","title":"<code>unique</code>","text":"<p>Specifies whether all values in the field must be unique.</p> <ul> <li>Applicable to: All field types</li> <li>Default: <code>False</code></li> <li>Values: <code>True</code> or <code>False</code></li> </ul> <p>Precedence rules (highest to lowest):</p> <ol> <li>Explicit <code>unique</code> metadata key</li> <li>SQLAlchemy column <code>unique</code> property (auto-detected)</li> <li>Default: <code>False</code></li> </ol> <p>SQLAlchemy Auto-Detection</p> <p>For SQLAlchemy tables, the <code>unique</code> constraint is automatically detected from column properties. You can override this by explicitly setting <code>unique</code> in the column's <code>info</code> parameter.</p> <p>Example:</p> <pre><code>from pydantic import BaseModel, Field\nfrom anyschema import AnySchema\n\n\nclass User(BaseModel):\n    id: int = Field(json_schema_extra={\"anyschema\": {\"unique\": True}})\n    username: str = Field(json_schema_extra={\"anyschema\": {\"unique\": True}})\n    email: str\n\n\nschema = AnySchema(spec=User)\nprint(f\"id unique: {schema.fields['id'].unique}\")\nprint(f\"username unique: {schema.fields['username'].unique}\")\nprint(f\"email unique: {schema.fields['email'].unique}\")\n</code></pre> <pre><code>id unique: True\nusername unique: True\nemail unique: False\n</code></pre> <p>Overriding SQLAlchemy unique constraint with explicit metadata:</p> <pre><code>from sqlalchemy import Column, Integer, MetaData, String, Table\nfrom anyschema import AnySchema\n\nmetadata_obj = MetaData()\nuser_table = Table(\n    \"users\",\n    metadata_obj,\n    Column(\"username\", String(50), unique=True),  # SQLAlchemy unique=True\n    # Override SQLAlchemy's unique with explicit metadata\n    Column(\n        \"email\",\n        String(100),\n        unique=True,  # SQLAlchemy says unique=True\n        info={\"anyschema\": {\"unique\": False}},  # But metadata overrides to False\n    ),\n)\n\nschema = AnySchema(spec=user_table)\n\nprint(f\"username unique (from SQLAlchemy): {schema.fields['username'].unique}\")\nprint(f\"email unique (overridden by metadata): {schema.fields['email'].unique}\")\n</code></pre> <pre><code>username unique (from SQLAlchemy): True\nemail unique (overridden by metadata): False\n</code></pre>"},{"location":"user-guide/metadata/#combining-multiple-metadata","title":"Combining Multiple Metadata","text":"<p>You can specify multiple metadata keys for the same field:</p> <pre><code>from datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom anyschema import AnySchema\n\n\nclass LogEntry(BaseModel):\n    message: str\n    timestamp: datetime = Field(\n        json_schema_extra={\"anyschema\": {\"time_zone\": \"UTC\", \"time_unit\": \"ns\"}}\n    )\n\n\nschema = AnySchema(spec=LogEntry)\nprint(schema._nw_schema)\n</code></pre> <pre><code>Schema({'message': String, 'timestamp': Datetime(time_unit='ns', time_zone='UTC')})\n</code></pre>"},{"location":"user-guide/metadata/#how-to-specify-metadata","title":"How to specify metadata","text":"<p>Different specification formats have their own ways of attaching metadata to fields.</p> <p>Here's how to do it for each supported format.</p>"},{"location":"user-guide/metadata/#pydantic-models","title":"Pydantic Models","text":"<p>For Pydantic models, use the <code>json_schema_extra</code> parameter in <code>Field()</code>:</p> <pre><code>from datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom anyschema import AnySchema\n\n\nclass EventModel(BaseModel):\n    name: str\n    created_at: datetime  # Default datetime (no metadata)\n    scheduled_at: datetime = Field(  # Specify timezone\n        json_schema_extra={\"anyschema\": {\"time_zone\": \"UTC\"}}\n    )\n    started_at: datetime = Field(  # Specify time precision\n        json_schema_extra={\"anyschema\": {\"time_unit\": \"ms\"}}\n    )\n    completed_at: datetime = Field(  # Specify both timezone and precision\n        json_schema_extra={\n            \"anyschema\": {\"time_zone\": \"Europe/Berlin\", \"time_unit\": \"ns\"}\n        }\n    )\n\n\nschema = AnySchema(spec=EventModel)\nprint(schema._nw_schema)\n</code></pre> <pre><code>Schema({'name': String, 'created_at': Datetime(time_unit='us', time_zone=None), 'scheduled_at': Datetime(time_unit='us', time_zone='UTC'), 'started_at': Datetime(time_unit='ms', time_zone=None), 'completed_at': Datetime(time_unit='ns', time_zone='Europe/Berlin')})\n</code></pre>"},{"location":"user-guide/metadata/#pydantic-special-datetime-types","title":"Pydantic Special Datetime Types","text":"<p>When using Pydantic's special datetime types (<code>AwareDatetime</code>, <code>NaiveDatetime</code>), metadata is particularly useful:</p> <pre><code>from pydantic import AwareDatetime, NaiveDatetime, Field\n\n\nclass TimeModel(BaseModel):\n    aware_utc: AwareDatetime = Field(  # AwareDatetime **requires** timezone metadata\n        json_schema_extra={\"anyschema\": {\"time_zone\": \"UTC\"}}\n    )\n    naive: NaiveDatetime = (\n        Field(  # NaiveDatetime **rejects** timezone metadata (will raise an error)\n            json_schema_extra={\"anyschema\": {\"time_unit\": \"ns\"}}\n        )\n    )\n\n\nschema = AnySchema(spec=TimeModel)\nprint(schema._nw_schema)\n</code></pre> <pre><code>Schema({'aware_utc': Datetime(time_unit='us', time_zone='UTC'), 'naive': Datetime(time_unit='ns', time_zone=None)})\n</code></pre> <p>AwareDatetime Requires Timezone</p> <p>Pydantic's <code>AwareDatetime</code> type requires you to specify a timezone via <code>{\"anyschema\": {\"time_zone\": ...}}</code> metadata, otherwise anyschema will raise an <code>UnsupportedDTypeError</code>.</p> <p>NaiveDatetime Cannot Have Timezone</p> <p>Pydantic's <code>NaiveDatetime</code> type will raise an <code>UnsupportedDTypeError</code> if you specify a timezone in metadata.</p>"},{"location":"user-guide/metadata/#attrs-classes","title":"attrs Classes","text":"<p>For attrs classes, use the <code>metadata</code> parameter in <code>attrs.field()</code>:</p> <pre><code>from datetime import datetime\nimport attrs\nfrom anyschema import AnySchema\n\n\n@attrs.define\nclass EventModel:\n    name: str\n    created_at: datetime  # Default datetime (no metadata)\n    scheduled_at: datetime = attrs.field(  # Specify timezone\n        metadata={\"anyschema\": {\"time_zone\": \"UTC\"}}\n    )\n    started_at: datetime = attrs.field(  # Specify time precision\n        metadata={\"anyschema\": {\"time_unit\": \"ms\"}}\n    )\n    completed_at: datetime = attrs.field(  # Specify both timezone and precision\n        metadata={\"anyschema\": {\"time_zone\": \"Europe/Berlin\", \"time_unit\": \"ns\"}}\n    )\n\n\nschema = AnySchema(spec=EventModel)\nprint(schema._nw_schema)\n</code></pre> <pre><code>Schema({'name': String, 'created_at': Datetime(time_unit='us', time_zone=None), 'scheduled_at': Datetime(time_unit='us', time_zone='UTC'), 'started_at': Datetime(time_unit='ms', time_zone=None), 'completed_at': Datetime(time_unit='ns', time_zone='Europe/Berlin')})\n</code></pre> <p>This also works with <code>@attrs.frozen</code> classes:</p> <pre><code>@attrs.frozen\nclass ImmutableEvent:\n    event_id: int\n    timestamp: datetime = attrs.field(\n        metadata={\"anyschema\": {\"time_zone\": \"UTC\", \"time_unit\": \"ms\"}}\n    )\n\n\nschema = AnySchema(spec=ImmutableEvent)\nprint(schema._nw_schema)\n</code></pre> <pre><code>Schema({'event_id': Int64, 'timestamp': Datetime(time_unit='ms', time_zone='UTC')})\n</code></pre>"},{"location":"user-guide/metadata/#dataclasses","title":"Dataclasses","text":"<p>For standard Python dataclasses, use the <code>metadata</code> parameter in <code>field()</code>:</p> <pre><code>from dataclasses import dataclass, field\nfrom datetime import datetime\nfrom anyschema import AnySchema\n\n\n@dataclass\nclass EventModel:\n    name: str\n    created_at: datetime  # Default datetime (no metadata)\n    scheduled_at: datetime = field(  # Specify timezone\n        metadata={\"anyschema\": {\"time_zone\": \"UTC\"}}\n    )\n    started_at: datetime = field(  # Specify time precision\n        metadata={\"anyschema\": {\"time_unit\": \"ms\"}}\n    )\n    completed_at: datetime = field(  # Specify both timezone and precision\n        metadata={\"anyschema\": {\"time_zone\": \"Europe/Berlin\", \"time_unit\": \"ns\"}}\n    )\n\n\nschema = AnySchema(spec=EventModel)\nprint(schema._nw_schema)\n</code></pre> <pre><code>Schema({'name': String, 'created_at': Datetime(time_unit='us', time_zone=None), 'scheduled_at': Datetime(time_unit='us', time_zone='UTC'), 'started_at': Datetime(time_unit='ms', time_zone=None), 'completed_at': Datetime(time_unit='ns', time_zone='Europe/Berlin')})\n</code></pre> <p>This also works with Pydantic's dataclass decorator:</p> <pre><code>from pydantic.dataclasses import dataclass as pydantic_dataclass\n\n\n@pydantic_dataclass\nclass PydanticDataclassEvent:\n    event_id: int\n    timestamp: datetime = field(\n        metadata={\"anyschema\": {\"time_zone\": \"UTC\", \"time_unit\": \"ms\"}}\n    )\n\n\nschema = AnySchema(spec=PydanticDataclassEvent)\nprint(schema._nw_schema)\n</code></pre> <pre><code>Schema({'event_id': Int64, 'timestamp': Datetime(time_unit='ms', time_zone='UTC')})\n</code></pre>"},{"location":"user-guide/metadata/#sqlalchemy-tables","title":"SQLAlchemy Tables","text":"<p>For SQLAlchemy tables and ORM models, use the <code>info</code> parameter in <code>Column()</code> or <code>mapped_column()</code>.</p> <p>Additionally, SQLAlchemy automatically populates <code>nullable</code> and <code>unique</code> metadata based on column properties:</p> <pre><code>from sqlalchemy import Column, Integer, MetaData, String, Table\nfrom anyschema import AnySchema\n\nmetadata = MetaData()\n\nuser_table = Table(\n    \"users\",\n    metadata,\n    Column(\"id\", Integer, primary_key=True, nullable=False),  # Not nullable\n    Column(\"username\", String(50), unique=True),  # Unique constraint\n    Column(\"email\", String(100), nullable=True),  # Explicitly nullable\n    Column(\"bio\", String(500)),  # Nullable by default\n)\n\nschema = AnySchema(spec=user_table)\n\nprint(schema.fields[\"id\"])\nprint(schema.fields[\"username\"])\nprint(schema.fields[\"email\"])\nprint(schema.fields[\"bio\"])\n</code></pre> <pre><code>AnyField(name='id', dtype=Int32, nullable=False, unique=False, description=None, metadata={})\nAnyField(name='username', dtype=String, nullable=True, unique=True, description=None, metadata={})\nAnyField(name='email', dtype=String, nullable=True, unique=False, description=None, metadata={})\nAnyField(name='bio', dtype=String, nullable=True, unique=False, description=None, metadata={})\n</code></pre> <p>SQLAlchemy DateTime Behavior</p> <ul> <li>Use <code>DateTime()</code> (or <code>DateTime(timezone=False)</code>) for naive datetimes.     You can specify <code>time_unit</code> metadata but not <code>time_zone</code>.</li> <li>Use <code>DateTime(timezone=True)</code> for timezone-aware datetimes. You must specify     <code>time_zone</code> metadata via the <code>info</code> parameter.</li> </ul> <pre><code>from sqlalchemy import Column, DateTime, Integer, MetaData, String, Table\nfrom anyschema import AnySchema\n\nmetadata = MetaData()\n\nevent_table = Table(\n    \"events\",\n    metadata,\n    Column(\"id\", Integer, primary_key=True),\n    Column(\"name\", String(100)),\n    Column(\"created_at\", DateTime),  # Default datetime (no metadata)\n    Column(\n        \"scheduled_at\",\n        DateTime(timezone=True),\n        info={\"anyschema\": {\"time_zone\": \"UTC\"}},  # Specify timezone via info\n    ),\n    Column(\n        \"started_at\",\n        DateTime,\n        info={\"anyschema\": {\"time_unit\": \"ms\"}},  # Specify time precision via info\n    ),\n    Column(\n        \"completed_at\",\n        DateTime(timezone=True),\n        info={  # Specify both timezone and precision\n            \"anyschema\": {\"time_zone\": \"Europe/Berlin\", \"time_unit\": \"ns\"}\n        },\n    ),\n)\n\nschema = AnySchema(spec=event_table)\nprint(schema._nw_schema)\n</code></pre> <pre><code>Schema({'id': Int32, 'name': String, 'created_at': Datetime(time_unit='us', time_zone=None), 'scheduled_at': Datetime(time_unit='us', time_zone='UTC'), 'started_at': Datetime(time_unit='ms', time_zone=None), 'completed_at': Datetime(time_unit='ns', time_zone='Europe/Berlin')})\n</code></pre> <p>This also works with SQLAlchemy ORM models using <code>mapped_column()</code>:</p> <pre><code>from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\n\n\nclass Base(DeclarativeBase):\n    pass\n\n\nclass EventORM(Base):\n    __tablename__ = \"event_orm\"\n\n    id: Mapped[int] = mapped_column(primary_key=True)\n    name: Mapped[str]\n    created_at: Mapped[DateTime] = mapped_column(DateTime)\n    scheduled_at: Mapped[DateTime] = mapped_column(\n        DateTime(timezone=True), info={\"anyschema\": {\"time_zone\": \"UTC\"}}\n    )\n    started_at: Mapped[DateTime] = mapped_column(\n        DateTime, info={\"anyschema\": {\"time_unit\": \"ms\"}}\n    )\n    completed_at: Mapped[DateTime] = mapped_column(\n        DateTime(timezone=True),\n        info={\"anyschema\": {\"time_zone\": \"Europe/Berlin\", \"time_unit\": \"ns\"}},\n    )\n\n\nschema = AnySchema(spec=EventORM)\nprint(schema._nw_schema)\n</code></pre> <pre><code>Schema({'id': Int32, 'name': String, 'created_at': Datetime(time_unit='us', time_zone=None), 'scheduled_at': Datetime(time_unit='us', time_zone='UTC'), 'started_at': Datetime(time_unit='ms', time_zone=None), 'completed_at': Datetime(time_unit='ns', time_zone='Europe/Berlin')})\n</code></pre>"},{"location":"user-guide/metadata/#sqlalchemy-timezone-aware-datetime","title":"SQLAlchemy Timezone-Aware DateTime","text":"<p>When using <code>DateTime(timezone=True)</code> in SQLAlchemy, you must specify a timezone via the <code>info</code> parameter:</p> <pre><code>event_table_tz = Table(\n    \"events_tz\",\n    metadata,\n    Column(\"id\", Integer, primary_key=True),\n    Column(\n        \"timestamp_utc\",\n        DateTime(timezone=True),  # timezone=True requires time_zone in info\n        info={\"anyschema\": {\"time_zone\": \"UTC\"}},\n    ),\n    Column(\n        \"timestamp_berlin\",\n        DateTime(timezone=True),\n        info={\"anyschema\": {\"time_zone\": \"Europe/Berlin\", \"time_unit\": \"ms\"}},\n    ),\n)\n\nschema = AnySchema(spec=event_table_tz)\nprint(schema._nw_schema)\n</code></pre> <pre><code>Schema({'id': Int32, 'timestamp_utc': Datetime(time_unit='us', time_zone='UTC'), 'timestamp_berlin': Datetime(time_unit='ms', time_zone='Europe/Berlin')})\n</code></pre> <p>DateTime(timezone=True) Requires Timezone</p> <p>SQLAlchemy's <code>DateTime(timezone=True)</code> does not specify a fixed timezone value (it only indicates the database should store timezone information).</p> <p>You must specify a timezone via <code>info={'anyschema': {'time_zone': 'UTC'}}</code>, otherwise anyschema will raise an <code>UnsupportedDTypeError</code>.</p> <p>DateTime() Cannot Have Timezone with timezone=False</p> <p>SQLAlchemy's <code>DateTime()</code> or <code>DateTime(timezone=False)</code> is for naive datetimes and will raise an <code>UnsupportedDTypeError</code> if you specify a timezone in the <code>info</code> parameter.</p>"},{"location":"user-guide/metadata/#typeddict","title":"TypedDict","text":"<p>TypedDict Limitation</p> <p>TypedDict classes do not support field metadata at the type annotation level. If you need to specify metadata for TypedDict fields, consider using dataclasses, Pydantic models, or attrs classes instead.</p>"},{"location":"user-guide/metadata/#custom-metadata","title":"Custom Metadata","text":"<p>While anyschema recognizes the special <code>\"anyschema\"</code> key (and its OpenAPI variant <code>\"x-anyschema\"</code>), you can also include custom metadata for your own purposes. This metadata will be passed to custom parser steps, allowing you to implement domain-specific parsing logic.</p> <p>For example:</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass Product(BaseModel):\n    name: str = Field(\n        json_schema_extra={\n            \"anyschema\": {\"time_zone\": \"UTC\"},  # Recognized by anyschema\n            \"my_app/description\": \"Product name\",  # Custom metadata\n            \"my_app/max_length\": 100,  # Custom metadata\n        }\n    )\n</code></pre> <p>To handle custom metadata, you would need to implement a custom parser step.</p> <p>See the Advanced Usage guide for more information on creating custom parsers that process metadata.</p>"},{"location":"user-guide/metadata/#custom-parsers-vs-dtype-override","title":"Custom parsers vs <code>dtype</code> override","text":"<p>Both custom parsers and the <code>dtype</code> metadata override allow you to control how types are converted to Narwhals dtypes, but they serve different purposes and work at different levels.</p> <p>Custom parser approach characteristics:</p> <ul> <li>Global scope: Affects all fields with the specified type across your entire schema</li> <li>Runs in pipeline: Integrated into the parser pipeline, respects order and precedence</li> <li>Reusable: Define once, applies to all schemas using that pipeline</li> <li>Type-driven: Makes decisions based on types, constraints and metadata</li> <li>Composable: Can be combined with other parsers in the pipeline</li> </ul> <p><code>dtype</code> override approach characteristics:</p> <ul> <li>Field-specific: Affects only the individual field where it's specified</li> <li>Bypasses pipeline: Completely skips type parsing for that field</li> <li>Declarative: Specified in field metadata, not in parser code</li> <li>Granular control: Different fields with the same type can have different dtypes</li> <li>Configuration-friendly: Can be stored in config files, database schemas, etc.</li> </ul> <p>It's entirely possible to combine both approaches: you can use both custom parsers and dtype overrides together.</p> <p>The dtype override takes precedence:</p> <pre><code>Traceback (most recent call last):\n  File \"/home/runner/work/anyschema/anyschema/.venv/lib/python3.12/site-packages/markdown_exec/_internal/formatters/python.py\", line 71, in _run_python\n    exec_python(code, code_block_id, exec_globals)\n  File \"/home/runner/work/anyschema/anyschema/.venv/lib/python3.12/site-packages/markdown_exec/_internal/formatters/_exec_python.py\", line 8, in exec_python\n    exec(compiled, exec_globals)  # noqa: S102\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"&lt;code block: session combined-example; n1&gt;\", line 8, in &lt;module&gt;\n    class Int32ParserStep(ParserStep):\n  File \"&lt;code block: session combined-example; n1&gt;\", line 16, in Int32ParserStep\n    ) -&gt; nw.DType | None:\n          ^^^^^^^^\n  File \"/home/runner/work/anyschema/anyschema/.venv/lib/python3.12/site-packages/narwhals/__init__.py\", line 191, in __getattr__\n    raise AttributeError(msg)\nAttributeError: module 'narwhals' has no attribute 'DType'. Did you mean: 'dtypes'?\n</code></pre> <p>In this example:</p> <ul> <li><code>id</code> and <code>stock</code> use the custom parser -&gt; <code>Int32</code></li> <li><code>quantity</code> uses dtype override -&gt; <code>Int16</code> (override takes precedence)</li> <li><code>name</code> uses standard parsing -&gt; <code>String</code></li> </ul>"},{"location":"user-guide/openapi-compatibility/","title":"OpenAPI Compatibility","text":"<p><code>anyschema</code> supports OpenAPI-compatible metadata through the <code>x-anyschema</code> prefix, which is an extension point defined in the OpenAPI specification.</p>"},{"location":"user-guide/openapi-compatibility/#what-is-openapi","title":"What is OpenAPI?","text":"<p>OpenAPI (formerly known as Swagger) is a widely-adopted specification for describing REST APIs. It allows you to define your API's structure, endpoints, request/response formats, and more in a standardized way.</p>"},{"location":"user-guide/openapi-compatibility/#extension-fields-in-openapi","title":"Extension fields in OpenAPI","text":"<p>The OpenAPI specification allows custom extensions through fields prefixed with <code>x-</code>. These extension fields can contain any valid JSON and are used to add vendor-specific or custom information that's not part of the core OpenAPI specification.</p>"},{"location":"user-guide/openapi-compatibility/#using-x-anyschema-prefix","title":"Using <code>x-anyschema</code> prefix","text":"<p>In <code>anyschema</code>, you can use either <code>\"anyschema\"</code> or <code>\"x-anyschema\"</code> as the metadata namespace key. Both work identically:</p> <pre><code>from pydantic import BaseModel, Field\nfrom anyschema import AnySchema\n\n\nclass Product(BaseModel):\n    # Standard anyschema format\n    name: str = Field(json_schema_extra={\"anyschema\": {\"nullable\": False}})\n\n    # OpenAPI-compatible format (with x- prefix)\n    price: float = Field(json_schema_extra={\"x-anyschema\": {\"nullable\": True}})\n\n\nschema = AnySchema(spec=Product)\n\nprint(f\"name nullable: {schema.fields['name'].nullable}\")\nprint(f\"price nullable: {schema.fields['price'].nullable}\")\n</code></pre> <pre><code>name nullable: False\nprice nullable: True\n</code></pre>"},{"location":"user-guide/openapi-compatibility/#why-support-x-anyschema","title":"Why support <code>x-anyschema</code>?","text":"<p>There are several reasons to support the <code>x-anyschema</code> prefix:</p> <ol> <li> <p>OpenAPI Integration: If you're generating OpenAPI specifications from Pydantic models and want to include     anyschema metadata, using the <code>x-</code> prefix makes it clear that this is an extension field.</p> </li> <li> <p>Tool Compatibility: Some OpenAPI tools and validators may flag unknown fields without the <code>x-</code> prefix as errors.     Using <code>x-anyschema</code> ensures better compatibility.</p> </li> <li> <p>Standards Compliance: Following the OpenAPI convention makes your API documentation more standardized and easier     for other developers to understand.</p> </li> </ol>"},{"location":"user-guide/openapi-compatibility/#choosing-between-anyschema-and-x-anyschema","title":"Choosing between <code>anyschema</code> and <code>x-anyschema</code>","text":"<p>Both formats work identically in <code>anyschema</code>. Choose based on your needs:</p> <ul> <li> <p>Use <code>\"anyschema\"</code> if:</p> <ul> <li>You're only using anyschema internally</li> <li>You want cleaner, shorter metadata keys</li> <li>You're not generating OpenAPI specifications</li> </ul> </li> <li> <p>Use <code>\"x-anyschema\"</code> if:</p> <ul> <li>You're generating OpenAPI specifications</li> <li>You want to be explicit that this is an extension field</li> <li>You're integrating with OpenAPI tooling</li> <li>You want maximum standards compliance</li> </ul> </li> </ul>"},{"location":"user-guide/openapi-compatibility/#mixing-both-formats","title":"Mixing both formats","text":"<p>Warning</p> <p>You should not mix both formats in the same metadata dictionary.</p> <p>If both <code>\"anyschema\"</code> and <code>\"x-anyschema\"</code> are present, <code>anyschema</code> will use whichever it finds first (with <code>\"anyschema\"</code> taking precedence).</p> <pre><code># \u274c Don't do this - mixing both formats\nmetadata = {\n    \"anyschema\": {\"nullable\": True},\n    \"x-anyschema\": {\"unique\": True},  # This will be ignored!\n}\n\n# \u2705 Do this - use one format consistently\nmetadata = {\n    \"x-anyschema\": {\n        \"nullable\": True,\n        \"unique\": True,\n    }\n}\n</code></pre>"},{"location":"user-guide/openapi-compatibility/#further-reading","title":"Further Reading","text":"<ul> <li>OpenAPI Specification</li> <li>OpenAPI Extension Fields</li> <li>Pydantic and OpenAPI</li> </ul>"},{"location":"user-guide/serde/","title":"Serialization &amp; Deserialization","text":"<p>The <code>anyschema.serde</code> module provides utilities to serialize and deserialize Narwhals dtypes to and from string representations.</p> <p>This is essential when you need to store, transmit, or share schema information in JSON-compatible formats.</p>"},{"location":"user-guide/serde/#overview","title":"Overview","text":"<p>Two main functions are available:</p> <ul> <li><code>serialize_dtype(dtype: DType) -&gt; str</code>: Converts a Narwhals dtype to its string representation.</li> <li><code>deserialize_dtype(into_dtype: str) -&gt; DType</code>: Reconstructs a Narwhals dtype from a string.</li> </ul> <p>These functions support all Narwhals dtypes, including complex nested structures like <code>List</code>, <code>Struct</code>, and <code>Array</code>.</p>"},{"location":"user-guide/serde/#why-serialization-matters","title":"Why Serialization Matters","text":"<p>Serialization enables you to:</p> <ul> <li>Store schemas in databases or configuration files as JSON</li> <li>Transmit schemas via APIs or network requests</li> <li>Share schemas across different services or programming languages</li> <li>Document types explicitly in JSON schemas beyond Python's type system</li> <li>Version control schema definitions in human-readable formats</li> </ul>"},{"location":"user-guide/serde/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/serde/#simple-types","title":"Simple Types","text":"<p>Converting basic Narwhals dtypes to strings and back:</p> <pre><code>import narwhals as nw\nfrom anyschema.serde import serialize_dtype, deserialize_dtype\n\n\ndtype = nw.Int64()\n\n# Serialize a dtype to string\ndtype_str = serialize_dtype(dtype)\nprint(f\"Serialized: {dtype_str}\")\n\n# Deserialize back to dtype\nreconstructed = deserialize_dtype(dtype_str)\nprint(f\"Deserialized: {reconstructed}\")\nprint(f\"Round-trip successful: {dtype == reconstructed}\")\n</code></pre> <pre><code>Serialized: Int64\nDeserialized: Int64\nRound-trip successful: True\n</code></pre>"},{"location":"user-guide/serde/#nested-types","title":"Nested Types","text":"<p>Serde handles nested and complex types seamlessly:</p> <pre><code>import narwhals as nw\nfrom anyschema.serde import serialize_dtype, deserialize_dtype\n\n\n# Complex nested structure\ncomplex_dtype = nw.Struct(\n    {\n        \"id\": nw.Int64(),\n        \"tags\": nw.List(nw.String()),\n        \"metadata\": nw.Struct(\n            {\n                \"created\": nw.Datetime(time_unit=\"ms\", time_zone=\"UTC\"),\n                \"active\": nw.Boolean(),\n            }\n        ),\n    }\n)\n\n# Serialize\ndtype_str = serialize_dtype(complex_dtype)\nprint(f\"Serialized:\\n{dtype_str}\")\n\n# Deserialize\nreconstructed = deserialize_dtype(dtype_str)\nprint(f\"\\nDeserialized: {reconstructed}\")\nprint(f\"Round-trip successful: {complex_dtype == reconstructed}\")\n</code></pre> <pre><code>Serialized:\nStruct({'id': Int64, 'tags': List(String), 'metadata': Struct({'created': Datetime(time_unit='ms', time_zone='UTC'), 'active': Boolean})})\n\nDeserialized: Struct({'id': Int64, 'tags': List(String), 'metadata': Struct({'created': Datetime(time_unit='ms', time_zone='UTC'), 'active': Boolean})})\nRound-trip successful: True\n</code></pre>"},{"location":"user-guide/serde/#integration-with-pydantic","title":"Integration with Pydantic","text":"<p>A common use case is storing Narwhals dtype information in Pydantic model JSON schemas. This allows you to specify precise type information beyond what Python's type system provides.</p>"},{"location":"user-guide/serde/#the-challenge","title":"The Challenge","text":"<p>Pydantic's <code>json_schema_extra</code> parameter must contain JSON-serializable values. Since Narwhals dtype objects are not JSON-serializable, it's necessary to pre-serialize them at field definition time.</p> <p>Important: Pre-serialization Required</p> <p>Pydantic processes <code>json_schema_extra</code> during Field initialization, before any custom schema generators run.</p> <p>This means you cannot use a custom <code>GenerateJsonSchema</code> to automatically serialize Narwhals dtypes.</p> <p>You must serialize them explicitly.</p>"},{"location":"user-guide/serde/#helper-class","title":"Helper Class","text":"<p>Create a helper class to make serialization convenient:</p> <pre><code>from typing import Any\n\nimport narwhals as nw\nfrom anyschema.serde import serialize_dtype, deserialize_dtype\n\n\nclass NarwhalsTypeSerializer:\n    \"\"\"Helper to serialize/deserialize Narwhals dtypes.\"\"\"\n\n    @staticmethod\n    def serialize(dtype: Any) -&gt; str | Any:\n        \"\"\"Convert Narwhals dtype to string, pass through other values.\"\"\"\n        return serialize_dtype(dtype) if isinstance(dtype, nw.dtypes.DType) else dtype\n\n    @staticmethod\n    def deserialize(into_dtype: str) -&gt; nw.dtypes.DType:\n        \"\"\"Convert string back to Narwhals dtype, pass through values that cannot be converted.\"\"\"\n        dtype = deserialize_dtype(into_dtype)\n        return into_dtype if dtype is nw.Unknown() else dtype\n</code></pre>"},{"location":"user-guide/serde/#basic-example","title":"Basic Example","text":"<p>Store dtype information alongside Pydantic fields:</p> <pre><code>import json\nfrom datetime import datetime\nfrom pydantic import BaseModel, Field\n\n\nclass UserModel(BaseModel):\n    \"\"\"User model with Narwhals dtype metadata.\"\"\"\n\n    user_id: int = Field(\n        description=\"Unique user identifier\",\n        json_schema_extra={\n            \"anyschema\": {\n                \"dtype\": NarwhalsTypeSerializer.serialize(nw.UInt64()),\n            },\n        },\n    )\n\n    username: str = Field(\n        description=\"Username\",\n        json_schema_extra={\n            \"anyschema\": {\n                \"dtype\": NarwhalsTypeSerializer.serialize(nw.String()),\n            }\n        },\n    )\n\n    created_at: datetime = Field(\n        description=\"Account creation timestamp\",\n        json_schema_extra={\n            \"anyschema\": {\n                \"dtype\": NarwhalsTypeSerializer.serialize(\n                    nw.Datetime(time_unit=\"ms\", time_zone=\"UTC\")\n                ),\n            }\n        },\n    )\n\n\n# Generate JSON schema\njson_schema = UserModel.model_json_schema()\nfor field_name in UserModel.model_fields:\n    dtype = json.dumps(json_schema[\"properties\"][field_name][\"anyschema\"][\"dtype\"])\n    print(f\"{field_name} dtype is {dtype}\")\n</code></pre> <pre><code>user_id dtype is \"UInt64\"\nusername dtype is \"String\"\ncreated_at dtype is \"Datetime(time_unit='ms', time_zone='UTC')\"\n</code></pre>"},{"location":"user-guide/serde/#complex-field-types","title":"Complex Field Types","text":"<p>Serde works with complex nested types in Pydantic models:</p> <pre><code>import json\nimport narwhals as nw\nfrom anyschema.serde import serialize_dtype, deserialize_dtype\nfrom pydantic import BaseModel, Field\n\n\nclass DataModel(BaseModel):\n    \"\"\"Model with complex nested dtypes.\"\"\"\n\n    tags: list[str] = Field(\n        description=\"List of tags\",\n        json_schema_extra={\n            \"anyschema\": {\"dtype\": serialize_dtype(nw.List(nw.String()))}\n        },\n    )\n\n    metadata: dict = Field(\n        description=\"Structured metadata\",\n        json_schema_extra={\n            \"anyschema\": {\n                \"dtype\": serialize_dtype(\n                    nw.Struct(\n                        {\"id\": nw.Int64(), \"name\": nw.String(), \"active\": nw.Boolean()}\n                    )\n                )\n            }\n        },\n    )\n\n    matrix: list[list[float]] = Field(\n        description=\"2D array\",\n        json_schema_extra={\n            \"anyschema\": {\n                \"dtype\": serialize_dtype(nw.Array(nw.Float32(), shape=(3, 2)))\n            }\n        },\n    )\n\n\njson_schema = DataModel.model_json_schema()\nfor field_name in DataModel.model_fields:\n    dtype = json.dumps(json_schema[\"properties\"][field_name][\"anyschema\"][\"dtype\"])\n    print(f\"{field_name} dtype is {dtype}\")\n</code></pre> <pre><code>tags dtype is \"List(String)\"\nmetadata dtype is \"Struct({'id': Int64, 'name': String, 'active': Boolean})\"\nmatrix dtype is \"Array(Float32, shape=(3, 2))\"\n</code></pre>"},{"location":"user-guide/serde/#deserialize-dtypes-from-json-schema","title":"Deserialize dtypes from json schema","text":""},{"location":"user-guide/serde/#manual-deserialization","title":"Manual deserialization","text":"<p>Extract and deserialize dtypes from the JSON schema:</p> <pre><code>json_schema = UserModel.model_json_schema()\n\nprint(\"Field dtypes from JSON schema:\")\nfor field_name, field_info in json_schema[\"properties\"].items():\n    if into_dtype := field_info.get(\"anyschema\", {}).get(\"dtype\"):\n        dtype = NarwhalsTypeSerializer.deserialize(into_dtype)\n        print(f\"\\t* {field_name}: {dtype}\")\n</code></pre> <pre><code>Field dtypes from JSON schema:\n    * user_id: UInt64\n    * username: String\n    * created_at: Datetime(time_unit='ms', time_zone='UTC')\n</code></pre>"},{"location":"user-guide/serde/#custom-json-decoder","title":"Custom JSON Decoder","text":"<p>For more sophisticated workflows, create custom JSON encoder/decoder classes that automatically handle Narwhals dtypes:</p> <pre><code>from contextlib import suppress\nimport json\nimport narwhals as nw\nfrom anyschema.exceptions import UnsupportedDTypeError\nfrom anyschema.serde import serialize_dtype, deserialize_dtype\n\n\nclass NarwhalsDTypeEncoder(json.JSONEncoder):\n    \"\"\"JSON encoder that automatically serializes Narwhals dtypes.\"\"\"\n\n    def default(self, obj):\n        if isinstance(obj, nw.dtypes.DType):\n            # Return a dict with a special marker\n            return serialize_dtype(obj)\n        return super().default(obj)\n\n\nclass NarwhalsDTypeDecoder(json.JSONDecoder):\n    \"\"\"JSON decoder that automatically deserializes Narwhals dtypes.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(object_hook=self.object_hook, *args, **kwargs)\n\n    @staticmethod\n    def object_hook(obj):\n        \"\"\"Hook to intercept dict objects during decoding.\"\"\"\n        if isinstance((into_dtype := obj.get(\"dtype\")), str):\n            with suppress(UnsupportedDTypeError):\n                dtype = deserialize_dtype(into_dtype)\n                obj[\"dtype\"] = dtype\n\n        return obj\n\n\n# Example: Store arbitrary data with Narwhals dtypes\ndata = {\n    \"schema_version\": \"1.0\",\n    \"fields\": {\n        \"id\": {\n            \"dtype\": nw.UInt64(),\n            \"nullable\": False,\n        },\n        \"tags\": {\n            \"dtype\": nw.List(nw.String()),\n            \"nullable\": True,\n        },\n        \"unknown\": {\"dtype\": \"not a narwhals object\", \"nullable\": \"Maybe!\"},\n    },\n}\n\n# Encode with custom encoder\njson_str = json.dumps(data, cls=NarwhalsDTypeEncoder, indent=2)\nprint(\"Encoded JSON\")\nprint(json_str)\n\n# Decode with custom decoder\nloaded_data = json.loads(json_str, cls=NarwhalsDTypeDecoder)\n\nprint(\"\\nDecoded dtypes:\")\nfor field_name, field_info in loaded_data[\"fields\"].items():\n    dtype = field_info[\"dtype\"]\n    print(f\"  {field_name}: {dtype} (type: {type(dtype).__name__})\")\n</code></pre> <pre><code>Encoded JSON\n{\n  \"schema_version\": \"1.0\",\n  \"fields\": {\n    \"id\": {\n      \"dtype\": \"UInt64\",\n      \"nullable\": false\n    },\n    \"tags\": {\n      \"dtype\": \"List(String)\",\n      \"nullable\": true\n    },\n    \"unknown\": {\n      \"dtype\": \"not a narwhals object\",\n      \"nullable\": \"Maybe!\"\n    }\n  }\n}\n\nDecoded dtypes:\n  id: UInt64 (type: UInt64)\n  tags: List(String) (type: List)\n  unknown: not a narwhals object (type: str)\n</code></pre>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting","text":"<p>This page covers common issues you might encounter when using anyschema and how to resolve them.</p>"},{"location":"user-guide/troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/troubleshooting/#type-not-supported","title":"Type Not Supported","text":"<p>If you are prompted the following error message:</p> <p>NotImplementedError: No parser in the pipeline could handle type: 'type-name'</p> <p>it means that the type you're trying to convert isn't handled by any parser in the pipeline.</p> <p>The solution is to implement a custom parser step and adding it to the pipeline steps:</p> <pre><code>from typing import Any\nimport narwhals as nw\n\nfrom anyschema import AnySchema\nfrom anyschema.parsers import ParserStep, PyTypeStep\nfrom anyschema.typing import FieldConstraints, FieldMetadata, FieldType\n\n\nclass MyCustomTypeStep(ParserStep):\n    def parse(\n        self,\n        input_type: FieldType,\n        constraints: FieldConstraints,\n        metadata: FieldMetadata,\n    ) -&gt; nw.dtypes.DType | None:\n        if input_type is MyCustomType:\n            return nw.String()  # or appropriate dtype\n        return None\n\n\nschema = AnySchema(spec=your_spec, pipeline=[MyCustomTypeStep(), PyTypeStep()])\n</code></pre>"},{"location":"user-guide/troubleshooting/#union-type-error","title":"Union Type Error","text":"<p>If you are prompted the following error message:</p> <p>UnsupportedDTypeError: Union with mixed types is not supported.</p> <p>it means that the dataframe libraries don't support true union types. <code>Optional[T]</code> (i.e., <code>T | None</code>) is supported, however true union types such as <code>float | str</code> are not.</p>"},{"location":"user-guide/troubleshooting/#custom-parser-not-being-called","title":"Custom Parser Not Being Called","text":"<p>There could be multiple reasons why your custom parser's <code>parse</code> method never gets invoked.</p> <p>One reason might be that the order in step sequence is wrong.</p> <p>Parser steps are tried in order. If an earlier parser handles the type, yours won't be called.</p> <pre><code># \u274c PyTypeStep catches everything before CustomStep runs\nsteps = [PyTypeStep(), CustomStep()]\n\n# \u2705 Place custom parsers before general ones\nsteps = [CustomStep(), PyTypeStep()]\n</code></pre> <p>Another reason might be that the step is not registered: make sure you pass the parser to <code>AnySchema</code>:</p> <pre><code># \u2705 Explicitly provide steps\nschema = AnySchema(spec=spec, pipeline=[CustomStep(), PyTypeStep()])\n</code></pre>"},{"location":"user-guide/troubleshooting/#metadata-not-flowing-through","title":"Metadata Not Flowing Through","text":"<p>If your parser steps expects metadata, yet it receives empty metadata, then make sure to include <code>AnnotatedStep</code> in the pipeline:</p> <pre><code>from anyschema.parsers import AnnotatedStep\nfrom anyschema.typing import FieldConstraints, FieldMetadata, FieldType\n\nsteps = (\n    AnnotatedStep(),  # Must be present to extract metadata\n    YourCustomStep(),\n    PyTypeStep(),\n)\n</code></pre> <p>On top of that, verify metadata are preserved when recursively parsing:</p> <pre><code>class CustomStep(ParserStep):\n    def parse(\n        self,\n        input_type: FieldType,\n        constraints: FieldConstraints,\n        metadata: FieldMetadata,\n    ) -&gt; nw.dtypes.DType | None:\n        inner_type = get_args(input_type)[0]\n        # \u2705 Pass constraints and metadata through\n        return self.pipeline.parse(\n            inner_type, constraints=constraints, metadata=metadata\n        )\n</code></pre>"},{"location":"user-guide/troubleshooting/#adapter-not-processing-nested-structures","title":"Adapter Not Processing Nested Structures","text":"<p>If nested specification are not being converted correctly, we suggest to \"hack\" using <code>TypedDict</code>'s when adapting nested schemas:</p> <pre><code>from typing import TypedDict\nfrom anyschema.typing import FieldSpecIterable\n\n\ndef my_adapter(spec: MySchema) -&gt; FieldSpecIterable:\n    for field_name, field_value in spec.fields.items():\n        if is_nested(field_value):\n            # Create a TypedDict for nested structures\n            nested_dict = {name: type_ for name, type_, _, _ in my_adapter(field_value)}\n            nested_typed_dict = TypedDict(f\"{field_name.title()}Schema\", nested_dict)\n            yield field_name, nested_typed_dict, (), {}\n        else:\n            yield field_name, field_value, (), {}\n</code></pre> <p>In general, make sure your adapter recursively processes nested structures.</p> <p>See the Advanced Usage - Adapter for Nested Structures for a complete example.</p>"},{"location":"user-guide/troubleshooting/#why-are-all-my-fields-nullable-in-pyarrow","title":"Why are all my fields nullable in <code>pyarrow</code>?","text":"<p>If you're creating PyArrow tables without passing a schema, all fields will be nullable by default (this is PyArrow's standard behavior):</p> <pre><code>import pyarrow as pa\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    id: int  # Non-nullable in Pydantic\n    name: str  # Non-nullable in Pydantic\n    email: str | None  # Nullable\n\n\nusers = [{\"id\": 1, \"name\": \"Alice\", \"email\": \"alice@example.com\"}]\n\n# Without schema: everything is nullable\ntable = pa.Table.from_pylist(users)\nprint(\"Without explicit schema:\")\nprint(table.schema)\n</code></pre> <pre><code>Without explicit schema:\nid: int64\nname: string\nemail: string\n</code></pre> <p>Solution: Use <code>anyschema</code> to generate a schema that preserves nullability information:</p> <pre><code>from anyschema import AnySchema\n\nschema = AnySchema(spec=User)\narrow_schema = schema.to_arrow()\n\n# With schema: nullability matches type annotations\ntable_with_schema = pa.Table.from_pylist(users, schema=arrow_schema)\nprint(\"With anyschema-generated schema:\")\nprint(table_with_schema.schema)\n</code></pre> <pre><code>With anyschema-generated schema:\nid: int64 not null\nname: string not null\nemail: string\n</code></pre> <p>Now <code>id</code> and <code>name</code> are correctly marked as <code>not null</code>!</p> <p>See the Metadata guide on nullable semantics for more details.</p>"},{"location":"user-guide/troubleshooting/#field-shows-as-nullable-when-it-shouldnt-be","title":"Field shows as nullable when it shouldn't be","text":"<p>If a field appears nullable but your type annotations say it's not nullable, check:</p> <ol> <li>Type annotation is correct: Use <code>str</code>, not <code>str | None</code> or <code>Optional[str]</code></li> <li>Not overridden by metadata: Check if <code>\"anyschema\": {\"nullable\": True}</code> was set in metadata</li> </ol> <pre><code>from pydantic import BaseModel, Field\nfrom anyschema import AnySchema\n\n\nclass Example(BaseModel):\n    # This WILL be nullable despite the type annotation\n    field: str = Field(json_schema_extra={\"anyschema\": {\"nullable\": True}})\n\n\nschema = AnySchema(spec=Example)\nprint(f\"field is nullable: {schema.fields['field'].nullable}\")\n</code></pre> <pre><code>field is nullable: True\n</code></pre> <p>Explicit metadata always takes precedence over type inference.</p>"},{"location":"user-guide/troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you're still stuck after trying these solutions:</p> <ol> <li>Check existing issues: Search the GitHub issue tracker for similar problems.</li> <li> <p>Create a minimal reproduction: Prepare a minimal code example that demonstrates the issue</p> <pre><code>from anyschema import AnySchema\nfrom pydantic import BaseModel\n\n\nclass MinimalExample(BaseModel):\n    field: YourProblematicType\n\n\nschema = AnySchema(spec=MinimalExample)\n# Error occurs here\n# ...\n\narrow_schema = schema.to_arrow()\n</code></pre> </li> <li> <p>Open an issue and make sure to include:</p> <ul> <li>Your Python and anyschema versions</li> <li>The minimal reproduction code</li> <li>The full error traceback</li> <li>What you've already tried</li> </ul> </li> </ol>"}]}